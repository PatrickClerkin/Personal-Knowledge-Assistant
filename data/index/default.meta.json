{
  "embedding_dim": 384,
  "chunks": [
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_0",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "Advanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus. 1 \n \n \nComposition and Object Reuse \n \nOne of the principal goals of the object-oriented paradigm is to promote the reuse of software \nartefacts within an application and between applications. There are two basic mechanisms for \nachieving reuse in the paradigm \u2013 generalisation and composition.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 399,
      "chunk_index": 0,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_1",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "The former relates to the \nreuse of state and behaviour in concrete classes that are derived from a more abstract entity and \nimplicitly supports polymorphism. Composition relates to the construction of new types from \na collection of existing classes and represents the primary mechanism for reuse in the object-\noriented paradigm.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 399,
      "end_char": 731,
      "chunk_index": 1,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_2",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "While it is usually not cited as a \u201cpillar\u201d of the object-oriented paradigm, object reuse through \ncomposition is predicated on the proper application of abstraction and encapsulation. Indeed, \ncomposition promotes reuse precisely through the encapsulation of reusable behaviour in an \nabstract entity.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 731,
      "end_char": 1033,
      "chunk_index": 2,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_3",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "Moreover, although inheritance and polymorphism are immediately identifiable \nas key pillars in the object-oriented paradigm, these concepts are also a consequence of the \napplication of abstraction and encapsulation. The whole point of encapsulation is to separate \nthe stable and volatile parts of an application into abstract and concrete types respectively.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 1033,
      "end_char": 1394,
      "chunk_index": 3,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_4",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "The \nextracted abstract components are highly reusable as their generality confers upon them the \nflexibility to be used in a variety of different contexts. When not part of a clear hierarchy of \nresponsibility, such abstract entities should be reused through composition. When used \ncorrectly, composition enables behaviours and responsibilities to be dynamically assigned to \nan object at run-time. Implementation inheritance assigns behaviours and responsibilities \nstatically at compile time.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 1394,
      "end_char": 1890,
      "chunk_index": 4,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_5",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "Composition and Scope \nThere exist a number of different forms of composition that can be used in an application and \neach form is best categorised on the basis of the scope of the relationship between the container \nobject and the objects that it is composed with. Consider the following UML diagram that \nillustrates four forms of composition: \n \n \n \n \nThe diamonds denote a strong form of composition and require that the containing object \nmaintain an instance variable of that type.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 1890,
      "end_char": 2377,
      "chunk_index": 5,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_6",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "For example, the type C should have instance \nvariables of type A and E, defined at a class level. The full line represents an association \nbetween class C and class D, where the classes interact with another class at a method level. The dashed line denotes a dependency, the weakest form of composition. When an object has \n\nAdvanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 2377,
      "end_char": 2807,
      "chunk_index": 6,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_7",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "2 \ngone out of scope, the JVM will invoke the inherited finalize() method before the garbage \ncollector removes the instance from the heap. The finalize() method can thus be used to \ndetermine the scope of a composed object and the exact form of composition it manifests. There are four basic forms of composition: \n \n1. Dependency (Dashed Line with Arrow): the scope of composition is restricted to the \nimplementation details of a method.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 2807,
      "end_char": 3247,
      "chunk_index": 7,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_8",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "The dependency is not visible or accessible outside \nthe containing class in any way. The composed object is normally fully encapsulated, \nbut may also be shared if singletons are being used. 2. Association (Fill Line with Arrow):  The scope of a composed object is outside of the \nclass itself, as the composed object is either passed in as a method argument or returned \nas a type from the method. Either way, the method invocator will have a reference to \nthe composed object at that point in time. 3.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 3247,
      "end_char": 3751,
      "chunk_index": 8,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_9",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "3. Aggregation (White Diamond with Arrow): The composed object is declared at a \nclass level, i.e. has the scope of an instance variable. An external call may have direct \naccess to the composed object if it passed the object to the constructor of a container \nclass. Indirect access to the composed object can arise from returning a reference to a \nmutable instance variable from an accessor method.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 3749,
      "end_char": 4149,
      "chunk_index": 9,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_10",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "In an aggregation relationship, the \ncomposed object may be referenced by some other object and thus may have a scope \ngreater than its container. Consequently, if the container goes out of scope and is \ngarbage collected, the composed object may outlive its container. 4. Full Composition  (Black Diamond with Arrow): The composed object is \ncompletely encapsulated inside the containing class, with no possibility of direct or \nindirect access.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 4149,
      "end_char": 4595,
      "chunk_index": 10,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_11",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "The scope of the composed object is thus restricted to the class using \nit. Consequently, when the containing class goes out of scope and is garbage collected, \nthe composed class is guaranteed to be garbage collected with it. Note that composition and delegation go hand-in-glove. The whole point of composing objects \nis to reuse their behaviour in another class. That reuse of behaviour relates to the invocation of \nthe methods exposed by the composed class.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 4595,
      "end_char": 5057,
      "chunk_index": 11,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_12",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "Consequently, the methods of the containing class \nshould delegate tasks to composed objects where possible. It is important to realise that, using \ncomposition, a class can be capable of doing many different things without violating the Single \nResponsibility Principle.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 5057,
      "end_char": 5328,
      "chunk_index": 12,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_13",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "Finalise \n \n \nExercises \nIn this practical, we will employ composition to reuse the cryptographic capability already \nprovided in the Java SDK and explore how composition can be combined with abstraction \nand encapsulation to create cohesive and loosely coupled designs.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 5328,
      "end_char": 5598,
      "chunk_index": 13,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_14",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "\u2022 \nUsing Eclipse create a new class called RSACypher, composed with the following \ninstance variables imported from the java.security an javax.crypto APIs:  \no Cipher cypher  \no KeyPair keyRing \n \nOverride the inherited method finalize() to output the name of the class and its Object \nID. Advanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 5598,
      "end_char": 5992,
      "chunk_index": 14,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_15",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "3 \n\u2022 \nIn the constructor of RSACypher, implement the functionality to initialise both \ninstance variables as follows: \n \nKeyPairGenerator keyGen = KeyPairGenerator.getInstance(\"RSA\"); \nkeyGen.initialize(2048); \nkeyRing = keyGen.generateKeyPair(); \ncypher = Cipher.getInstance(\"RSA/ECB/PKCS1Padding\"); \n \nExplain the scope of the three variables defined and the form of composition for \neach.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 5992,
      "end_char": 6383,
      "chunk_index": 15,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_16",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "\u2022 \nDeclare the following two methods for encryption and decryption: \n \npublic byte[] encrypt(byte[] plainText) throws Throwable{ \npublic byte[] decrypt(byte[] cypherText) throws Throwable{ \n \nImplement each method by calling the init() method of cypher with the appropriate \nparameters. Use the key returned by keyRing.getPublic() and keyRing.getPublic() for \nencryption and decryption respectively.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 6383,
      "end_char": 6782,
      "chunk_index": 16,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_17",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "\u2022 \nCreate a TestRunner class with a main method and exercise the functionality of the \nclass to ensure that it works correctly. \u2022 \nConsider the following code required to encrypt using the DES and AES standards.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 6782,
      "end_char": 6993,
      "chunk_index": 17,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_18",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "Note that DES and AES are symmetric cryptographic methods: \n \nKeyGenerator keyGen = KeyGenerator.getInstance(\"DES\"); \nkeyGen.init(128);  \nkey key = keyGen.generateKey(); \nCipher cypher = Cipher.getInstance(\"DES/ECB/PKCS5Padding\") \ncypher.init(Cipher.DECRYPT_MODE, key); \nbytep[] result =  cypher.doFinal(cypherText); \n \nKeyGenerator keyGen = KeyGenerator.getInstance(\"AES\");  \nkeyGen.init(128);  \nkey key = keyGen.generateKey(); \nCipher cypher  = Cipher.getInstance(\"AES/ECB/PKCS5Padding\"); \ncypher.init(Cipher.DECRYPT_MODE, key); \nbytep[] result =  cypher.doFinal(cypherText); \n \nIdentify and implement the alterations to the class RSACypher required to ensure \ncompatibility with symmetric keys.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 6993,
      "end_char": 7690,
      "chunk_index": 18,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_19",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "\u2022 \nUsing the refactoring menu, exact a superclass called AbstractCypher from the class \nRSACypher that contains a corpus of reusable code. Use abstract methods where \nappropriate. Explain the impact that this action has on the overall design of the \napplication. Create two new classes called DESCypher and AESCypher that directly inherit from \nAbstractCypher and implement the remaining code required to complete the \nfunctionality of each class.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 7690,
      "end_char": 8137,
      "chunk_index": 19,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_20",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "In each class, override the inherited method finalize() to \noutput the name of the class and its Object ID. \u2022 \nUsing the refactoring menu, extract an interface called Cypherable from the class \nAbstractCypher and alter the type used in TestRunner to the new interface. Explain \nthe impact that this action has on the overall design of the application. Advanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 8137,
      "end_char": 8593,
      "chunk_index": 20,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_21",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "4 \n \n\u2022 \nCreate an enum called CypherType with the options of AES, DES and RSA.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 4,
      "start_char": 8593,
      "end_char": 8671,
      "chunk_index": 21,
      "total_chunks": 23
    },
    {
      "chunk_id": "23d9cb02ad53f8adf3b2dfc29fb7957a_chunk_22",
      "doc_id": "23d9cb02ad53f8adf3b2dfc29fb7957a",
      "content": "\u2022 \nCreate a new class called CypherFactory defined as follows and alter TestRunner to \nreturn the type returned by the factory method: \n \npublic class CypherFactory { \n   private static CypherFactory f = new Throwable(); \n \n \n   private CypherFactory(){} \n \n \n   public static CypherFactory getInstance(){ \n      return f; \n   } \n \n \n   public Cypherable getCypher(CypherType type) throws Throwable { \n      if (type == CypherType.DES) { \n         return new DESCypher(); \n      }else if (type == CypherType.RSA) { \n         return new RSACypher(); \n      }else {  \n         return new AESCypher(); \n      } \n \n \n   } \n} \n \n \nExplain the impact that this action has on the overall design of the application.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 4,
      "start_char": 8671,
      "end_char": 9378,
      "chunk_index": 22,
      "total_chunks": 23
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_0",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Artificial Neural Networks\nDept. of Computer Science & Applied Physics \u2013 Artificial Intelligence\n\u2022 Biological Neural Networks\n\u2022 A Basic Prediction Machine\n\u2022 A Basic Classifying Machine\n\u2022 Activation Functions\n\u2022 The Neuron \n\u2022 The Perceptron\n\u2022 Training a Perceptron\n\u2022 Multilayer Neural Networks\n\u2022 Backpropagation \n\u2022 Pattern Matching Example\n\u2022 Radial Basis Function Networks\n\nArtificial Neural Networks (ANNs)\n\u2022 ANNs are a model of reasoning based on the human brain.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 463,
      "chunk_index": 0,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_1",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The brain consists of a densely interconnected\nset of nerve cells (information-processing units) \ncalled neurons. \u2022 Approximately 1010 neurons interconnected by ~60x1012\nconnections called synapses \u2192 ~103-4 synapses/neuron. \u2022 The human brain can use multiple neurons simultaneously. \u2022 Your brain is multi-threaded!",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 2,
      "start_char": 463,
      "end_char": 779,
      "chunk_index": 1,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_2",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Your brain is multi-threaded! \u2022 Allows for massively parallel processing\nbeyond the capability of current supercomputers\u2026\n\u2022 Neurons consist of a cell body (soma), a number of fibres \ncalled dendrites, and a single long fibre called an axon. \u2022 Dendrites branch into a network around the soma. \u2022 Axon stretches out to the dendrites and somas of \nother neurons.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 2,
      "start_char": 748,
      "end_char": 1108,
      "chunk_index": 2,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_3",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "2\n\nGoogle QuickDraw\n\u2022 See https://quickdraw.withgoogle.com\n3\n\nBiological Neural Networks\n\u2022 Signals are propagated by complex electrochemical reactions between neurons. \u2022 Chemical substances released from synapses cause a change in the electrical potential of \nthe cell body. \u2022 When the potential reaches a threshold, an electrical pulse (action potential) is sent down \nthrough the axon. \u2022 Pulse spreads out and eventually reaches synapses, causing them to increase or decrease \ntheir potential.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 2,
      "start_char": 1108,
      "end_char": 1603,
      "chunk_index": 3,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_4",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 A neural network exhibits plasticity\n\u2022 Neurons demonstrate long-term changes in connection strength\nin response to patterns of stimulation. \u2022 Can form new connections with other neurons \n\u2022 Early childhood. \u2022 Entire collections of neurons can migrate from one \nplace to another. Highly evolved and complex. \u2022 These mechanisms form the basis for learning in the brain.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 4,
      "start_char": 1603,
      "end_char": 1971,
      "chunk_index": 4,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_5",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "4\n\nHuman Neuron\n5\n\nNeurons in Nature\u2026\n6\nNeurons\nOrganism\n0 (Yes, Zero!)\nSponge\n302\nRoundworm\n5600\nJellyfish\n10x103\nLeech\n100x103\nLobster\n250x103\nFruit Fly\n250x103\nAnt\n960x103\nHoney bee\n16x106\nFrog\n71x106\nHouse Mouse\n200x106\nBrown Rat\n500x106\nOctopus\n760x106\nCat\n6.3x109\nRhesus Macaque\n86x109\nHuman\n257x109\nAfrican Elephant\n\nHuman Brains v/s Computers\n\u2022 Deep Blue: 512 processors, 1 TFLOP (1012 Floating point Operations Per Second)\n\u2022 Customised IBM supercomputer for chess-playing\n\u2022 First computer to win a match against a reigning world champion (1997)\n7\nComputers (Intel Core Ultra 9 Arrow Lake)\nBrains (Adult Cortex)\nSurface Area: 243 mm2\nSurface Area: 2500 cm2\nCrystalline\nSquishy\nTransistors: 17.8 Billion\nNeurons: 20 billion\nSynapses:  240 trillion\nTransistor size: 3 nm\nNeuron Size: 15 um\nSynapse Size: 1 um\nFLOPS: 1.42 trillion\nClock frequency: ~5 GHz\nSynaptic OPS: 30 trillion\nNeuron firing rate: 0.1-200 times/sec\nSynaptic delay between neurons: 1-2 ms\nPower Usage: ~135 W\nPower Usage: 12 W\nOperations/Joule: ~8 billion\nOperations/Joule: 2.5 trillion\n\nArtificial Neural Networks\n\u2022 Artificial Neural Networks (ANNs) are mathematical models that attempt to \nsimulate the structure and functional aspects of biological neural networks.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 4,
      "start_char": 1971,
      "end_char": 3213,
      "chunk_index": 5,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_6",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Several simple and highly interconnected processors called neurons. \u2022 Neurons connected by weighted links that propagate signals. \u2022 Each neuron receives multiple input signals through its connections, but only \never produces one output. \u2022 Output signal transmitted through outgoing connection (axon). \u2022 Axon splits into a number of branches that transmit the same signal. The signal is not\ndivided! \u2022 Outgoing branches terminate at the incoming connections of other neurons in \nthe ANN.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 8,
      "start_char": 3213,
      "end_char": 3701,
      "chunk_index": 6,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_7",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The more nodes (neurons), the more powerful the network. \u2022 Weights express the strength (importance) of each input. \u2022 A NN learns by repeatedly adjusting these weights. 8\n\nArtificial Neural Networks\n\u2022 A NN is a graph composed of a hierarchy of layers of neurons. \u2022 Neurons connected to the external environment by I/O layers and internally by links \n(weighted edges). \u2022 Weights are the basic means of long-term learning. \u2022 Weights are modified to align the NN I/O with the external environment. \u2022 i.e.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 8,
      "start_char": 3701,
      "end_char": 4204,
      "chunk_index": 7,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_8",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 i.e. the NN has to be trained. 9\nThere is no NN design methodology for a topology \napplicable to a large number of problems. NN design is typically empirical, and the topology \ndepends on the specific problem. Difficult to interpret the result of a classification. Cannot tell why a NN came to a decision.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 9,
      "start_char": 4198,
      "end_char": 4505,
      "chunk_index": 8,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_9",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Cannot tell why a NN came to a decision. Artificial Neural Networks\n10\nhttps://commons.wikimedia.org/wiki/File:Sinh_cosh_tanh.svg#/media/File:Sinh_cosh_tanh.svg\n\ud835\udc4c(\ud835\udc4b\u0bdc) = tanh \ud835\udc60\ud835\udc62\ud835\udc5a= 0.999\n\nData Sets\n11\nEach column is extracted as a feature vector for preprocessing. Each element corresponds to the value of that feature for one instance.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 9,
      "start_char": 4465,
      "end_char": 4799,
      "chunk_index": 9,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_10",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Or sample\n\u2022 For a typical ANN:\n\u2022 Inputs fed by numerical values\n\u2022 Outputs numerical values\n\nA Basic Prediction Machine\n\u2022 Consider a simple prediction machine that takes some input (miles), does some \n\u201cthinking\u201d, and then outputs an answer (kilometres). \u2022 In its initial state, the machine does not know the translation formula. \u2022 Relationship between input and output is an adjustable linear function, i.e. \ud835\udc4c =  \ud835\udc4e +  \ud835\udc4f\ud835\udc4b, where \ud835\udc4f\ncontrols the slope of the line, and \ud835\udc4eis the intercept.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 11,
      "start_char": 4799,
      "end_char": 5282,
      "chunk_index": 10,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_11",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Can the machine be trained to learn the correct function? \u2022 Yes! The variable \ud835\udc4fcontrols the equation of the line and is analogous to a weight that can be \napplied to the input to compute the output. \u2022 Machine learns by processing a set of training data and adjusting the weight (\ud835\udc4f) until \nthe actual and expected outputs converge. \u2022 Called linear regression. Output is a real number.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 12,
      "start_char": 5282,
      "end_char": 5667,
      "chunk_index": 11,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_12",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Output is a real number. 12\nInitialise the weight b to a small random real number\nWhile an Error Exists in the Output //This loop runs until convergence\nFor Each Item in the Training Set\nCalculate the current output of the predictor for the Item\nUpdate the weight, depending on the level of error in the output \nNext\nWend\n\nA Basic Prediction Machine\n13\nKM\nMiles\n32\n20\n60\n37\n272\n169\n513\n319\n758\n471\nA predictor takes an input and predicts \nthe output. Training a linear predictor involves \nfinding the right slope.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 12,
      "start_char": 5643,
      "end_char": 6156,
      "chunk_index": 12,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_13",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "https://bharathikannann.github.io/blogs/a-visual-intro-to-linear-regression-math/\n\nA Basic Classifying Machine\n14\n\u2022 Consider the following classes of bacteria (there are many others):\n\u2022 Can a training technique similar to the predictor be used? Yes, but only for \nclassification, not \nfor prediction. Only works if the \nclasses are linearly \nseparable, i.e. can \nbe divided by a \nstraight line. A Basic Classifying Machine\n15\nThe classifier takes an input and \nclassifies it from a set of patterns.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 13,
      "start_char": 6156,
      "end_char": 6654,
      "chunk_index": 13,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_14",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Training a linear classifier also \ninvolves finding the right slope that \ncompletely separates the two classes. Could add an extra Z-axis for flexuous \nbacteria.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 15,
      "start_char": 6654,
      "end_char": 6815,
      "chunk_index": 14,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_15",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Could add an extra Z-axis for flexuous \nbacteria. Activation Functions\n\u2022 Crucial component in neural networks\n\u2022 Introduce non-linearity into the network\n\u2022 Allow it to learn complex patterns\n\u2022 Each has its own\u2026 \n\u2022 Characteristics\n\u2022 Advantages\n\u2022 Potential use cases\n\u2022 Understanding activation functions\n(and their derivatives) is part of \nunderstanding how neural networks\u2026 \n\u2022 Learn \n\u2022 Make predictions\n16\n\nActivation Functions \u2013 Hard Limiters\n17\nz\nz\nz\nNote: Not a hard limiter\n(here only as a reference)\nz\nz\n\u2208{\u22121, +1}\n\nActivation Functions \u2013 Soft Limiters\n18\nz\nz\nz\ny\n\nActivation Functions & Derivatives\n19\nIdentity\nStep\n\ud835\udc53\u11f1\ud835\udc65\u2245\u0394\ud835\udc66\n\u0394\ud835\udc65= \ud835\udc66\u0bd5\u2212\ud835\udc66\u0bd4\n\ud835\udc65\u0bd5\u2212\ud835\udc65\u0bd4\n\u0394\ud835\udc65, \u0394\ud835\udc66\u21920\n\ud835\udc87\u11f1\ud835\udfd0\u2245\u0394\ud835\udc66\n\u0394\ud835\udc65= 2.01 \u22121.99\n2.01 \u22121.99 = 0.02\n0.02 = \ud835\udfcf\n\u2208(\u2212\u221e, \u221e)\n\u2208{0, 1}\n\n20\nSigmoid\nHyperbolic Tangent (Tanh)\nActivation Functions & Derivatives\n\ud835\udc53\u11f12 \u2245\ud835\udc532.01 \u2212\ud835\udc531.99\n2.01 \u22121.99\n\ud835\udc532.01 =\n1\n1 + \ud835\udc52\u0b3f\u0b36.\u0b34\u0b35\u22450.8818\n\ud835\udc531.99 =\n1\n1 + \ud835\udc52\u0b3f\u0b35.\u0b3d\u0b3d\u22450.8797\n\u21d2\ud835\udc53\u11f12 \u22450.8818 \u22120.8797\n2.01 \u22121.99\n=\n\u22450.105\n\ud835\udf0e\u11f1\ud835\udc65= \ud835\udf0e\ud835\udc65\u22c51 \u2212\ud835\udf0e\ud835\udc65\nhttps://www.tinkershop.net/ml/sigmoid_calculator.html\n\u2208[0, 1]\n\ud835\udc53(\ud835\udc65) \u2208[\u22121, 1]\n\nActivation Functions & Derivatives\n21\nHyperbolic Tangent Sigmoid (Tansig)\nInverse Tangent  (Arctan)\n\nActivation Functions & Derivatives\n22\nLog-Sigmoid (LogSig)\nGaussian (Bell Curve)\n\nActivation Functions & Derivatives\n23\nRectified Linear Unit (ReLU)\nExponential Linear Unit (ELU)\nCould be considered as \ud835\udf03= 0\n\u2208[0, \u221e)\n\nActivation Functions & Derivatives\n24\nLeaky ReLU\nScaled Exponential Linear Unit (SELU)\n\u2208(\u2212\u221e, \u221e)\n\nActivation Functions & Derivatives\n25\nInverse Root Square Unit (IRSU)\nInverse Root Square Linear Unit (IRSLU)\n\nActivation Functions & Derivatives\n26\nSoftplus\nSoftsign\n\nActivation Functions & Derivatives\n27\nSwish\nCardinal Sine  (Sinc)\n\nActivation Functions \u2013 SoftMax\n\u2022 Widely used in the output layer of a NN.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 15,
      "start_char": 6766,
      "end_char": 8495,
      "chunk_index": 15,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_16",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "The function transforms all the values in \nthe output layer into a probability distribution. \u2022 All the SoftMax scores in the output layer sum up to 1. \u2022 Ideal for classification problems, especially multi-class classification. \u2022 The SoftMax function is the confidence score for each classification. \u2022 The predicted class is the output node with the highest value. 28\nhttps://archive.ics.uci.edu/dataset/111/zoo\nhttps://www.kaggle.com/datasets/uciml/zoo-animal-classification\nhttp://mycalcsolutions.com/calculator?",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 28,
      "start_char": 8495,
      "end_char": 9008,
      "chunk_index": 16,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_17",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "mathematics;functions;softmax\n\nNormalising Data\n\u2022 Data is usually normalised before presented to a NN for training or processing. \u2022 Normalisation squashes the input into a consistent range with upper and lower bounds, \ntypically in [0..1], [-1..1], [-2..2] (depends on the function). \u2022 Essential for activation functions, as inputs too high/low will drive neurons to \nproduce too high/low values. \u2022 E.g., the Sigmoid function\u2019s practical input range is in ~[-4..4].",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 28,
      "start_char": 9008,
      "end_char": 9473,
      "chunk_index": 17,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_18",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Similarly, for the Tanh\nfunction, it is in ~[-2..2]. \u2022 To normalise between a specific range, \ud835\udc41\ud835\udc5a\ud835\udc56\ud835\udc5band \ud835\udc41\ud835\udc5a\ud835\udc4e\ud835\udc65, e.g. [-2..2]:\n\u2022 Where \ud835\udc4b\ud835\udc5a\ud835\udc56\ud835\udc5band \ud835\udc4b\ud835\udc5a\ud835\udc4e\ud835\udc65denote the bounds of the input variable \ud835\udc4b. For example\u2026\n29\n\nNormalising Data\n\u2022 To normalise data between 0 and 1, the equation can be simplified as:\n\u2022 Applying normalisation to a data set trains an \nNN to produce normalised output values.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 29,
      "start_char": 9473,
      "end_char": 9853,
      "chunk_index": 18,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_19",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The output must be de-normalised too:\n\u2022 To de-normalise between 0 and 1, the equation can be simplified as:\n30\nExample:\nFishLength = {36, 25, 44, 38, 35, 19, 14, 31, 26}\nNormalise this attribute vector to [0..1]\nPartial solution:\n\ud835\udc4b\u0be1\u0be2\u0be5\u0be0=\n\ud835\udfd0\ud835\udfd3\u0b3f\u0b35\u0b38\n\u0b37\u0b3c\u0b3f\u0b35\u0b38=\n\u0b35\u0b35\n\u0b36\u0b38\u22450.458\n{\u2026, 0.458, \u2026, \u2026, \u2026, \u2026, \u2026, \u2026, \u2026}\n\nThe Neuron (McCulloch & Pitts, 1943)\n\u2022 Each neuron is an elementary information processing unit. \u2022 Has a means of computing its activation level given the inputs and numerical weights.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 30,
      "start_char": 9853,
      "end_char": 10334,
      "chunk_index": 19,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_20",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Constructing a NN involves deciding on the network topology. \u2022 The number of neurons and connections. \u2022 How neurons are connected together. \u2022 The type of learning algorithm to use. 31\nThe first (though highly simplified) mathematical \nmodel of a biological neuron, which\n1. receives several signals from its input links, \n2. computes a new activation level, and \n3. sends it as an output signal through its output \nlinks. Output can be\u2026\n-\nraw data (final solution) or\n-\noutputs to other neurons.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 31,
      "start_char": 10334,
      "end_char": 10831,
      "chunk_index": 20,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_21",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "McCulloch, Warren S and Pitts, Walter, \"A logical calculus of the ideas immanent in nervous activity\", Bulletin of Mathematical Biophysics, Vol. 5, pp. 115--133, Springer, 1943. Fixed\nBinary\nBinary\n\nThe Neuron (McCulloch & Pitts, 1943)\n\u2022 A neuron computes the weighted sum of the input signals and compares the \nresult with a threshold \u03b8. \u2022 If net input < \u03b8, then neuron output is 0. If net input >= \u03b8, the neuron becomes activated,\nand output is set to 1.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 31,
      "start_char": 10831,
      "end_char": 11287,
      "chunk_index": 21,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_22",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Neuron uses the following transfer/activation function:\n\u2022 Where \ud835\udc65is the net weighted input to the neuron, \ud835\udc65\ud835\udc56is the value of input \ud835\udc56, \ud835\udc64\ud835\udc56is the weight \nof input \ud835\udc56, \ud835\udc5bis the number of neuron inputs, and \ud835\udc53\ud835\udc65= \ud835\udc66is the output of the neuron. \u2022 Called a step function. Actual output of neuron with step activation function is:\n\u2022 The activation function in a neuron does not have to be a step function. \u2022 Neurons can use any hard limiter or soft transfer functions.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 32,
      "start_char": 11287,
      "end_char": 11743,
      "chunk_index": 22,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_23",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "32\n\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5d\n\nThe Perceptron (Rosenblatt, 1958)\n\u2022 Simplest form of a NN. Introduced by Frank Rosenblatt in 1958. \u2022 Consists of a single neuron with adjustable synaptic weights and a hard limiter for \nclassification. \u2022 Based on McCulloch & Pitts model. The aim is to classify inputs. \u2022 Model consists of a linear combiner and a hard limiter. \u2022 The weighted sum of inputs is applied to the hard limiter.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 32,
      "start_char": 11743,
      "end_char": 12139,
      "chunk_index": 23,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_24",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "33\nRosenblatt, Frank, \"The perceptron: a probabilistic model for information storage and organization in the brain.\" Psychological Review, Vol. 65, No. 6, pp. 386, American \nPsychological Association, 1958. The Perceptron (Rosenblatt, 1958)\n\u2022 The objective of a perceptron is to classify inputs (external stimuli):\n\u2022 Inputs \ud835\udc651, \ud835\udc652, \ud835\udc653 are classified into one of two classes, \ud835\udc341 and \ud835\udc342. \u2022 Divides n-dimensional space into decision regions by a hyperplane.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 33,
      "start_char": 12139,
      "end_char": 12593,
      "chunk_index": 24,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_25",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Hyperplane defined by a linearly separable function:\n\u2022 Decision boundary is a straight line for two inputs, \ud835\udc651 and \ud835\udc652. 34\nThe threshold \u03b8 is used to \nshift the decision boundary\nDecision Boundary\nPoint 1 lies above the \nboundary line and belongs to \nClass \ud835\udc34\u0b35. Point 2, under the boundary \nline, belongs to Class \ud835\udc34\u0b36.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 34,
      "start_char": 12593,
      "end_char": 12910,
      "chunk_index": 25,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_26",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Threshold is used to shift the \nboundary\n\nThe Perceptron (Rosenblatt, 1958)\n\u2022 Three-input perceptron defined by the following hyperplane:\n\u2022 Separating plane defined by the same equation, and can be implemented as:\n\ud835\udc651\ud835\udc641 + \ud835\udc652\ud835\udc642 + \ud835\udc653\ud835\udc643 \u2013 \ud835\udf03= 0\n35\nDecision Boundaries\n\nTraining a Perceptron\n\u2022 Classification achieved by making small adjustments in weights to reduce the \ndifference between actual and desired outputs. \u2022 Initial weights are randomly assigned. Usually in the range [-0.5, 0.5].",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 34,
      "start_char": 12910,
      "end_char": 13397,
      "chunk_index": 26,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_27",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Usually in the range [-0.5, 0.5]. \u2022 Weights are then updated to obtain output consistent with training examples. \u2022 The process of weight-training in a perceptron is as follows:\n\u2022 If the actual output at iteration \ud835\udc5dis \ud835\udc4c(\ud835\udc5d) and the desired output is \ud835\udc4c\ud835\udc51(\ud835\udc5d), then the error is \ngiven by:\n\u2022 Iteration \ud835\udc5dis the \ud835\udc5d\ud835\udc61\u210etraining example given to the perceptron.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 36,
      "start_char": 13364,
      "end_char": 13712,
      "chunk_index": 27,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_28",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 As each input contributes \ud835\udc65\ud835\udc56(\ud835\udc5d)\ud835\udc64\ud835\udc56(\ud835\udc5d) to the total input \ud835\udc4b(\ud835\udc5d):\n\u2022 If input \ud835\udc65\ud835\udc56(\ud835\udc5d) is positive, an increase in its weight \ud835\udc64\ud835\udc56(\ud835\udc5d) tends to increase perceptron \noutput \ud835\udc4c(\ud835\udc5d). Decreases \ud835\udc4c(\ud835\udc5d) if negative. \u2022 Therefore, the following perceptron learning rule holds:\n\u2022 Where \ud835\udefc(alpha) is the learning rate (a positive constant)\n36\n\nTraining a Perceptron\n\u2022 The perceptron training algorithm consists of four steps:\n1. Initialisation: Set the initial weights and threshold value. 2. Activation: Apply inputs and desired outputs. 3.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 36,
      "start_char": 13712,
      "end_char": 14229,
      "chunk_index": 28,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_29",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "2. Activation: Apply inputs and desired outputs. 3. Weight Training: Update the weights to conform with output. 4. Iteration: Go back to Step 2 until convergence. \u2022 Step 1: Initialisation\n\u2022 Set the initial weights \ud835\udc641, \ud835\udc642, \u2026 , \ud835\udc64\ud835\udc5b and threshold \u03b8 to random numbers \nin the range [-0.5, 0.5]. \u2022 Step 2: Activation\n\u2022 Activate the perceptron by applying inputs \ud835\udc651(\ud835\udc5d), \ud835\udc652(\ud835\udc5d), \u2026 , \ud835\udc65\ud835\udc5b(\ud835\udc5d) and the desired output \n\ud835\udc4c\ud835\udc51(\ud835\udc5d). \u2022 Calculate the actual output at iteration \ud835\udc5d =  1.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 37,
      "start_char": 14178,
      "end_char": 14639,
      "chunk_index": 29,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_30",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Calculate the actual output at iteration \ud835\udc5d =  1. \u2022 Where \ud835\udc5bis the number of perceptron inputs, and step is a step activation function. 37\n\nTraining a Perceptron\n\u2022 Step 3: Weight Training\n\u2022 Update the weights of the perceptron:\n\u2022 Where \u0394\ud835\udc64\ud835\udc56(\ud835\udc5d) is the weight correction at iteration \ud835\udc5d, computed by the delta rule:\n\u2022 Step 4: Iteration\n\u2022 Increase the iteration \ud835\udc5dby one, \n\u2022 go back to Step 2, and \n\u2022 repeat the process until the actual output converges\nwith the expected output.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 37,
      "start_char": 14589,
      "end_char": 15062,
      "chunk_index": 30,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_31",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "38\n\nLogical Operation Training\n\u2022 After completing the initialisation step (1), the perceptron is activated (2) by the \nsequence of four input patterns representing an epoch. \u2022 Perceptron weights are updated (3) after each activation. \u2022 Process iterates (4) until weights converge to stable values. 39\nXOR\nOR\nAND\nInput Variables\n\ud835\udc7f\ud835\udfcf\u2a01\ud835\udc7f\ud835\udfd0\n\ud835\udc7f\ud835\udfcf+ \ud835\udc7f\ud835\udfd0\n\ud835\udc7f\ud835\udfcf\u22c5\ud835\udc7f\ud835\udfd0\nX2\nX1\n0\n0\n0\n0\n0\n1\n1\n0\n1\n0\n1\n1\n0\n0\n1\n0\n1\n1\n1\n1\n\nPerceptron Training for Logical AND\n\u2022 Threshold: \u03b8 = 0.2.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 38,
      "start_char": 15062,
      "end_char": 15513,
      "chunk_index": 31,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_32",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Learning Rate: \u03b1= 0.1\n40\nFinal \nWeights\nError\nActual \nOutput\nInitial \nWeights\nDesired \nOutput\nInputs\nEpoch\nw2\nw1\ne\nY\nw2\nw1\nYd\nX2\nX1\n-0.1\n0.3\n0\n0\n-0.1\n0.3\n0\n0\n0\n1\n-0.1\n0.3\n0\n0\n-0.1\n0.3\n0\n1\n0\n-0.1\n0.2\n-1\n1\n-0.1\n0.3\n0\n0\n1\n0.0\n0.3\n1\n0\n-0.1\n0.2\n1\n1\n1\n0.0\n0.3\n0\n0\n0.0\n0.3\n0\n0\n0\n2\n0.0\n0.3\n0\n0\n0.0\n0.3\n0\n1\n0\n0.0\n0.2\n-1\n1\n0.0\n0.3\n0\n0\n1\n0.0\n0.2\n0\n1\n0.0\n0.2\n1\n1\n1\n\n41\nFinal \nWeights\nError\nActual \nOutput\nInitial \nWeights\nDesired \nOutput\nInputs\nEpoch\nw2\nw1\ne\nY\nw2\nw1\nYd\nX2\nX1\n0.0\n0.2\n0\n0\n0.0\n0.2\n0\n0\n0\n3\n0.0\n0.2\n0\n0\n0.0\n0.2\n0\n1\n0\n0.0\n0.1\n-1\n1\n0.0\n0.2\n0\n0\n1\n0.0\n0.2\n1\n0\n0.0\n0.1\n1\n1\n1\n0.1\n0.2\n0\n0\n0.1\n0.2\n0\n0\n0\n4\n0.1\n0.2\n0\n0\n0.1\n0.2\n0\n1\n0\n0.1\n0.1\n-1\n1\n0.1\n0.2\n0\n0\n1\n0.1\n0.1\n0\n1\n0.1\n0.1\n1\n1\n1\n0.1\n0.1\n0\n0\n0.1\n0.1\n0\n0\n0\n5\n0.1\n0.1\n0\n0\n0.1\n0.1\n0\n1\n0\n0.1\n0.1\n0\n0\n0.1\n0.1\n0\n0\n1\n0.1\n0.1\n0\n1\n0.1\n0.1\n1\n1\n1\nPerceptron Training for Logical AND\n\nPerceptrons & Linear Separability\n\u2022 A similar approach can be applied to train a perceptron to learn a logical OR.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 40,
      "start_char": 15513,
      "end_char": 16448,
      "chunk_index": 32,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_33",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 But, a single-layer perceptron cannot be trained to learn XOR. \u2022 Black dots indicate where Y=1. White dots indicate Y=0. \u2022 Can draw a line in AND and OR to separate black/white dots. XOR is not linearly separable. \u2022 A perceptron can only learn a function that is linearly separable. 42\n0.1\ud835\udc65\u0b35+ 0.1\ud835\udc65\u0b36= 0.2\n=\n\ud835\udc65\u0b35+ \ud835\udc65\u0b36= 2\n\nPerceptrons & Linear Separability\n\u2022 The linear separable limitation of a perceptron \nis derived directly from its step function. \u2022 Actual output \ud835\udc4c= 1 only if the total weighted input \ud835\udc4b \u2265 \ud835\udf03.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 42,
      "start_char": 16448,
      "end_char": 16956,
      "chunk_index": 33,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_34",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Implies that the entire input space is divided into two along \na boundary defined by \ud835\udc4b =  \ud835\udf03. \u2022 E.g. the separating line for AND is defined by \ud835\udc651\ud835\udc641 + \ud835\udc652\ud835\udc642 = \ud835\udf03. \u2022 Adding weights from our table gives 0.1\ud835\udc651 + 0.1\ud835\udc652 =  0.2 or \ud835\udc651 + \ud835\udc652 =  2. Thus:\n\u2022 The region below the line (where \ud835\udc4c= 0) is \ud835\udc651 + \ud835\udc652 \u22122 <  0 and \n\u2022 The region above the line (where \ud835\udc4c= 1) is \ud835\udc651 + \ud835\udc652 \u22122 \u2265 0. \u2022 Means that a single-layer perceptron can only classify linearly separable tasks.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 43,
      "start_char": 16956,
      "end_char": 17406,
      "chunk_index": 34,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_35",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Q: How can a perceptron be trained to learn XOR and other functions? \u2022 A: Use a multi-layer neural network, where each layer performs some operation \n(activation function) using the outputs (Y) or the layer before it. 43\n\nPerceptrons & Threshold vs.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 43,
      "start_char": 17406,
      "end_char": 17657,
      "chunk_index": 35,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_36",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "43\n\nPerceptrons & Threshold vs. Bias\n\u2022 These are equivalent to each other\n\u2022 \ud835\udc64\u0b35\ud835\udc65\u0b35+ \ud835\udc64\u0b36\ud835\udc65\u0b36+ \u22ef+ \ud835\udc64\u0be1\ud835\udc65\u0be1= \u03b8\n\u2022 Making \u03b8 = 0 (fixed)\n\u2022 Renaming \u03b8 as bias (b)\n\u2022 Training b as another weight\n\u2022 \ud835\udc64\u0b35\ud835\udc65\u0b35+ \ud835\udc64\u0b36\ud835\udc65\u0b36+ \u22ef+ \ud835\udc64\u0be1\ud835\udc65\u0b37\u2212\u03b8 = 0\n\u2022 \ud835\udc64\u0b35\ud835\udc65\u0b35+ \ud835\udc64\u0b36\ud835\udc65\u0b36+ \u22ef+ \ud835\udc64\u0be1\ud835\udc65\u0b37\u2212\ud835\udfcf\u22c5\u03b8 = 0\n\u2022 \ud835\udc64\u0b35\ud835\udc65\u0b35+ \ud835\udc64\u0b36\ud835\udc65\u0b36+ \u22ef+ \ud835\udc64\u0be1\ud835\udc65\u0b37\u2212\ud835\udfcf\u22c5\ud835\udc4f= 0\n44\nXb=-1\nBias (b)\n0.1\n\ud835\udf03= 0.0\n\nPerceptron Training for XOR\n\u2022 A two-layer perceptron can separate a non-linearly separable function.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 43,
      "start_char": 17626,
      "end_char": 18021,
      "chunk_index": 36,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_37",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The bias (Xb) ensures correct operation when the activation threshold is 0\n45\nThe two hidden perceptrons correspond to two hyperplanes. The final combining perceptron is equivalent to a logical AND on the output of \nthe two hidden perceptrons. Analogous to picking out the (x1, x2) coords where the two hidden perceptrons\nactivate together. This example uses a step function. MLP usually use a sigmoidal function.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 45,
      "start_char": 18021,
      "end_char": 18436,
      "chunk_index": 37,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_38",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "MLP usually use a sigmoidal function. Y\nPerceptron 3\nPerceptron 2\nPerceptron 1\nYd\nX2\nX1\n0\nY = (1*1) + (0*1) + (-1.5) = -0.5 \u21d20\nY = (0*1) + (0*1) + (-0.5) = -0.5 \u21d20\nY = (0*-1) + (0*-1) + (1.5) = 1.5 \u21d21\n0\n0\n0\n1\nY = (1*1) + (1*1) + (-1.5) = 0.5 \u21d21\nY = (0*1) + (1*1) + (-0.5) = 0.5 \u21d21\nY = (0*-1) + (1*-1) + (1.5) = 0.5 \u21d21\n1\n1\n0\n1\nY = (1*1) + (0*1) + (-1.5) = 0.5 \u21d21\nY = (1*1) + (0*1) + (-0.5) = 0.5 \u21d21\nY = (1*-1) + (0*-1) + (1.5) = 0.5 \u21d21\n1\n0\n1\n0\nY = (0*1) + (1*1) + (-1.5) = -0.5 \u21d20\nY = (1*1) + (1*1) + (-0.5) = 1.5 \u21d21\nY = (1*-1) + (1*-1) + (1.5) = -0.5 \u21d20\n0\n1\n1\n\nPerceptron Training for XOR\n46\n\ud835\udc65\u0b36= \u2212\ud835\udc65\u0b35+ 1.5\n0\n0\n1\n1\n\nBackpropagation\n\u2022 Commercial ANNs usually have 3-4 layers.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 45,
      "start_char": 18399,
      "end_char": 19071,
      "chunk_index": 38,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_39",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Each layer increases the \ncomputational burden exponentially\n\u2022 A backpropagation network is fully connected. Every neuron in each layer is connected to \nevery other neuron in the next layer. \u2022 Backpropagation NN originally proposed by Bryson & Ho (1969). \u2022 Not enough processing power to implement. Rediscovered in the 1980s. \u2022 Learning in a multi-layer NN is similar to perceptron learning. \u2022 Training input set is processed and an output pattern computed.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 47,
      "start_char": 19071,
      "end_char": 19528,
      "chunk_index": 39,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_40",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 NN propagates the input pattern forward from layer to layer until an output pattern is generated by the \noutput layer. \u2022 If \ud835\udc4c(\ud835\udc4b) is different to \ud835\udc4c\ud835\udc51(\ud835\udc4b), the error is propagated backwards through the network from \noutput to input layers. Weights are modified as the error is propagated backwards. \u2022 Neuron output determined using Rosenblatt\u2019s net weighted input. \u2022 But a sigmoidal activation function is used: \ud835\udc4c= 1/(1 + \ud835\udc52\u0bd1) \u2192[0, 1]. \u2022 Real numbers are propagated forward and errors backwards.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 47,
      "start_char": 19528,
      "end_char": 20020,
      "chunk_index": 40,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_41",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "47\n\nBackpropagation Algorithm\n\u2022 A backpropagation network typically has 3-4 fully connected layers that use a \nsigmoidal derivative. \u2022 Recap: The derivative of a function \ud835\udc66 =  \ud835\udc53(\ud835\udc65) of a variable \ud835\udc65is a measure of the rate at \nwhich the value \ud835\udc66of the function changes w.r.t the change of the variable \ud835\udc65, i.e. its slope. \u2022 The algorithm has the following key steps:\n48\nThe sum of the squared errors is \noften used to indicate if the actual \nvs. expected outputs have \nconverged.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 47,
      "start_char": 20020,
      "end_char": 20495,
      "chunk_index": 41,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_42",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "expected outputs have \nconverged. Backpropagation\n\u2022 How can the weights in the hidden layer be adjusted if more than one node \ncontributes to each \ud835\udc4c\ud835\udc58and \ud835\udc52in the output layer? \u2022 Can\u2019t split the error evenly across all hidden nodes (linked to an output node) as different \nweights in the hidden layer contribute with different amounts to both \ud835\udc4c\ud835\udc58and \ud835\udc52.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 48,
      "start_char": 20462,
      "end_char": 20811,
      "chunk_index": 42,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_43",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Solution is to backpropagate more of the error to nodes with greater weights, as \nthey contributed more to the error:\n49\nNote that output nodes are independent of each other w.r.t. \ud835\udc4c\ud835\udc51and \n\ud835\udc52; therefore back propagation can be executed independently from \neach output node. The weights serve two functions:\n1)\nTo propagate signals forward from the input to the output \nlayer. 2)\nTo propagate errors backwards through the network.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 49,
      "start_char": 20811,
      "end_char": 21240,
      "chunk_index": 43,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_44",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Backpropagation - Gradient Descent\n\u2022 The n weights in a NN can be considered as points in an n-dimensional space, \nwith an additional dimension for the observed error. \u2022 A gradient descent algorithm can be used to minimise the error by finding the lowest point \non the error surface. 50\n\nBackpropagation\n\u2022 Input signals x1, x2, \u2026, xn propagated L\uf0e8R and\u2026 \n\u2022 Error signals, e1, e2, \u2026, en propagated  L\uf0e7R\n51\nA three-layer back-propagation neural network.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 49,
      "start_char": 21240,
      "end_char": 21691,
      "chunk_index": 44,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_45",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Indices i, j, k refer to neurons in the input, \nhidden and output layers. wij denotes the weight for the connection \nbetween neuron i in the input layer and neuron \nj in the hidden layer. The training data cannot yield the expected \noutputYd for the hidden-layer neurons. Backpropagation in 4 Steps\n\u2022 Step 1: Initialisation\n\u2022 The weights and threshold levels (biases) are assigned random numbers within a range, \ntypically [-0.5, 0.5].",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 51,
      "start_char": 21691,
      "end_char": 22126,
      "chunk_index": 45,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_46",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The data set should be normalised within the bounds required for the activation function. \u2022 Step 2: Activation\n\u2022 Forward propagate the set of inputs \ud835\udc4b1(\ud835\udc5d), \ud835\udc4b2(\ud835\udc5d), \u2026 , \ud835\udc4b\ud835\udc5b(\ud835\udc5d) through the NN. a. Calculate the outputs of the neurons in the hidden layer as:\nwhere \ud835\udc5bis the number of inputs of neuron \ud835\udc57in the hidden layer. b. Calculate the actual output of the neurons in the output layer as:\nwhere \ud835\udc5ais the number of inputs of neuron \ud835\udc58in the output layer.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 52,
      "start_char": 22126,
      "end_char": 22576,
      "chunk_index": 46,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_47",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "52\n\nBackpropagation in 4 Steps\n\u2022 Step 3: Weight Training\n\u2022 Update the weights by backpropagating the errors in the output layer. a. Calculate the error gradient for the output layer neurons:\nwhere\nCalculate the weight correction:\nUpdate the weights at the \noutput layer:\n53\n\nBackpropagation in 4 Steps\nb. Calculate the error gradient for the hidden layer neurons.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 52,
      "start_char": 22576,
      "end_char": 22939,
      "chunk_index": 47,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_48",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Calculate the weight corrections:\nUpdate the weights at the hidden neurons:\n\u2022 Step 4: Iteration\n\u2022 Increase the iteration p by one and go back to Step 2 and iterate the process until the error \nrate converges. 54\n\nLoss Functions\n\u2022 A Loss Function:1\n\u2022 Compares expected values from the training set with predicted values from the NN. \u2022 Residual = (actual_value \u2013 predicted_value); represents the error in each prediction. \u2022 Measures how well an NN models the training data.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 54,
      "start_char": 22939,
      "end_char": 23410,
      "chunk_index": 48,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_49",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Guides the weights\u2019 optimisation process during training\n\u2022 Common Loss Functions:\n55\nBest Use Case\nFormula\nLoss Function\nClassification problems\nNegative_sum_of (log_of_predicted_value \u00d7\nexpected_value)\nCross Entropy Loss2\n- Softmax is applied to the output values\n- Poor predictions result in higher log errors\nGeneral use, better for regression\nSum of squares of residuals in the output layer\nSum of Squared Errors\nLess sensitive to outliers\nMean of absolute differences between residuals\nMean Absolute Error (L1)\nRegression, penalises larger errors \nmore heavily\nMean of squared differences between residuals\nMean Squared Error (L2)\n1Hastie, T., Tibshirani, R., & Friedman, J.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 55,
      "start_char": 23410,
      "end_char": 24091,
      "chunk_index": 49,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_50",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "(2009). The elements of statistical learning : Data mining, inference, and prediction, second edition. Springer New York. 2StatQuest with Josh Starmer: Neural Networks Part 6: Cross Entropy: https://youtu.be/6ArSys5qHAU?si=J-FwVd2m1KNBwMAb\n\nAccelerated Learning\n\u2022 Changing the value of the learning rate \ud835\udf36is one of the most \neffective ways of accelerating convergence. \u2022 A small \ud835\udf36will result in smaller changes to the weights, a slower rate of learning and a smooth\nlearning curve.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 55,
      "start_char": 24091,
      "end_char": 24572,
      "chunk_index": 50,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_51",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Large \ud835\udf36values will accelerate learning but may induce instability to the NN and cause oscillations \nand abrupt changes to outputs. \u2022 A multilayer network usually learns faster when a hyperbolic tangent activation \nfunction is used. \u2022 Where \ud835\udc82and \ud835\udc83are constants with typical values of 1.716 and 0.667. \u2022 A common technique is to add an extra momentum parameter \ud835\udec3\nto the delta rule as follows, to accelerate the gradient descent:\n\u2022 Where 0 \u2264\ud835\udec3< 1. A \ud835\udec3value of 0.95 is normally used for momentum.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 56,
      "start_char": 24572,
      "end_char": 25065,
      "chunk_index": 51,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_52",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "A \ud835\udec3value of 0.95 is normally used for momentum. 56\n\nAccelerated Learning\n57\n\u201cLooks ahead\" by \ncomputing the \ngradient at the \nposition one would \nreach after the \nmomentum step\nAdapts \ud835\udf36per parameter. Parameters \nthat have historically received large \ngradients get a smaller effective \nlearning rate, and vice versa. Accelerated Learning\nDept.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 56,
      "start_char": 25018,
      "end_char": 25361,
      "chunk_index": 52,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_53",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Accelerated Learning\nDept. of Computer Science & Applied \nPhysics \u2013 Artificial Intelligence\n58\n\nAccelerated Learning\n\u2022 The following heuristics can be applied to change \ud835\udf36and accelerate learning \nwithout inducing instability:\n1. Heuristic 1: If the change in the sum of the squared errors has the same sign for several \nepochs, \ud835\udf36should be increased, typically by multiplying by 1.05. 2.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 57,
      "start_char": 25335,
      "end_char": 25720,
      "chunk_index": 53,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_54",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "2. Heuristic 2: If the algebraic sign of the sum of the squared errors alternates for several \nconsecutive epochs, \ud835\udf36should be decreased by multiplying by 0.7. \u2022 The overall effect of applying these heuristics will be to avoid \u201cbumps\u201d in the \nlearning curve, where weight adjustments cause oscillations in the level of error. Dr. John Healy, Dept.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 59,
      "start_char": 25718,
      "end_char": 26064,
      "chunk_index": 54,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_55",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Dr. John Healy, Dept. of Computer Science \n& Applied Physics \u2013 Artificial Intelligence\n59\n\nPattern Matching Learning Curve\n60\n\nMultilayer Neural Networks\n\u2022 A feed-forward NN with one or more hidden \nlayers. \u2022 An input layer of source neurons, \u2026\n\u2022 \u2026 a hidden layer (middle) of computational neurons \nand\u2026  \n\u2022 \u2026 an output layer of computational neurons. \u2022 Input signals are propagated forward on a layer-by-\nlayer basis. \u2022 The hidden layer allows the NN to learn any \ncontinuous function (universality).",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 59,
      "start_char": 26043,
      "end_char": 26544,
      "chunk_index": 55,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_56",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "61\n\nMultilayer Neural Networks\n\u2022 Each layer in a multi-layer neural network has its own function. \u2022 Input layer accepts signals from the external environment and \nredistributes them to all the neurons in the hidden layer. \u2022 Rarely includes computing neurons, i.e., does not process input patterns. \u2022 Output layer accepts the output signals from the hidden layer and establishes \nthe output signals for the entire network. \u2022 Neurons in the hidden layer detect features.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 61,
      "start_char": 26544,
      "end_char": 27012,
      "chunk_index": 56,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_57",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Neurons in the hidden layer detect features. Neuron weights represent features hidden in \ninput patterns. \u2022 Features are used by the output layer to determine the output pattern. \u2022 Hidden layer hides its desired output \ud835\udc4c\ud835\udc51(\ud835\udc5d). \u2022 One hidden layer can learn any continuous function. \u2022 Hidden layer\u2019s neurons cannot be observed through I/O behaviour. No obvious way to \nknow what the desired output of the hidden layer should be.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 62,
      "start_char": 26966,
      "end_char": 27393,
      "chunk_index": 57,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_58",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Desired output of hidden layer defined by the layer itself as its weights adjust to conform the actual \noutput of the NN to the expected output. 62\n\nNumber of Layers & Nodes\n\u2022 Creating an NN architecture means determining the number of hidden layers \nand the number of nodes in all layers (input, hidden, and output)\n\u2022 Until the advent of deep learning, NNs usually had a single hidden layer. \u2022 Input Layer (x1)\n\u2022 The number of neurons is completely determined by the shape of \nyour training data, i.e.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 62,
      "start_char": 27393,
      "end_char": 27897,
      "chunk_index": 58,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_59",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "is equal to the number of features (columns) \nin the data. \u2022 Some NN configurations add one additional node \nfor a bias term. \u2022 Output Layer (x1)\n\u2022 The number of neurons is determined by the chosen model \nconfiguration. \u2022 Classification: one node/class, e.g. OCR and image recognition. \u2022 Prediction (Regression): a single node, e.g. stock price. 63\n\nNumber of Layers & Nodes\n\u2022 Hidden Layers (0, 1, 2 or a Deep Neural Network)\n\u2022 0: Only capable of representing linearly separable functions.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 63,
      "start_char": 27897,
      "end_char": 28386,
      "chunk_index": 59,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_60",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Quite limiting for most real-world problems\n\u2022 Remember Perceptron. \u2022 1: Can approximate any function that contains a continuous mapping from one finite space \nto another (given enough neurons). \u2022 i.e., can learn complex, non-linear relationships in the data. \u2022 The Universal Approximation Theorem\n\u2022 Can get arbitrarily close to any continuous function. \u2022 But might require an impractically large number of neurons for very complex functions. \u2022 It\u2019s like drawing a complex curve with a series of short lines.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 64,
      "start_char": 28386,
      "end_char": 28895,
      "chunk_index": 60,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_61",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 With enough lines, you can get very close to the true curve, but you might need \na huge number of them for high accuracy. 64\n\nNumber of Layers & Nodes\n\u2022 Hidden Layers (0, 1, 2 or a Deep Neural Network)\n\u2022 2: Can represent an arbitrary decision boundary to any arbitrary accuracy. \u2022 Often with fewer total neurons than a single hidden layer would require for the same task. \u2022 Allows the network to learn hierarchical features, e.g., image recognition:\n\u2022 First layer might detect edges.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 64,
      "start_char": 28895,
      "end_char": 29380,
      "chunk_index": 61,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_62",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The second layer could recognise shapes formed by the edges. \u2022 More than 2: Deep NNs\n\u2022 Each additional layer allows the NN to learn\nincreasingly abstract and complex features \n(e.g. image/speech recognition, NLP). \u2022 However, deeper isn't always better. \u2022 The optimal number of layers depends on the specific problem, \nthe amount and nature of your data, and computational constraints. \u2022 Deeper NNs are more prone to overfitting and can be more challenging to train effectively.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 65,
      "start_char": 29380,
      "end_char": 29859,
      "chunk_index": 62,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_63",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "65\n\nNumber of Nodes in a Hidden Layer\n\u2022 Computing the correct number of nodes in a hidden layer is a black art. \u2022 Too many nodes cause overfitting* and result in a network that is difficult to train without \na very large data set. Nodes are \u201cstarved\u201d. \u2022 Underfitting results from too few neurons in the hidden layers to adequately detect the \nsignals in a complicated data set.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 65,
      "start_char": 29859,
      "end_char": 30236,
      "chunk_index": 63,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_64",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The following heuristic gives an upper bound on the number of hidden neurons \nin a single hidden-layer architecture:\n\u2022 Ni = #input_neurons, No = #output_neurons, Ns = number of samples in training data set, \nand alpha = an arbitrary scaling factor usually 2-10.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 66,
      "start_char": 30236,
      "end_char": 30499,
      "chunk_index": 64,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_65",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 These heuristics can also be used for single hidden layers:\n\u2022 \ud835\udc41\u210e <  2\ud835\udc41\u0bdc\n\u2022 \ud835\udc41\u210e=\n\ud835\udc41\u0bdc\ud835\udc41\u0be2Geometric Pyramid Rule\n66\n\ud835\udc41\u0bdb=\n100\n2(4 + 3) \u22487.1\nEx: Iris dataset:\n*https://youtube.com/shorts/ciLveEWIj5k?si=VCtA0ErEkphv7IY5\n\nNumber of Nodes in a Hidden Layer\n\u2022 Regularisation techniques \naddress overfitting, e.g.:\n\u2022 L2 (Weight Decay): Penalises\nlarge weights in the loss \nfunction\n\u2022 \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60= \ud835\udc40\ud835\udc46\ud835\udc38+ \ud835\udf06\u2211\ud835\udc64\u0b36\n\u2022 Dropout: Randomly disable \nneurons during training\n\u2022 Early Stopping: Stop training \nwhen validation error increases\n67\n\nDeep Learning & Feature Hierarchy\n\u2022 Deep-learning networks consist of a feature hierarchy (a multi-layer NN) with \nincreasing complexity and abstraction.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 66,
      "start_char": 30499,
      "end_char": 31160,
      "chunk_index": 65,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_66",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Can handle very large, high-dimensional data sets with billions of parameters that pass \nthrough nonlinear functions. \u2022 Can discover latent structures within unlabeled and unstructured data, i.e. pictures, texts, \nvideo, audio, etc. 68\n\nRegression: Learn an Equation\n69\n\ud835\udc32= \ud835\udfd0\ud835\udc31+ \ud835\udfd5\n\nRegression: Learn 2\ud835\udc4e3 + 3\ud835\udc4f2 + 2\ud835\udc50+ 7\n70\nOnly one output node needed for \nregression. Number of inputs depends \non features (variables). 71\nOnly 100 samples in the data set. Increasing the dataset size will improve \nprediction accuracy.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 68,
      "start_char": 31160,
      "end_char": 31676,
      "chunk_index": 66,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_67",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Regression: Learn 2\ud835\udc4e3 + 3\ud835\udc4f2 + 2\ud835\udc50+ 7\n\nPattern Matching Example\n\u2022 A common application of NNs is to analyse and classify an image. This raises a \nnumber of questions:\n\u2022 How can a NN learn what a picture is? What architecture should be used? \u2022 A NN can be viewed as consisting of nothing more than arrays (vectors and matrices), \nactivation functions and training algorithms. 72\n\u2022\nAn image is really an n-dimension array of pixels (RGBa). \u2022\nAn 8x8 string array can be used to represent an LED display.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 71,
      "start_char": 31676,
      "end_char": 32174,
      "chunk_index": 67,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_68",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022\nEach pixel (array element) is fed into the NN, requiring 64 nodes \nin the input layer. \u2022\nThe number of output nodes depends on the number of patterns \navailable, e.g. 0-9 will require 10 nodes in the output layer. \u2022\nThe output node with the highest value indicated the pattern  \nclassification. Pattern Matching Example\n73\n0.87\n0.15\n0.70\n0.69\n0.19\n0.70\n0.91\n0.66\n0.77\n0.53\n\nFeature Selection (Data \u201cShape\u201d)\n\u2022 The input to a NN is an array of real numbers called a feature vector.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 72,
      "start_char": 32174,
      "end_char": 32655,
      "chunk_index": 68,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_69",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The fixed-length vector is fed into the input layer of a neural network. \u2022 Each index in the array represents a single or composite feature and maps to the \ncorresponding input node of the NN. \u2022 Selecting the input features for an NN is domain-specific and depends also on \nthe data types used. Curse of dimensionality (e.g. #pixels). \u2022 Numeric values can map 1:1 or n:m to an array of features. \u2022 Images can be decomposed into their channels and represented as bytes. Vector size will \nbe width * height * #channels.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 74,
      "start_char": 32655,
      "end_char": 33174,
      "chunk_index": 69,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_70",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Vector size will \nbe width * height * #channels. \u2022 Greyscale: 1 channel with values in the range [0...255]. \u2022 RBGA: 4 channels with values in the range [0...255]. \u2022 Video, sound, and time series data can be sampled at fixed intervals. \u2022 Text can be converted to Unicode per character, n-gram or into a \ndifferent encoding scheme, e.g. PseACC Composition for proteins. \u2022 Meta-data, tags can also be used to reduce dimensionality. \u2022 Can also compress, e.g., LZW or any other lossless algorithm.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 74,
      "start_char": 33126,
      "end_char": 33618,
      "chunk_index": 70,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_71",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "74\n\nIncremental Feature Selection\n\u2022 We can apply the Generate & Test Principle to NN topologies. 1. Incrementally, dynamically generate i sets of feature vectors \nof size k using some encoding scheme. 2. For each feature vector of size k, generate an n-layer neural network with k input nodes. 3. Train and test the NN topology with 10-fold Cross-validation. Compute some measure of \naccuracy for the NN topology. 4. Store the NN model with the best score.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 74,
      "start_char": 33618,
      "end_char": 34074,
      "chunk_index": 71,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_72",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "4. Store the NN model with the best score. 75\n\ud835\udc56\n\ud835\udc58\n\ud835\udc58\ud835\udc58\ud835\udc58\n\ud835\udc58\nExample: time series\n\nFixed-size Input Vectors\n\u2022 A multi-layer perceptron has a fixed-size number of input neurons. \u2022 Perfect for many situations where the number of input signals never changes, e.g. inputs \nfrom a gaming controller or the sensors of a drone. \u2022 Problems with variable-length input like text, images, video, etc. \u2022 How can we convert variable-length input into fixed-length? \u2022 This is not just a NN issue but a more general computing issue.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 75,
      "start_char": 34032,
      "end_char": 34544,
      "chunk_index": 72,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_73",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Answer is to encode the input in some way (encode \u2260 encrypt). \u2022 Depends on the domain and what we\u2019re trying to achieve. \u2022 At an abstract level, we can view encoding as:\n\u2022 Transformation: variable input transformed into fixed output, e.g. Huffman encoding, \nvector hashing, k-mer spectrum, PseudoAA, scaling. \u2022 Useful for text and images. \u2022 Sampling: variable length input sampled at fixed size and rate. \u2022 Useful for video and sound.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 76,
      "start_char": 34544,
      "end_char": 34979,
      "chunk_index": 73,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_74",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Useful for video and sound. \u2022 These are also forms of compression and possibly encryption, but the intent is \ndifferent. We are not trying to pack or protect data. 76\n\nVector Hashing\n\u2022 Vector hashing helps eliminate the \u201ccurse of dimensionality\u201d problem. \u2022 Hash each feature to an input vector using hash(\ud835\udc53) mod \ud835\udc5a, where \ud835\udc5ais the size of the \ninput vector for the NN. \u2022 Converts variable-length input data into fixed-length input vectors. \u2022 Especially used for character sequences like text, DNA or proteins.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 76,
      "start_char": 34950,
      "end_char": 35459,
      "chunk_index": 74,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_75",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Proteins are composed of an alphabet of 20 amino acids (A, C, D, E, F, G, H, I, K, L, ..., W, Y). \u2022 Using n-gram representations gives 201=200, 202=400, 203=8,000, 204=160,000 and \n205=3,200,000 1-,2-,3-,4-, and 5-grams. \u2022 Vector hashing can \u201csquash\u201d all 1,2,3,4 and 5-grams into a single fixed-size vector as \nfollows:\n1. Create an array of size \ud835\udc5bas an input vector for a NN. 2. For each feature f in the data set \n1. Hash the item using a hash function \u210e(\ud835\udc53), e.g. Python\u2019s hash()\n2.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 77,
      "start_char": 35459,
      "end_char": 35945,
      "chunk_index": 75,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_76",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Python\u2019s hash()\n2. Compute the array index as index =  \u210e(\ud835\udc53) % \ud835\udc5a. 3. Increment the value at index. 77\nDefinition (k-mers): An n-gram is a contiguous \nsequence of n amino acids. E.g. For the protein \nsequence \u201cMISHW\u201d:\n\u2022\nUnigrams (n=1): Single amino acids \ne.g., M, I, S, H, W. \u2022\nBigrams (n=2): Pairs of amino acids\ne.g., MI, IS, SH, HW\n\u2022\nTrigrams (n=3): Triples of amino acids\ne.g., MIS, ISH, SHW\n\nVector Hashing\nA \u201ctoy\u201d example: a system to classify customer reviews as positive or negative.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 77,
      "start_char": 35927,
      "end_char": 36417,
      "chunk_index": 76,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_77",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Reviews:\n1.\u201cGreat product, love it!\u201d\n2.\u201cTerrible, don't buy\u201d\n3.\u201cOkay, but not great\u201d\n\u2022 Step 1: break the reviews into, e.g., 2-grams (pairs of words)\n1. \u201cGreat product\u201d, \u201cproduct love\u201d, \u201clove it\u201d\n2. \u201cTerrible don\u2019t\u201d, \u201cdon't buy\u201d\n3. \u201cOkay but\u201d, \u201cbut not\u201d, \u201cnot great\u201d\n\u2022 Step 2: set up the vector\n\u2022 E.g. input vector size of 10 \n\u2022 Would be typically much larger, but 10 makes the example easier to follow. 78\n\nVector Hashing\nExample: a system to classify customer reviews as positive or negative.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 77,
      "start_char": 36417,
      "end_char": 36913,
      "chunk_index": 77,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_78",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Step 3: hashing\n\u2022 E.g.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 78,
      "start_char": 36913,
      "end_char": 36937,
      "chunk_index": 78,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_79",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Step 3: hashing\n\u2022 E.g. add up the ASCII values of the characters and take the modulo with 10\n\u2022 \"Great product\" -> 1300 -> 1300 % 10 = 0\n\u2022 \"product love\" -> 1239 -> 1239 % 10 = 9\n\u2022 \"love it\" -> 691 -> 691 % 10 = 1\n\u2022 \"Terrible don't\" -> 1333 -> 1333 % 10 = 3\n\u2022 \"don't buy\" -> 844 -> 844 % 10 = 4\n\u2022 \"Okay but\" -> 767 -> 767 % 10 = 7\n\u2022 \"but not\" -> 700 -> 700 % 10 = 0\n\u2022 \"not great\" -> 900 -> 900 % 10 = 0\n\u2022 Step 4: create an input vector for each review\n\u2022 \"Great product, love it!\"\n[1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n\u2022 \"Terrible, don't buy\"\n[0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n\u2022 \"Okay, but not great\"\n[2, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n79\n\nVector Hashing\n\u2022 Hash the variable-sized inputs to a fixed-size vector and then map each of the \nvector indices to a node in the input layer of a neural network.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 78,
      "start_char": 36913,
      "end_char": 37700,
      "chunk_index": 79,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_80",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 When using text as a source, a common practice is to first decompose the text into a set of \nn-grams (k-mers) and then hash each of these. 80\n\nNN Performance Measures\n\u2022 Given the empirical nature of NN design, how can we know if our network design and \ntopology are good? \u2022 A NN that takes a very long time to train may indicate underfitting or overfitting. But it could also \nbe related to the number of hidden layers.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 80,
      "start_char": 37700,
      "end_char": 38121,
      "chunk_index": 80,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_81",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Training time doesn\u2019t tell us much\u2026\n\u2022 Could use overall error rate during testing: \ud835\udc52 = \ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59\n\u2044\n\u00d7 100. \u2022 Need to ensure that the training data and test data contain \nreasonable amounts of positive and negative test data. \u2022 Can also use 10-fold cross-validation and measure error. \u2022 Divide the dataset into 10 equal (or near-equal) folds. \u2022 Train on 9 folds (90%) and test on the remaining fold (10%). \u2022 Repeat for all 10 combinations, so every data point is tested exactly once.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 81,
      "start_char": 38121,
      "end_char": 38607,
      "chunk_index": 81,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_82",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Report the average (test) error across the 10 rounds. \u2022 Most common approach is to use 10-fold cross-validation along with statistical \nmeasures computed from a confusion matrix. \u2022 ROC (Receiver Operating Characteristic) curve and AUC (Area Under Curve). Commonly used \nstatistical measures. \u2022 MCC (Matthews Correlation Coefficient) value between -1 and +1. 81\ne.g. 5-fold\n\n\uf071A confusion matrix allows us to compute four statistical measures (TP, FP, TN, FN) that can be used to \ncompute other metrics.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 81,
      "start_char": 38607,
      "end_char": 39110,
      "chunk_index": 82,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_83",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\uf071Consider the following binary classifier for emails (spam/not spam). \uf071The confusion matrix can be summed for 10-fold cross-validation. \uf071An extra row and column are added for every additional classification, i.e. n classes will have a \nconfusion matrix of size n2. \uf071For any size n, the \u201ccorrect\u201d answer is represented by the green diagonal. Confusion Matrix\n82\n*NPV = Negative Predictive Value\n\nAUC/ROC Curve\n\u2022 Measures the performance of a classifier at different thresholds.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 82,
      "start_char": 39110,
      "end_char": 39586,
      "chunk_index": 83,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_84",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The Receiver Operating Characteristic (ROC) is a probability curve. The Area Under the \nCurve (AUC) represents the degree or measure of separability. \u2022 Shows how much an NN is capable of distinguishing between classes. \u2022 Measures Sensitivity (TPR) vs. 1 \u2013 Specificity (FPR). \u2022 Sensitivity (really is spam). Specificity (really is not spam).",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 82,
      "start_char": 39586,
      "end_char": 39928,
      "chunk_index": 84,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_85",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Specificity (really is not spam). \u2022 TPR (True Positive Rate) / Recall / Sensitivity = \ud835\udc47\ud835\udc43/(\ud835\udc47\ud835\udc43+ \ud835\udc39\ud835\udc41)\n\u2022 FPR (False Positive Rate) = 1 \u2013  Specificity = \ud835\udc39\ud835\udc43 + (\ud835\udc47\ud835\udc41+ \ud835\udc39\ud835\udc43)\no Sensitivity and Specificity are inversely proportional to each other. o An \u2191 in sensitivity will result in a \u2193 in specificity. \uf071The higher the AUC value, the better the NN model is at classification. 1. AUC = 1 \u2192 Good measure of separability. Classifies perfectly. 2. AUC = 0.5 \u2192 Model has no class separa\u019fon capacity. Toss a coin\u2026\n3.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 83,
      "start_char": 39895,
      "end_char": 40392,
      "chunk_index": 85,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_86",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Toss a coin\u2026\n3. AUC = 0 \u2192 Reciprocates the correct result\u2026 completely wrong. \uf071The ROC curve measures one classification against all the others\u2026\n\uf071A n class NN will have n different AUC/ROC curves. 83\n\nAUC/ROC Curve\n84\nFPR = [0.0, 0.0, 0.5, 0.5, 1.0]\nTPR = [0.0, 0.5, 0.5, 1.0, 1.0]\n\nMatthews Correlation Coefficient\n\u2022 Sensitivity and specificity have some limitations as measures. \u2022 By classifying everything as class A, a NN can achieve 100% sensitivity!",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 83,
      "start_char": 40377,
      "end_char": 40831,
      "chunk_index": 86,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_87",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Similarly, by rejecting everything, a NN can achieve 100% specificity! \u2022 Can use the MCC as an alternative performance measure of a NN. \u2022 Also takes into account true and false positives and negatives. \u2022 Range [-1,1] where -1 \u2192 completely wrong binary classifier and +1 \u2192 completely correct\nbinary classifier. \u2022 Can be computed from the confusion matrix. \u2022 MCC is regarded as a more balanced measure where there is a significant \ndiscrepancy between the cardinality of each class in a dataset.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 85,
      "start_char": 40831,
      "end_char": 41326,
      "chunk_index": 87,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_88",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "85\n\nRadial Basis Function Networks\n\u2022 An RBF network is a three-layer NN used for classification, \nfunction approximation, time-series prediction, and system control. \u2022 A radial basis function (RBF) is a real-valued function whose value depends only on its \ndistance from a centre point, i.e. \ud835\udf19\ud835\udc65= \ud835\udf19( \ud835\udc65\u2212\ud835\udc50\u0bdc). \u2022 Typically computed using Euclidean Distance, but others (Hamming, Manhattan, Cosine \ndistance) can also be used. \u2022 Every RBF neuron stores a prototype (e.g., a training input vector).",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 85,
      "start_char": 41326,
      "end_char": 41817,
      "chunk_index": 88,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_89",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 For live data (inference), each RBF neuron computes \ud835\udf19( \ud835\udc65\u2212\ud835\udc50\u0bdc) between the input \nvector and its prototype and maps it to a Gaussian distribution (bell curve). \u2022 The weighted sum is computed by the output layer only. \u2022 The RBF neurons do not apply weights, and the output layer does not have an activation \nfunction. \u2022 The architecture of RBF networks allows them to be used with fuzzy rules and \ninference. \u2022 These types of NN are called neuro-fuzzy networks, aka hybrid intelligent systems.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 86,
      "start_char": 41817,
      "end_char": 42309,
      "chunk_index": 89,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_90",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "86\n\nRadial Basis Function Networks\n\u2022 E.g., Iris dataset\n\u2022 150 samples (dataset rows)\n\u2022 Net could have 150 hidden neurons, but\u2026\n\u2022 prone to overfitting\n\u2022 too long to train\n\u2022 Better example: 10 samples/class\n\u2022 3 \u00d7 10 = 30 RBF neurons\n\u2022 30 prototypes from k-means clustering\n\u2022 RBF prototype [0.22, 0.62, 0.06, 0.04]\nInput                 [0.24, 0.61, 0.07, 0.03]\n\u2022 Typically setosa\n\u2022 RBF output ~ 1.0\n87\n\nRBF Network Architecture\n88\n\u2022 A positive weight is given to the edge between an RBF node and its \ncorresponding output node.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 86,
      "start_char": 42309,
      "end_char": 42834,
      "chunk_index": 90,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_91",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Otherwise, a negative weight is used. xk\nk training samples\nn outputs\n(e.g., n classes)\n\nRBF Network Architecture\n\u2022 The input layer of an RBF network consists of an \nn-dimensional vector of data to classify. \u2022 Normalisation should be applied to reduce the data \nrange to a suitable universe of discourse. \u2022 The hidden layer consists of a set of RBF \nneurons. \u2022 The number of neurons in the hidden layer should \nbe a subset of the training examples selected as a \nkernel set.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 88,
      "start_char": 42834,
      "end_char": 43310,
      "chunk_index": 91,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_92",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 Each neuron compares the input vector to its \nprototype (kernel instance) and outputs a similarity \nmeasure value in the range [0, 1]. 89\nNote that as the distance from \nthe mean increases, the \noutput decreases\nexponentially towards 0. RBF Network Architecture\n\u2022 The output layer of an RBF network consists \nof one node for each classification type. \u2022 The classification is typically given by the \noutput node with the highest score (weighted sum) \nof all RBF neurons.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 89,
      "start_char": 43310,
      "end_char": 43781,
      "chunk_index": 92,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_93",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "\u2022 The value of each output node is thus:\n\u2022 where \ud835\udc41is the number of hidden neurons, \ud835\udc29\ud835\udc56is the prototype vector for neuron \ud835\udc56, and \ud835\udc64\ud835\udc56is the weight \nfrom neuron \ud835\udc56to the output neuron. The \ndenotes Euclidean Distance. \u2022 All hidden neurons are linked to all output neurons. \u2022 Every RBF neuron has some influence over the classification decision. \u2022 But\u2026 the exponential fall off means that neurons with prototypes that are far from the \ninput vector contribute little to the overall result.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 90,
      "start_char": 43781,
      "end_char": 44263,
      "chunk_index": 93,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_94",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "90\nRead as \u201cRBF of x is the \nweighted sum of the Euclidean \nDistance between x and the \nprototype vector for each node\u201d. Neuron \ud835\udc56\n\ud835\udc64\u0bdc\n\nRBF Activation Function\n\u2022 Remember: each RBF neuron computes the degree of similarity between the input and \nits prototype vector. \u2022 The higher the similarity, the closer the activation is to 1. \u2022 Usually computed using a Gaussian function:\n91\n\uf071In the one-dimensional function above, the \nelements in red control the height of the curve.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 90,
      "start_char": 44263,
      "end_char": 44734,
      "chunk_index": 94,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_95",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "These are made redundant by the weights in the \noutput layer. \uf071The elements in blue control the width of the \ncurve. These are replaced by the variable \ud835\udf37in the \nactivation function. RBF Activation Function\n92\n\uf071The prototypes to use the value of \u03b2 are required before training. \u2013 Prototype Selection: options available include:\n1. One RBF Neuron for each training sample. \u2013 But more RBF neurons increase space and time complexity. 2. Randomly select k prototypes from the data set. 3.",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 91,
      "start_char": 44734,
      "end_char": 45217,
      "chunk_index": 95,
      "total_chunks": 97
    },
    {
      "chunk_id": "6e42b938e45780146836ab95047c2f94_chunk_96",
      "doc_id": "6e42b938e45780146836ab95047c2f94",
      "content": "Randomly select k prototypes from the data set. 3. Perform a k-means clustering on the data set and use the cluster centroids as the prototypes. \u2013\nThe value of \u03b2: beta controls the width of the curve \nand is a measure of the confidence of a result. 1. Can be computed from k-means by setting the \nstandard deviation \ud835\udf0eto be the average distance\nbetween all points in the cluster and the cluster \ncentre. 2. \u03b2 can then be computed as: \ud835\udefd=\n\u0b35\n\u0b36\u0c19\u0c2e",
      "source_doc_title": "Microsoft PowerPoint - ai_NeuralNetworks.pptx",
      "section_id": null,
      "page_number": 92,
      "start_char": 45167,
      "end_char": 45608,
      "chunk_index": 96,
      "total_chunks": 97
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_0",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "Advanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus. 1 \n \n \nComposition and Object Reuse \n \nOne of the principal goals of the object-oriented paradigm is to promote the reuse of software \nartefacts within an application and between applications. There are two basic mechanisms for \nachieving reuse in the paradigm \u2013 generalisation and composition.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 399,
      "chunk_index": 0,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_1",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "The former relates to the \nreuse of state and behaviour in concrete classes that are derived from a more abstract entity and \nimplicitly supports polymorphism. Composition relates to the construction of new types from \na collection of existing classes and represents the primary mechanism for reuse in the object-\noriented paradigm.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 399,
      "end_char": 731,
      "chunk_index": 1,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_2",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "While it is usually not cited as a \u201cpillar\u201d of the object-oriented paradigm, object reuse through \ncomposition is predicated on the proper application of abstraction and encapsulation. Indeed, \ncomposition promotes reuse precisely through the encapsulation of reusable behaviour in an \nabstract entity.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 731,
      "end_char": 1033,
      "chunk_index": 2,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_3",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "Moreover, although inheritance and polymorphism are immediately identifiable \nas key pillars in the object-oriented paradigm, these concepts are also a consequence of the \napplication of abstraction and encapsulation. The whole point of encapsulation is to separate \nthe stable and volatile parts of an application into abstract and concrete types respectively.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 1033,
      "end_char": 1394,
      "chunk_index": 3,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_4",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "The \nextracted abstract components are highly reusable as their generality confers upon them the \nflexibility to be used in a variety of different contexts. When not part of a clear hierarchy of \nresponsibility, such abstract entities should be reused through composition. When used \ncorrectly, composition enables behaviours and responsibilities to be dynamically assigned to \nan object at run-time. Implementation inheritance assigns behaviours and responsibilities \nstatically at compile time.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 1394,
      "end_char": 1890,
      "chunk_index": 4,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_5",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "Composition and Scope \nThere exist a number of different forms of composition that can be used in an application and \neach form is best categorised on the basis of the scope of the relationship between the container \nobject and the objects that it is composed with. Consider the following UML diagram that \nillustrates four forms of composition: \n \n \n \n \nThe diamonds denote a strong form of composition and require that the containing object \nmaintain an instance variable of that type.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 1890,
      "end_char": 2377,
      "chunk_index": 5,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_6",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "For example, the type C should have instance \nvariables of type A and E, defined at a class level. The full line represents an association \nbetween class C and class D, where the classes interact with another class at a method level. The dashed line denotes a dependency, the weakest form of composition. When an object has \n\nAdvanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 1,
      "start_char": 2377,
      "end_char": 2807,
      "chunk_index": 6,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_7",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "2 \ngone out of scope, the JVM will invoke the inherited finalize() method before the garbage \ncollector removes the instance from the heap. The finalize() method can thus be used to \ndetermine the scope of a composed object and the exact form of composition it manifests. There are four basic forms of composition: \n \n1. Dependency (Dashed Line with Arrow): the scope of composition is restricted to the \nimplementation details of a method.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 2807,
      "end_char": 3247,
      "chunk_index": 7,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_8",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "The dependency is not visible or accessible outside \nthe containing class in any way. The composed object is normally fully encapsulated, \nbut may also be shared if singletons are being used. 2. Association (Fill Line with Arrow):  The scope of a composed object is outside of the \nclass itself, as the composed object is either passed in as a method argument or returned \nas a type from the method. Either way, the method invocator will have a reference to \nthe composed object at that point in time. 3.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 3247,
      "end_char": 3751,
      "chunk_index": 8,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_9",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "3. Aggregation (White Diamond with Arrow): The composed object is declared at a \nclass level, i.e. has the scope of an instance variable. An external call may have direct \naccess to the composed object if it passed the object to the constructor of a container \nclass. Indirect access to the composed object can arise from returning a reference to a \nmutable instance variable from an accessor method.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 3749,
      "end_char": 4149,
      "chunk_index": 9,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_10",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "In an aggregation relationship, the \ncomposed object may be referenced by some other object and thus may have a scope \ngreater than its container. Consequently, if the container goes out of scope and is \ngarbage collected, the composed object may outlive its container. 4. Full Composition  (Black Diamond with Arrow): The composed object is \ncompletely encapsulated inside the containing class, with no possibility of direct or \nindirect access.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 4149,
      "end_char": 4595,
      "chunk_index": 10,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_11",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "The scope of the composed object is thus restricted to the class using \nit. Consequently, when the containing class goes out of scope and is garbage collected, \nthe composed class is guaranteed to be garbage collected with it. Note that composition and delegation go hand-in-glove. The whole point of composing objects \nis to reuse their behaviour in another class. That reuse of behaviour relates to the invocation of \nthe methods exposed by the composed class.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 4595,
      "end_char": 5057,
      "chunk_index": 11,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_12",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "Consequently, the methods of the containing class \nshould delegate tasks to composed objects where possible. It is important to realise that, using \ncomposition, a class can be capable of doing many different things without violating the Single \nResponsibility Principle.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 5057,
      "end_char": 5328,
      "chunk_index": 12,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_13",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "Finalise \n \n \nExercises \nIn this practical, we will employ composition to reuse the cryptographic capability already \nprovided in the Java SDK and explore how composition can be combined with abstraction \nand encapsulation to create cohesive and loosely coupled designs.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 5328,
      "end_char": 5598,
      "chunk_index": 13,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_14",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "\u2022 \nUsing Eclipse create a new class called RSACypher, composed with the following \ninstance variables imported from the java.security an javax.crypto APIs:  \no Cipher cypher  \no KeyPair keyRing \n \nOverride the inherited method finalize() to output the name of the class and its Object \nID. Advanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 2,
      "start_char": 5598,
      "end_char": 5992,
      "chunk_index": 14,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_15",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "3 \n\u2022 \nIn the constructor of RSACypher, implement the functionality to initialise both \ninstance variables as follows: \n \nKeyPairGenerator keyGen = KeyPairGenerator.getInstance(\"RSA\"); \nkeyGen.initialize(2048); \nkeyRing = keyGen.generateKeyPair(); \ncypher = Cipher.getInstance(\"RSA/ECB/PKCS1Padding\"); \n \nExplain the scope of the three variables defined and the form of composition for \neach.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 5992,
      "end_char": 6383,
      "chunk_index": 15,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_16",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "\u2022 \nDeclare the following two methods for encryption and decryption: \n \npublic byte[] encrypt(byte[] plainText) throws Throwable{ \npublic byte[] decrypt(byte[] cypherText) throws Throwable{ \n \nImplement each method by calling the init() method of cypher with the appropriate \nparameters. Use the key returned by keyRing.getPublic() and keyRing.getPublic() for \nencryption and decryption respectively.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 6383,
      "end_char": 6782,
      "chunk_index": 16,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_17",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "\u2022 \nCreate a TestRunner class with a main method and exercise the functionality of the \nclass to ensure that it works correctly. \u2022 \nConsider the following code required to encrypt using the DES and AES standards.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 6782,
      "end_char": 6993,
      "chunk_index": 17,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_18",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "Note that DES and AES are symmetric cryptographic methods: \n \nKeyGenerator keyGen = KeyGenerator.getInstance(\"DES\"); \nkeyGen.init(128);  \nkey key = keyGen.generateKey(); \nCipher cypher = Cipher.getInstance(\"DES/ECB/PKCS5Padding\") \ncypher.init(Cipher.DECRYPT_MODE, key); \nbytep[] result =  cypher.doFinal(cypherText); \n \nKeyGenerator keyGen = KeyGenerator.getInstance(\"AES\");  \nkeyGen.init(128);  \nkey key = keyGen.generateKey(); \nCipher cypher  = Cipher.getInstance(\"AES/ECB/PKCS5Padding\"); \ncypher.init(Cipher.DECRYPT_MODE, key); \nbytep[] result =  cypher.doFinal(cypherText); \n \nIdentify and implement the alterations to the class RSACypher required to ensure \ncompatibility with symmetric keys.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 6993,
      "end_char": 7690,
      "chunk_index": 18,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_19",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "\u2022 \nUsing the refactoring menu, exact a superclass called AbstractCypher from the class \nRSACypher that contains a corpus of reusable code. Use abstract methods where \nappropriate. Explain the impact that this action has on the overall design of the \napplication. Create two new classes called DESCypher and AESCypher that directly inherit from \nAbstractCypher and implement the remaining code required to complete the \nfunctionality of each class.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 7690,
      "end_char": 8137,
      "chunk_index": 19,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_20",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "In each class, override the inherited method finalize() to \noutput the name of the class and its Object ID. \u2022 \nUsing the refactoring menu, extract an interface called Cypherable from the class \nAbstractCypher and alter the type used in TestRunner to the new interface. Explain \nthe impact that this action has on the overall design of the application. Advanced Software Design   \n \nDepartment of Computer Science & Applied Physics, ATU, Galway City Campus.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 3,
      "start_char": 8137,
      "end_char": 8593,
      "chunk_index": 20,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_21",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "4 \n \n\u2022 \nCreate an enum called CypherType with the options of AES, DES and RSA.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 4,
      "start_char": 8593,
      "end_char": 8671,
      "chunk_index": 21,
      "total_chunks": 23
    },
    {
      "chunk_id": "ff0450ebea1c43c1b4614829f2f621b3_chunk_22",
      "doc_id": "ff0450ebea1c43c1b4614829f2f621b3",
      "content": "\u2022 \nCreate a new class called CypherFactory defined as follows and alter TestRunner to \nreturn the type returned by the factory method: \n \npublic class CypherFactory { \n   private static CypherFactory f = new Throwable(); \n \n \n   private CypherFactory(){} \n \n \n   public static CypherFactory getInstance(){ \n      return f; \n   } \n \n \n   public Cypherable getCypher(CypherType type) throws Throwable { \n      if (type == CypherType.DES) { \n         return new DESCypher(); \n      }else if (type == CypherType.RSA) { \n         return new RSACypher(); \n      }else {  \n         return new AESCypher(); \n      } \n \n \n   } \n} \n \n \nExplain the impact that this action has on the overall design of the application.",
      "source_doc_title": "aswdCompositionLab",
      "section_id": null,
      "page_number": 4,
      "start_char": 8671,
      "end_char": 9378,
      "chunk_index": 22,
      "total_chunks": 23
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_0",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "Case Study: Cassandra\nJohn French\nATU Galway\n\nApache Cassandra\n\nWhat is Cassandra? Apache Cassandra: The Big Picture\nApache Cassandra is a NoSQL database designed for handling massive amounts of data\nacross many servers. Key characteristics:\n\u2022 No single point of failure\n\u2022 Data automatically replicated across multiple locations\n\u2022 Works across multiple data centers and cloud regions\nCase Study: Cassandra \n\u25b7 \nApache Cassandra\n2 / 21\n\nWhat is Cassandra? Why Do We Need Databases Like Cassandra?",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 494,
      "chunk_index": 0,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_1",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "Why Do We Need Databases Like Cassandra? Traditional relational databases (like MySQL, PostgreSQL) hit limits at scale:\nTraditional RDBMS challenges:\n\u2022 Single server bottleneck\n\u2022 Vertical scaling limits (bigger servers get\nexpensive)\n\u2022 Downtime for maintenance\n\u2022 Difficult multi-region operation\n\u2022 Slow with billions of writes per day\nCassandra\u2019s approach:\n\u2022 Spread data across many servers\n\u2022 Horizontal scaling (just add more servers)\n\u2022 Always available (no downtime)\n\u2022 Built for multi-region from day one\n\u2022 Optimized for high write volumes\nCase Study: Cassandra \n\u25b7 \nApache Cassandra\n3 / 21\n\nWhat is Cassandra?",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 4,
      "start_char": 454,
      "end_char": 1065,
      "chunk_index": 1,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_2",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "Who Uses Cassandra? Major companies running Cassandra at scale:\nNetflix\nViewing history, user preferences, billions of writes/day\nApple\niCloud services, 75,000+ nodes\nInstagram User feeds, photos metadata\nUber\nTrip data, location history\nDiscord\nMessage storage, 177+ trillion messages\nSpotify\nUser data, playlists\nCommon pattern: Companies that need to be \u201calways on\u201d at global scale\nCase Study: Cassandra \n\u25b7 \nApache Cassandra\n4 / 21\n\nWhat is Cassandra? What Makes Cassandra Different?",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 5,
      "start_char": 1065,
      "end_char": 1551,
      "chunk_index": 2,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_3",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "What Makes Cassandra Different? Relational Databases (MySQL):\n\u2022 Tables with strict schemas\n\u2022 ACID transactions\n\u2022 SQL queries with JOINs\n\u2022 Strong consistency\n\u2022 Primary-replica architecture\n\u2022 Best for: Complex queries, transactions\nCassandra (NoSQL):\n\u2022 Flexible column-family model\n\u2022 No multi-table transactions\n\u2022 Simple queries, no JOINs\n\u2022 Tunable consistency\n\u2022 Peer-to-peer (no master)\n\u2022 Best for: High availability, writes at scale\nThe trade-off: You give up some features (like JOINs and transactions) to gain massive scale\nand availability\nCase Study: Cassandra \n\u25b7 \nApache Cassandra\n5 / 21\n\nOrigins: From Facebook to\nApache\n\nOrigins: From Facebook to Apache\nThe Birth of Cassandra (2007-2008)\nCreated at Facebook by:\n\u2022 Avinash Lakshman \u2014 previously worked on Amazon\u2019s distributed database systems\n\u2022 Prashant Malik \u2014 systems engineer from Microsoft\nOriginal problem: Facebook\u2019s Inbox Search needed to handle 100+ million users with high\navailability\nThe design insight: Combine proven ideas from two tech giants:\n\u2022 From Amazon Dynamo: How to spread data across many servers without a single point\nof failure\n\u2022 From Google Bigtable: How to organize and store massive amounts of data efficiently\nFun fact: Named after the Greek prophet Cassandra \u2014 the creators believed it would predict\nthe future of databases\nCase Study: Cassandra \n\u25b7 \nOrigins: From Facebook to Apache\n7 / 21\n\nOrigins: From Facebook to Apache\nTimeline to Open Source\nJune 2008\nLaunched at Facebook for \u00a0100 million users\nJuly 2008\nOpen-sourced on Google Code\nMarch 2009 Accepted into Apache Incubator\nFeb 2010\nGraduated to Apache top-level project\nToday\nPowers Netflix, Apple, Instagram, Uber, Discord\u2026\nOne of the most successful database open-source projects in history\nCase Study: Cassandra \n\u25b7 \nOrigins: From Facebook to Apache\n8 / 21\n\nCassandra in CAP and PACELC\n\nCassandra in CAP and PACELC\nCAP Classification: AP System\nRemember the CAP theorem?",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 6,
      "start_char": 1520,
      "end_char": 3438,
      "chunk_index": 3,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_4",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "You can only pick 2 of 3: Consistency, Availability, Partition\ntolerance\nCassandra\u2019s choice:\n\u2022 \u2713 Availability \u2014 Always responds to requests (never says \u201cI\u2019m down\u201d)\n\u2022 \u2713 Partition Tolerance \u2014 Works even when network connections fail between data\ncenters\n\u2022 \u2717 Strong Consistency \u2014 Sometimes different servers might have slightly different data\ntemporarily\nWhat this means in practice:\n\u2022 If part of the network goes down, Cassandra keeps working\n\u2022 Both sides of a network split can accept writes\n\u2022 Conflicts get resolved later using timestamps (last write wins)\nCase Study: Cassandra \n\u25b7 \nCassandra in CAP and PACELC\n10 / 21\n\nCassandra in CAP and PACELC\nPACELC Classification: PA/EL\nPACELC extends CAP to consider what happens during normal operation (not just failures):\nIf P*artition happens \u2192 choose A*vailability over Consistency\nE*lse (normal operation) \u2192 choose L*atency over Consistency\nCassandra\u2019s philosophy: Speed and availability are more important than all servers\nagreeing immediately\nThis is intentional \u2014 designed for applications where being fast and always-on matters more\nthan instant consistency\nCase Study: Cassandra \n\u25b7 \nCassandra in CAP and PACELC\n11 / 21\n\nCassandra in CAP and PACELC\nTunable Consistency: You Can Choose\nCassandra lets you dial consistency up or down per query:\nLevel\nWhat it means\nSpeed\nONE\nWait for just 1 server to respond\nFastest\nQUORUM\nWait for majority of servers (e.g., 2 out of 3)\nBalanced\nLOCAL_QUORUM Wait for majority in your local data center\nGood for multi-region\nALL\nWait for all servers to respond\nSlowest but most consistent\nGetting strong consistency when you need it:\nIf you use QUORUM for both reads and writes (with 3 replicas):\n\u2022 Writes wait for 2 servers to acknowledge\n\u2022 Reads check 2 servers\n\u2022 Guaranteed overlap ensures you always see the latest data\nFlexibility: Use ONE for fast lookups, QUORUM when accuracy matters more\nCase Study: Cassandra \n\u25b7 \nCassandra in CAP and PACELC\n12 / 21\n\nArchitectural Foundations\n\nArchitectural Foundations\nGossip Protocol: How Nodes Stay in Sync\nNodes share information by \u201cgossiping\u201d with each other (like spreading rumors):\nEvery 1 second, each node:\nPicks 3 random other nodes\nExchanges info about: which nodes are alive, what data they have, cluster changes\nWhy gossip?",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 11,
      "start_char": 3438,
      "end_char": 5702,
      "chunk_index": 4,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_5",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "\u2022 No central \u201cmaster\u201d needed to track everything\n\u2022 Information spreads quickly (like actual gossip)\n\u2022 If a node dies, others find out within seconds\n\u2022 Works even if some nodes can\u2019t talk to each other\nReal-world analogy: Like a group chat where people occasionally share status updates \u2014\neventually everyone knows what\u2019s happening\nCase Study: Cassandra \n\u25b7 \nArchitectural Foundations\n14 / 21\n\nNetflix Case Study\n\nNetflix Case Study\nNetflix by the Numbers\nScale:\n\u2022 200+ million subscribers\n\u2022 Hundreds of Cassandra clusters\n\u2022 Tens of thousands of nodes\n\u2022 Petabytes of data\n\u2022 Millions of requests/second\nGlobal presence:\n\u2022 US regions\n\u2022 EU regions\n\u2022 Asia-Pacific regions\n\u2022 Active-active multi-region\n\u201c98% of Netflix streaming data is stored in Cassandra\u201d\nCase Study: Cassandra \n\u25b7 \nNetflix Case Study\n16 / 21\n\nNetflix Case Study\nWhy Netflix Chose Cassandra\nNetflix originally used Oracle, but it couldn\u2019t handle their growth:\nOracle (Before)\nCassandra (After)\nSingle datacenter\nMulti-region, global\nMaster-slave architecture\nMasterless, peer-to-peer\nDowntime for database changes\nNo downtime for updates\nLimited by server size\nJust add more servers\nSingle point of failure\nNo single point of failure\nThe killer requirement: Always-on availability for 200M+ users worldwide\nNetflix can\u2019t afford even 5 minutes of downtime \u2014 Cassandra makes that possible\nCase Study: Cassandra \n\u25b7 \nNetflix Case Study\n17 / 21\n\nNetflix Case Study\nPrimary Use Case: Viewing History\nEvery time you interact with Netflix, it writes to Cassandra:\n\u2022 Start watching a show\n\u2022 Pause, rewind, or fast-forward\n\u2022 Switch from TV to phone\n\u2022 Resume where you left off\nWhy viewing history is perfect for Cassandra:\n\u2022 9:1 write-to-read ratio \u2014 way more writes than reads (Cassandra\u2019s strength)\n\u2022 Each user\u2019s history is independent (easy to partition)\n\u2022 If your watch position is off by a few seconds temporarily, nobody cares (eventual\nconsistency is fine)\n\u2022 Time-series data (naturally fits Cassandra\u2019s clustering model)\nScale: Billions of writes per day across hundreds of millions of users\nCase Study: Cassandra \n\u25b7 \nNetflix Case Study\n18 / 21\n\nNetflix Case Study\nThe Full Netflix Data Architecture\nNetflix doesn\u2019t use just Cassandra \u2014 they use the right database for each job:\nCassandra for:\n\u2022 Viewing history (high write volume)\n\u2022 User activity / sessions\n\u2022 Content metadata\n\u2022 Device information\n\u2022 Personalization data\n\u2022 Application state\nOther databases they use:\n\u2022 MySQL: Billing/payments (need ACID\ntransactions)\n\u2022 CockroachDB: Complex financial\ntransactions\n\u2022 EVCache: Ultra-fast in-memory cache\n\u2022 Elasticsearch: Content search\n\u2022 Data warehouses: Analytics and reporting\nCassandra is very good for high availability and write-heavy workloads, but not for\neverything\nCase Study: Cassandra \n\u25b7 \nNetflix Case Study\n19 / 21\n\nKey Takeaways\nSummary: Why Cassandra Matters\n1.",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 15,
      "start_char": 5702,
      "end_char": 8534,
      "chunk_index": 5,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_6",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "Built for scale: Created at Facebook to handle 100M+ users, now powers some of the\nworld\u2019s largest applications\n2. CAP trade-offs: Chooses availability and speed over strong consistency \u2014 perfect for\ncertain use cases\n3. No single point of failure: Masterless architecture means any node can fail without\nbringing down the system\n4. Handles writes incredibly well: Append-only design makes writes extremely fast\n(under 10ms)\n5. Flexible consistency: You choose the right balance for each query\n6.",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 21,
      "start_char": 8534,
      "end_char": 9030,
      "chunk_index": 6,
      "total_chunks": 8
    },
    {
      "chunk_id": "e0b3d7ac10a53008b210f44d8e63dc3b_chunk_7",
      "doc_id": "e0b3d7ac10a53008b210f44d8e63dc3b",
      "content": "Real-world proof: Netflix serves 200M+ users globally with 99.99%+ uptime using\nCassandra\nCase Study: Cassandra \n\u25b7 \nNetflix Case Study\n20 / 21\n\nKey Takeaways\nBottom line: Cassandra excels when you need always-on availability, global distribution,\nand massive write throughput\nCase Study: Cassandra \n\u25b7 \nNetflix Case Study\n21 / 21",
      "source_doc_title": "Case Study: Cassandra",
      "section_id": null,
      "page_number": 21,
      "start_char": 9030,
      "end_char": 9358,
      "chunk_index": 7,
      "total_chunks": 8
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_0",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Distributed Systems\nCOMP08011\nB.Sc. (Hons) Software Development\nDr.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 67,
      "chunk_index": 0,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_1",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "(Hons) Software Development\nDr. John French\n\nIntroduction to Distributed \nStorage\n\nTypical System Architecture\nUser-\nFacing \nServer\nBack-End\nStorage\nRequest / \nReply\nQuery / Result\n\nDistributed System: Architecting for Scalability\nUser-\nFacing \nServer\nStorage\nUser-\nFacing \nServer\nUser-\nFacing \nServer\nLB\nLB\nZookeeper\nKafka\nBack-End\n\nWhat we want from a Database\n\u2022 We want our database to provide\n\u2022 Availability\n\u2022 Scalability\n\u2022 Fault Tolerance\n\u2022 A centralised database cannot achieve these goals\n\u2022 Centralised = a single database server machine\n\nCentralised Database Issues\n\u2022 Single Point of Failure\n\u2022 Losing a database is a lot worse than losing a compute node\n\u2022 Temporary failure to operate the business\n\u2022 Permanently lose data\n\u2022 Compute nodes can easily be restored\n\u2022 Permanent data loss can be detrimental to the business\n\nCentralised Database Issues\n\u2022 Performance Bottleneck\n\u2022 Parallelism limited to the number of cores in a machine\n\u2022 Limited connections the OS ad network card can support\n\u2022 Minimum latency depends on the geographical location of the database \ninstance and the user\n\u2022 Limited to the memory on a single machine\n\nDistributed System: Architecting for Scalability\nUser-\nFacing \nServer\nDistributed \nStorage\nUser-\nFacing \nServer\nUser-\nFacing \nServer\nLB\nLB\nZookeeper\nKafka\nBack-End\n\nSummary\n\u2022 Motivation for building a distributed database\n\u2022 Types of storage\n\u2022 File system\n\u2022 Database\n\u2022 Distributed File Systems\n\u2022 Types of Database\n\u2022 Relational\n\u2022 NoSQL\n\u2022 Problems with centralised databases\n\nDatabase Replication\nAlways Keep A Backup!",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 1,
      "start_char": 36,
      "end_char": 1585,
      "chunk_index": 1,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_2",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Replication and Sharding\n\u2022 Two main techniques in Distributed Databases to achieve\n\u2022 Scalability\n\u2022 Availability\n\u2022 Fault Tolerance\nReplication\nSharding (or Partitioning)\nFull copies of the data on \ndifferent machines\nSplitting up the data and storing \nchunks on different machines\nFull redundancy\nNo redundancy\n\nMotivation for Replication: High Availability\nDatabase\nInstance\nClient 1\nClient 2\nClient 3\n\nMotivation for Replication: High Availability\nMaster\nClient 1\nClient 2\nClient 3\nReplica\n\nMotivation for Replication: Fault Tolerance\nDatabase\nInstance\nClient 1\nClient 2\nClient 3\n\nMotivation for Replication: Fault Tolerance\nClient 1\nClient 2\nClient 3\nReplica\nMaster\n\nMotivation for Replication: Scalability / Performance\nDatabase\nInstance\nClient 1\nClient 2\nClient 3\nClient 4\nClient 5\nClient 6\n\nMotivation for Replication: Scalability / Performance\nClient 1\nClient 2\nClient 3\nClient 4\nClient 5\nClient 6\nDatabase \nInstance\nDatabase \nInstance\n\nReplicated Database \nArchitectures\nBuilding A Replicated Database\n\nPrimary/Replica Architecture\nPrimary\nClient 1\nClient 2\nClient 3\nReplica\nWrites\nReads\nWrites propagated to Replicas\n\nPrimary/Replica Architecture: Failover\nPrimary\nClient 1\nClient 2\nClient 3\nReplica\nWrites\nReads\nWrites\nPrimary\n\nMaster/Master Architecture\nMaster\nClient 1\nClient 2\nClient 3\nMaster\nMaster\n\nSummary\n\u2022 Motivation for Replicating a Database\n\u2022 Availability\n\u2022 Fault-Tolerance\n\u2022 Scalability / Performance\n\u2022 Database Replication Architectures\n\u2022 Primary / Replica\n\u2022 Master / Master\n\nReplication Challenges\nWHAT HAPPENS WHEN DATA CHANGES\n\nReplication\n\u2022 Keeping a copy of the same data on multiple machines that are \nconnected via a network.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 10,
      "start_char": 1585,
      "end_char": 3239,
      "chunk_index": 2,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_3",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Why Replicate data? \u2022 To keep data geographically close to your users \n\u2022 reduces latency\n\u2022 To allow the system to continue working even if some of its parts have failed\n\u2022 increases availability\n\u2022 To scale out the number of machines that can serve read queries \n\u2022 increase read throughput\n\nReplication \n\u2022 Assuming here full copy of data fits on single machine\n\u2022 If not then partitioning also needed\n\u2022 If data doesn\u2019t change, then replication is easy\n\u2022 Copy data to every node once, done!",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 24,
      "start_char": 3239,
      "end_char": 3727,
      "chunk_index": 3,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_4",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Difficulty lies in handling changes to the data\n\nREPLICATION & \nPARTITIONING \nWORKING TOGETHER\nA database split into two partitions, \nwith two replicas per partition.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 25,
      "start_char": 3727,
      "end_char": 3895,
      "chunk_index": 4,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_5",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Leader-Based Replication\n\u2022\nRelational DBs\n\u2022\nPostgreSQL, MySQL, Oracle \nDataGuard etc\n\u2022\nNon-Relational DBs\n\u2022\nMongoDB, Redis\n\u2022\nMessaging Systems\n\u2022\nKafka, RabbitMQ\n\u2022\nSome network file systems\n\nSynchronous vs Asynchronous Replication\n\u2022\nReplication to follower 1 is synchronous\n\u2022\nReplication to follower 2 is asynchronous\n\nSynchronous vs Asynchronous Replication\n\u2022 Synchronous replication to Follower 1\n\u2022 Leader waits until follower 1 has confirmed that it received the write before\n\u2022 reporting success to the user\n\u2022 making the write visible to other clients.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 26,
      "start_char": 3895,
      "end_char": 4449,
      "chunk_index": 5,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_6",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Asynchronous replication to Follower 2\n\u2022 the leader sends the message, but doesn\u2019t wait for a response from the \nfollower\n\u2022 Async replication usually fast <1 sec\n\u2022 Can be slower (minutes)\n\nWhy not make all followers Synchronous? \u2022 Good Idea!",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 29,
      "start_char": 4449,
      "end_char": 4692,
      "chunk_index": 6,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_7",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Good Idea! \u2022 Follower is guaranteed to have up-to-date copy of the data\n\u2022 If leader fails, data is still accessible on follower\n\u2022 Bad Idea\n\u2022 If follower doesn\u2019t respond, write can\u2019t be processed\n\u2022 Follower may have crashed,  fault in network etc\n\u2022 All writes blocked until synchronous follower is available again\n\u2022 Impractical to make all followers synchronous\n\u2022 One node outage and entire system grinds to a halt\n\nExample Leader-Based Database Configurations\n\u2022 Semi-synchronous replication\n\u2022 One follower is synchronous, others are asynchronous\n\u2022 If sync follower goes down / gets slow, an async follower is made \nsynchronous\n\u2022 Up-to-date copy of data always exists on 2 nodes\n\u2022 Fully Asynchronous\n\u2022 Leader can keep processing writes even if all followers have fallen behind\n\u2022 BUT: writes aren\u2019t durable\n\nQuorums in Leader-Based Replication\n\u2022 Quorum\n\u2022 A majority of nodes in a system.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 30,
      "start_char": 4680,
      "end_char": 5567,
      "chunk_index": 7,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_8",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Role in Replication\n\u2022 Ensures data consistency across nodes. \u2022  Required for each write operation to be successful. \u2022 Synchronous Replication: \n\u2022 Leader waits for acknowledgements from a quorum before considering a \nwrite successful. \u2022 Ensures data is safely stored on a majority of nodes.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 32,
      "start_char": 5567,
      "end_char": 5858,
      "chunk_index": 8,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_9",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Consistency Models\n\nReplication Lag\n\u2022 Replication isn\u2019t instantaneous\n\u2022 Client reading from async follower may see old data\n\u2022 Database inconsistencies: different response from leader and follower\n\u2022 How to deal with potential inconsistencies due to replication lag? \u2022 Consistency Models\n\nTypes of Consistency Models\n\u2022 Consistency models can be categorized as \n\u2022 strong consistency\n\u2022 weak consistency\n\u2022 eventual consistency\n\u2022 Each offers different guarantees and trade-offs.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 32,
      "start_char": 5858,
      "end_char": 6330,
      "chunk_index": 9,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_10",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Strong Consistency\n\u2022 Guarantees that after an update, all nodes reflect the same data \ninstantaneously (or appear to do so). \u2022 Example: In a bank transfer, the updated balance is immediately \nvisible to all nodes. \u2022 Advantages: Predictable and easy for developers to reason about. \u2022 Disadvantages: High latency and lower availability during network \npartitions\n\nWeak Consistency\n\u2022 Definition: Does not guarantee immediate consistency across \nnodes; updates propagate over time.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 35,
      "start_char": 6330,
      "end_char": 6807,
      "chunk_index": 10,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_11",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Example: A caching system where data may be stale briefly. \u2022 Advantages: Lower latency and higher availability. \u2022 Disadvantages: Developers must handle inconsistencies in the \napplication logic. Eventual Consistency\n\u2022 Ensures that if no new updates are made, all nodes will eventually \nconverge to the same state. \u2022 Example: DNS updates propagate globally over time. \u2022 Advantages: High availability and low latency.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 37,
      "start_char": 6807,
      "end_char": 7224,
      "chunk_index": 11,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_12",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Advantages: High availability and low latency. \u2022 Disadvantages: No guarantees about the time required to reach \nconsistency\n\nTrade-offs in Consistency Models\n\u2022 Latency vs. Consistency:\n\u2022\nStrong consistency increases latency. \u2022\nEventual consistency provides low-latency responses. \u2022 Availability vs. Consistency:\n\u2022\nStrong consistency systems may sacrifice availability during partitions. \u2022\nWeak or eventual consistency systems prioritize availability. \u2022 Application Needs:\n\u2022\nBanking systems demand strong consistency.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 38,
      "start_char": 7176,
      "end_char": 7694,
      "chunk_index": 12,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_13",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022\nSocial media can operate with eventual consistency. CAP Theorem\n\nNetwork Partitions\nNode Failure\nNode is down\nNetwork Partition\nNodes still up but can\u2019t \ncommunicate\n\nCAP Theorem\n\u2022 Consistency\n\u2022 all nodes see the same data at the same time. \u2022 Every read receives the most recent write\n\u2022 Availability \n\u2022 node failures do not prevent survivors from continuing to operate.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 39,
      "start_char": 7694,
      "end_char": 8065,
      "chunk_index": 13,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_14",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Every request receives a (non-error) response \n\u2022 No guarantee that it contains the most recent write\n\u2022 Partition tolerance\n\u2022 the system continues to operate despite message loss due to network \nand/or node failure\n\nCAP Theorem\n\u2022\n2 node distributed data store with a network \npartition\n\u2022\nNode 1 and node 2 can\u2019t communicate\n\u2022\nData item is updated on Node 1\n\u2022\nSame data item is read from node 2 \n\u2022\nOptions:\n1. Node 2 returns the old stale data \n\u2022\nConsistency is violated\n2.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 42,
      "start_char": 8065,
      "end_char": 8538,
      "chunk_index": 14,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_15",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Wait for Node 2 to be updated with fresh data \nfrom Node 2\n\u2022\nAvailability is violated\n3. Node 1 talking to Node 2: Not partition tolerant\n1\n2\n\nCAP Theorem\n\u2022 If a partition occurs, problem \nreduces to a binary choice between \n\u2022 Consistency\n\u2022 Availability\n\nCAP Theorem\n\nCAP Theorem: Summary\n\u2022 In a distributed data system, it is impossible to simultaneously \nprovide all three guarantees:\n\u2022 Consistency (C): Every read receives the most recent write or an error.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 43,
      "start_char": 8538,
      "end_char": 8998,
      "chunk_index": 15,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_16",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Availability (A): Every request receives a (non-error) response, without \nguarantee of the most recent write. \u2022 Partition Tolerance (P): The system continues to function despite arbitrary \nmessage loss or failure of part of the network. \u2022 During a network partition, a system must choose between:\n\u2022 Consistency: Ensuring all nodes reflect the same data. \u2022 Availability: Ensuring the system responds to requests.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 46,
      "start_char": 8998,
      "end_char": 9411,
      "chunk_index": 16,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_17",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "PACELC Theorem\n\nPACELC Theorem\n\u2022 A framework for understanding trade-offs in Distributed Database \ndesign\n\u2022 Extends the CAP theorem by adding considerations for latency \nduring normal operations. \u2022 Provides a more nuanced view of trade-offs faced by distributed \ndatabases in different scenarios.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 46,
      "start_char": 9411,
      "end_char": 9707,
      "chunk_index": 17,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_18",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "PACELC extends CAP\n\u2022 Trade-offs\n\u2022 In distributed systems, certain guarantees like consistency, availability, \npartition tolerance, and latency cannot be simultaneously optimized\n\u2022 CAP Theorem\n\u2022 States that in the presence of a network partition (P), a system can choose \neither Consistency (C) or Availability (A), but not both. \u2022 PACELC Extension\n\u2022 Extends CAP to consider scenarios where there is no network partition.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 48,
      "start_char": 9707,
      "end_char": 10127,
      "chunk_index": 18,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_19",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "PACELC Definition\n\u2022 If a network partition occurs (P), a system must choose between \nAvailability (A) and Consistency (C)\n\u2022 Else (E), when the system is running normally, it must choose \nbetween Latency (L) and Consistency (C). \u2022 Formula: PACELC is often represented as:\n\u2022 PAC (during partition) + ELC (else condition)\n\nCAP and PACELC\n\u2022 Highlights two separate decision points in distributed system \ndesign:\n1. During a network partition, prioritize availability or consistency. 2.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 49,
      "start_char": 10127,
      "end_char": 10608,
      "chunk_index": 19,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_20",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "2. During normal operation, prioritize low latency or strong consistency. \u2022 Implications:\n\u2022 CAP: Considers only network partition scenarios. \u2022 PACELC: Broader view, addresses trade-offs during normal operations, \nmaking it more practical for real-world systems. Trade-Offs\n\u2022 Partition Scenario (CAP):\n\u2022 Consistency: \n\u2022 Ensures all nodes reflect the same state at the same time. \u2022 Example: Databases like Google\u2019s Spanner prioritize consistency.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 51,
      "start_char": 10606,
      "end_char": 11050,
      "chunk_index": 20,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_21",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Availability: \n\u2022 Ensures that every request receives a response, even if the response might be stale. \u2022 Example: Amazon\u2019s DynamoDB prioritizes availability. Trade-Offs\n\u2022 Normal Operations Scenario (ELC):\n\u2022 Latency: \n\u2022 Low response time for read/write operations. \u2022 Example: DynamoDB optimizes for latency. \u2022 Consistency: \n\u2022 Guarantees that all clients see the same data at the same time. \u2022 Example: Spanner optimizes for strong consistency.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 52,
      "start_char": 11050,
      "end_char": 11492,
      "chunk_index": 21,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_22",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "Examples of PACELC in Practice\n\u2022 Amazon DynamoDB:\n\u2022\nDuring a partition: Prioritizes Availability (A) over Consistency (C). \u2022\nDuring normal operations: Prioritizes Latency (L) over Consistency (C). \u2022\nResult: DynamoDB is an AP/EL system. \u2022 Google Spanner:\n\u2022\nDuring a partition: Prioritizes Consistency (C) over Availability (A). \u2022\nDuring normal operations: Prioritizes Consistency (C) over Latency (L). \u2022\nResult: Spanner is a CP/EC system.",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 53,
      "start_char": 11492,
      "end_char": 11929,
      "chunk_index": 22,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_23",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022\nResult: Spanner is a CP/EC system. \u2022 Cassandra:\n\u2022\nDuring a partition: Offers tunable consistency, often leaning towards Availability (A). \u2022\nDuring normal operations: Optimizes for Latency (L) over strict Consistency (C). \u2022\nResult: Cassandra is an AP/EL system with tunable parameters. Factors Influencing PACELC Decisions\n\u2022 Application Requirements:\n\u2022\nBanking systems may prioritize consistency (C) over availability (A). \u2022\nSocial media platforms may prioritize availability (A) and latency (L).",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 54,
      "start_char": 11893,
      "end_char": 12390,
      "chunk_index": 23,
      "total_chunks": 25
    },
    {
      "chunk_id": "4f7d502a980b2f9d957d2eb3fa824be2_chunk_24",
      "doc_id": "4f7d502a980b2f9d957d2eb3fa824be2",
      "content": "\u2022 Workload Characteristics:\n\u2022\nWrite-heavy workloads may tolerate slightly stale reads (favoring L). \u2022\nRead-heavy workloads may demand up-to-date data (favoring C). \u2022 System Design Constraints:\n\u2022\nNetwork reliability, geographical distribution of nodes, and replication \nstrategies influence trade-offs. Database Summary\nDatabase\nPartition Scenario (CAP)\nNormal Operation (ELC)\nDynamoDB\nAP\nEL\nGoogle Spanner\nCP\nEC\nCassandra\nAP\nEL",
      "source_doc_title": "DistributedDatabases",
      "section_id": null,
      "page_number": 55,
      "start_char": 12390,
      "end_char": 12817,
      "chunk_index": 24,
      "total_chunks": 25
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_0",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "MOBILE APPLICATIONS DEVELOPMENT 2\nIHAB SALAWDEH\nDEPARTMENT OF COMPUTER SCIENCE & APPLIED PHYSICS \n\nINTERACTION DESIGN\n\nINTRODUCTION\n\uf0a1Design Thinking\n\uf0a1User Experience VS User Interface\n\uf0a1Principles of Interaction Design\n\uf0a1Usability\n\uf0a1Prototyping Interaction\n\nRECOMMENDED READING\n\uf0a1Book: The design of everyday things\n \nAvailable at ATU library\n\nDesign thinking is a non-linear, iterative process that teams use to:\n\uf0a1Understand users\n\uf0a1Challenge assumptions\n\uf0a1Redefine problems\n\uf0a1Create innovative solutions\n\uf0a1Prototype \n\uf0a1Test\nIt is most useful to tackle ill-defined or unknown problems and involves five phases: \nEmpathize, Define, Ideate, Prototype and Test.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 650,
      "chunk_index": 0,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_1",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "DESIGN THINKING\n\nDESIGN THINKING: THE GOAL\n\nDESIGN THINKING: THE PROCESS\n\nSTAGE 1: EMPATHIZE\n\uf0a1Research Users' Needs\n\uf0a1The team aims to understand the problem, typically through user research. Empathy is \ncrucial to design thinking because it allows designers to set aside your assumptions about \nthe world and gain insight into users and their needs.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 5,
      "start_char": 650,
      "end_char": 999,
      "chunk_index": 1,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_2",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "STAGE 2: DEFINE\n\uf0a1State Users' Needs and Problems\n\uf0a1Once the team accumulates the information, they analyse the observations and synthesize \nthem to define the core problems\n\uf0a1These definitions are called problem statements\n\uf0a1The team may create personas to help keep efforts human-centred.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 8,
      "start_char": 999,
      "end_char": 1285,
      "chunk_index": 2,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_3",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "STAGE 3: IDEATE\n\uf0a1Challenge Assumptions and Create Ideas\n\uf0a1With the foundation ready, teams gear up to \u201cthink outside the box.\u201d \n\uf0a1They brainstorm alternative ways to view the problem and identify innovative solutions to \nthe problem statement. STAGE 4: PROTOTYPE\n\uf0a1Start to Create Solutions \n\uf0a1This is an experimental phase. \uf0a1The aim is to identify the best possible solution for each problem.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 9,
      "start_char": 1285,
      "end_char": 1674,
      "chunk_index": 3,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_4",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1The team produces inexpensive, scaled-down versions of the product (or specific features \nfound within the product) to investigate the ideas. \uf0a1This may be as simple as paper prototypes\n\nSTAGE 5: TEST\n\uf0a1Try the Solutions Out\n\uf0a1The team tests these prototypes with real users to evaluate if they solve the problem. \uf0a1The test might throw up new insights, based on which the team might refine the prototype \nor even go back to the Define stage to revisit the problem.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 11,
      "start_char": 1674,
      "end_char": 2136,
      "chunk_index": 4,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_5",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "DESIGN THINKING: NON-LINEAR PROCESS\n\nWHY DESIGN THINKING? \uf0a1Designer\u2019s way of thinking is so powerful when it comes to complex problems\n\nUSER EXPERIENCE (UX)?",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 12,
      "start_char": 2136,
      "end_char": 2293,
      "chunk_index": 5,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_6",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\u201ca person\u2019s perceptions and responses that result from the use or anticipated use of a \nproduct, system or service\u201d ISO Definition\n\uf0a1User experience (UX) can be defined as all the experiences: Physical, sensory, emotional \nand mental That a person has when interacting with a digital tool\n\uf0a1It includes the practical, experiential, affective, meaningful and valuable aspects of human\u2013\ncomputer interaction and product ownership\n\uf0a1Additionally, it includes a person\u2019s perceptions of system aspects such as utility, ease of use \nand efficiency\n\nUNDERSTANDING UI/UX DESIGN \n\uf0a1Online UX can be divided into two broad categories: \n\uf0a1Functional UX\n\uf0a1\nThis covers the elements of the user experience that relate to actually using the tool, such as \nworking technical elements, navigation, search and links \n\uf0a1Creative UX\n\uf0a1\nThis is the bigger, harder to define impression created by the tool.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 15,
      "start_char": 2293,
      "end_char": 3171,
      "chunk_index": 6,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_7",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "The so-called \u2018wow\u2019 factor \nthat covers visual and creative elements\n\nUX VS UI\n\n\nVISUAL IDENTITY AND DESIGNING FOR PERSUASION\n\uf0a1There is a close relationship between UX and visual design. Ideally the visual designer \nwill use the documents created by the UX designer and add the visual skin\n\uf0a1UX and visual designer plays a key role in defining few basic considerations: \n\uf0a1\nNavigation: the signage of app, indicating to users where they are and where they can go. \uf0a1\nLayout: how content is structured and displayed.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 16,
      "start_char": 3171,
      "end_char": 3683,
      "chunk_index": 7,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_8",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\nLayout: how content is structured and displayed. \uf0a1\nHeaders: the element with a fixed position at the top of every page. They usually includes all \nprimary navigation items which need to be presented on every page such as main menu, login \nand search.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 19,
      "start_char": 3633,
      "end_char": 3885,
      "chunk_index": 8,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_9",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\nFooters: the usually consistent bottom part of the page \n\uf0a1\nCredibility: telling users that you are who you say you are \n\nCOLLECTING AND COLLATING DESIGN ASSETS\n\uf0a1A list of brand assets that a \ndesigner requires to start working \non a site:\n\uf0a1Brand guidelines or style guide. \uf0a1Logo and other key brand elements\n\uf0a1Image libraries\n\uf0a1Fonts folder\n\uf0a1Brand colours\n\uf0a1Any existing creative assets (Print \ndesigns, business cards, brochure, \nadvertisements).",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 19,
      "start_char": 3885,
      "end_char": 4331,
      "chunk_index": 9,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_10",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "DESIGN ASSETS (KISS)\n\nCREATE A NEW LOGO\n\nPRINCIPLES OF INTERACTION DESIGN \n\uf0a1Visibility\n\uf0a1Feedback\n\uf0a1Constraints\n\uf0a1Mapping\n\uf0a1Consistency\n\uf0a1Affordance\n\nPRINCIPLES OF INTERACTION DESIGN \n\uf0a1Visibility: Can I see it? \uf0a1Feedback: What is it doing? \uf0a1Constraints: What can/can\u2019t I do? \uf0a1Mapping: How the controls of a product impact the objects? \uf0a1Consistency: Similar behaviour should look alike\n\uf0a1Affordance: How do I use it?",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 20,
      "start_char": 4331,
      "end_char": 4740,
      "chunk_index": 10,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_11",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "VISIBILITY\n\uf0a1 Visibility is the basic principle that the more visible an element is, the more likely users \nwill know about them and how to use them. \uf0a1Equally important is the opposite: when something is out of sight, it\u2019s difficult to know \nabout and use.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 24,
      "start_char": 4740,
      "end_char": 4995,
      "chunk_index": 11,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_12",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1In mobile apps, the screen size limits the amount of content to show\n\uf0a1Applying this principle is realizing that you can\u2019t make everything visible, because it\u2019ll \nultimately clutter the interface \n\uf0a1Instead, you need to prioritize what interface elements are by far the most important for the \nuser experience and prioritize their visibility.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 25,
      "start_char": 4995,
      "end_char": 5336,
      "chunk_index": 12,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_13",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "FEEDBACK\n\uf0a1Feedback is the principle of making it clear to the user\n\uf0a1\nWhat action has been taken \n\uf0a1\nWhat has been accomplished\n\uf0a1Many forms of feedback exist in interaction design, including \n\uf0a1\nVisual \n\uf0a1\nTactile \n\uf0a1\nAudio, and more\n\nCONSTRAINTS\n\uf0a1Constraints is about limiting the range of interaction possibilities for the user to simplify the \ninterface and guide the user to the appropriate next action\n\uf0a1Limitless possibilities often leave the user confused.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 25,
      "start_char": 5336,
      "end_char": 5793,
      "chunk_index": 13,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_14",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "MAPPING\n\uf0a1Mapping is about having a clear relationship between controls and the effect they have on \nthe world\n\uf0a1You want this mapping to feel as natural as possible\n\uf0a1Example: This slider has a strong mapping, since it\u2019s clear moving it to the right will \nincrease its value versus moving it to the left will decrease it.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 27,
      "start_char": 5793,
      "end_char": 6112,
      "chunk_index": 14,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_15",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "MAPPING\n\nCONSISTENCY\n\uf0a1Consistency refers to having similar operations and similar elements for achieving similar \ntasks\n\uf0a1By leveraging consistent elements throughout your entire experience, you make your \nexperience far easier to use\n\uf0a1One of the best ways to drive consistency across applications, when designing mobile \napplications, is to make use of:\n\uf0a1\nBootstrap UI elements\n\uf0a1\nIONIC Framework\n\uf0a1\nGoogle\u2019s Material Design Guidelines\n\uf0a1\niOS\u2019s Human Interaction Guidelines\n\nCONSISTENCY\n\nAFFORDANCE\n\uf0a1Affordance refers to an attribute of an object that allows people to know how to use it\n\uf0a1Essentially to afford means to give a clue\n\uf0a1Example: \n\uf0a1\nThe physical button on a mouse gives a clue that it can be clicked to perform an action.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 28,
      "start_char": 6112,
      "end_char": 6842,
      "chunk_index": 15,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_16",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "When an \nobject has strong affordances, it\u2019s very clear how to use it\n\nAFFORDANCE\n\uf0a1Does this door open to the left or to the right? AFFORDANCE\n\uf0a1It is impossible to know on which side to push for the \ndoor to open\n\uf0a1Has a flat plate mounted on the side that is to be \npushed \n\uf0a1This is a naturally interpreted signal\n\uf0a1A nice design, no frustration for the user\n\nMEASURING USABILITY \n\uf0a1Usability: Refers to how user-friendly and efficient a digital product is\n\uf0a1How to assess usability? \uf0a1\nWhat criteria or measures?",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 32,
      "start_char": 6842,
      "end_char": 7351,
      "chunk_index": 16,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_17",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\nWhat criteria or measures? \uf0a1\nWhat thresholds?",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 35,
      "start_char": 7323,
      "end_char": 7370,
      "chunk_index": 17,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_18",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\nWhat criteria or measures? \uf0a1\nWhat thresholds? \uf0a1\nWhat is \u201cusable enough?\u201d \n\nMEASURING USABILITY: QUALITATIVE METHODS \n\uf0a1Stakeholder Interviews\n\uf0a1Subject Matter Expert (SME)\n\uf0a1Competitive Reviews\n\uf0a1Literature Reviews\n\uf0a1In-Depth Interviews (Customer & User)\n\uf0a1Direct Observation (Ethnography)\n\uf0a1Contextual Inquiry\n\uf0a1Usability Testing\n\uf0a1Co-design/Participatory Group/Social \n\uf0a1Research\n\uf0a1Focus Groups\n\uf0a1Telephone Interview\n\nMEASURING USABILITY: QUANTITATIVE METHODS\n\uf0a1 Surveys\n\uf0a1Remote \"Testing\" Tools\n\uf0a1Metrics/Analytics Analysis\n\uf0a1Multivariate, A/B Testing \n\uf0a1Cohort Analysis\n\uf0a1Eye-tracking\n\nTRIANGULATION/BLENDED RESEARCH \n\nUSABILITY LAB \n\uf0a1Users are often in a more informal environment \n(comfortable and relaxed) \n\uf0a1Great for bringing in teams together to observe users \n(and discuss)\n\uf0a1Help and support when defining tasks and scenarios\n\uf0a1Less likely to introduce moderator bias\n\uf0a1Detailed external provider analysis and reports\n\uf0a1More likely to use advanced data collection (Eye \ntracking)\n\nUSABILITY LAB: DISADVANTAGES \n\uf0a1Can be costly \n\uf0a1Less likely to want to invest in early-stage \ntesting (Prototypes and early beta code)\n\uf0a1Less likely to perform iterative tests\n\nTHINK ALOUD \n\uf0a1A method used to gather data in usability testing \nin product design and development\n\uf0a1Very widely used, useful technique\n\uf0a1 The user is given a task, and the evaluator just \nwatches the user \n\uf0a1Subjects are asked to say what they are \nthinking/doing:\n\uf0a1\nWhat they believe is happening\n\uf0a1\nWhat they are trying to do\n\uf0a1\nWhy they took an action\n\uf0a1\nGives insight into what the user is thinking\n\nTHINK ALOUD: DISADVANTAGES \n\uf0a1Awkward/uncomfortable for the subject (thinking \naloud is not normal!) \n\uf0a1\u201cThinking\u201d about it may alter the way people \nperform their task\n\uf0a1Hard to talk when they are concentrating on a \nproblem\n\uf0a1Still the most widely used method in the industry\n\nPROTOTYPING IN HUMAN-COMPUTER INTERACTION(HCI)\n\uf0a1Prototyping in Human-Computer Interaction (HCI) refers to the process of creating a \nsimplified, preliminary version of a digital product or system to gather feedback and test \ndesign concepts.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 35,
      "start_char": 7323,
      "end_char": 9385,
      "chunk_index": 18,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_19",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1The main objectives of prototyping in Human-Computer Interaction (HCI) are:\n\uf0a1\nGather Feedback\n\uf0a1\nTest Design Concepts\n\uf0a1\nRefine Interaction Design\n\uf0a1\nValidate Design Decisions\n\uf0a1\nImprove Communication\n\uf0a1\nReduce Development Risks\n\nPROTOTYPING IN HUMAN-COMPUTER INTERACTION(HCI)\ne.g. A paper sketch of the \napp's main screens, showing \nbasic layout and navigation. e.g. More detailed screens with \nbasic interactive elements like \nbuttons and links, but without \nactual content or graphics. e.g.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 43,
      "start_char": 9385,
      "end_char": 9874,
      "chunk_index": 19,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_20",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "e.g. Fully designed screens with \nrealistic content, graphics, and \ninteractive elements that closely \nresemble the final app. PROTOTYPING IN HUMAN-COMPUTER INTERACTION(HCI)\n\uf0a1\nLow-Fidelity Prototype\n\uf0a1\nDesigners create rough sketches on paper showing different screens and basic interactions of the app. \uf0a1\nWhy?: To quickly explore and iterate on different layout and interaction ideas, gather initial feedback from \nusers.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 44,
      "start_char": 9870,
      "end_char": 10291,
      "chunk_index": 20,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_21",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\nMedium-Fidelity Prototype:\n\uf0a1\nDesigners use digital tools to create wireframes of the app, showing more detailed layouts and interactions. \uf0a1\nWhy?: To test and refine the user flow, navigation, and overall usability of the app. \uf0a1\nHigh-Fidelity Prototype:\n\uf0a1\nDesigners create a digital prototype of the app that closely resembles the final product, including realistic \ngraphics and interactive elements.",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 45,
      "start_char": 10291,
      "end_char": 10693,
      "chunk_index": 21,
      "total_chunks": 23
    },
    {
      "chunk_id": "51e2fde88932ccb5ad5b75c77228023d_chunk_22",
      "doc_id": "51e2fde88932ccb5ad5b75c77228023d",
      "content": "\uf0a1\nWhy?: To conduct usability testing with users, validate design decisions, and provide stakeholders with a \nrealistic preview of the app. MORE RESOURCES\n\uf0a1\nUser Experience (UX) Design\n\uf0a1\nhttps://www.interaction-design.org/literature/topics/ux-design\n\uf0a1\nHuman-Computer Interaction (HCI)\n\uf0a1\nhttps://www.interaction-design.org/literature/topics/human-computer-interaction \n\nQUESTIONS",
      "source_doc_title": "Lecture 5 - Interaction Design",
      "section_id": null,
      "page_number": 45,
      "start_char": 10693,
      "end_char": 11070,
      "chunk_index": 22,
      "total_chunks": 23
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_0",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Machine Learning Technologies\nWeek 1 Introduction\nDonny Hurley\nDonny Hurley\nIntro\n1 / 55\n\nWhat is Machine Learning (ML)? Algorithms that automatically improve performance through experience\nOften this means define a model by hand, and use data to fit its parameters\nThere are problems that are difficult for humans but easy for computers\nE.g. calculating large arithmetic problems\nAnd there are problems easy for humans but difficult for computers\nE.g.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 452,
      "chunk_index": 0,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_1",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "recognising a picture of a person from the side\nMachine Learning often tries to leverage the things computers are good at (large\narithmetic and repetitive problems) to solve things that we are usually able to do more\neasily. Donny Hurley\nIntro\n2 / 55\n\nWhy ML? The real world is complex \u2013 difficult to hand-craft solutions. Think about how many \u201dif\u201d statements would be needed! ML is the preferred framework for applications in many fields:\nComputer Vision\nNatural Language Processing\nSpeech Recognition\nRobotics\n. . .",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 2,
      "start_char": 452,
      "end_char": 969,
      "chunk_index": 1,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_2",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": ". . Humans can typically parse sentences but computers do not get the context. Humans are not consistent with grammar etc but computers expect consistency. Donny Hurley\nIntro\n3 / 55\n\nBook\nThere are absolutely loads\nof\nbooks\non\nmachine\nlearning\nDonny Hurley\nIntro\n4 / 55\n\nCourse Syllabus - Subject to Change...",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 3,
      "start_char": 966,
      "end_char": 1275,
      "chunk_index": 2,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_3",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "General ML stuff:\nSupervised and Unsupervised Algorithms\nClassification\nRegression\nGeneralisation\nUnderfit/Overfit\nBias/Variance\nTraining/Test sets\nError/Loss Functions\nCross-Validation\nGradient Descent\nRegularisation\nDonny Hurley\nIntro\n5 / 55\n\nModels/Algorithms:\nNaive Bayes\nK-Nearest Neighbour\nSupport Vector Machines\nNeural Networks\nPrinciple Component Analysis\nApplications:\nNatural Language Processing (NLP) (Basic not LLMs!)\nImage Recognition\nSome other things\nDonny Hurley\nIntro\n6 / 55\n\nAssessment\nThree components for assessment:\nEither an in-class lab exam or project about shallow AI around week 7 (30%)\nA project about Convolutional Neural Networks due at end of semester (30%)\nAn Exam (40%) during exam time\nDonny Hurley\nIntro\n7 / 55\n\nBasic idea of all machine learning\nMachine Learning is using an algorithm that can learn from Data.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 5,
      "start_char": 1275,
      "end_char": 2121,
      "chunk_index": 3,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_4",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "We want to create a Model that takes input and gives output. It is basically\ny = f (x)\nWe figure out the right-hand side of the equation by gathering a lot of data and\nmaking the parameters best fit our base model. Donny Hurley\nIntro\n8 / 55\n\nBasic Example\nTake a base model of y = w0 + w1x, we need the figure out the \u201dbest\u201d w0 and w1\nparameters e.g. Of course our base model can be much more complicated, have many more parameters\nthat need to be learnt, but the basic idea is the same.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 8,
      "start_char": 2121,
      "end_char": 2608,
      "chunk_index": 4,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_5",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "After we have made the model, we can then make predictions in the future. Above,\nhow many daily minutes do we predict a person with 30 friends on the platform? Donny Hurley\nIntro\n9 / 55\n\nWhy does it take so long? In the above example, one parameter, a basic line as the model, learning all the\nparameters is very very quick. But real world is not that quick. Some ML models can takes hours, days, weeks or even months to be trained.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 9,
      "start_char": 2608,
      "end_char": 3040,
      "chunk_index": 5,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_6",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "ChatGPT (some version of it anyway!) is actually 8 different models, with 220 billion\nparameters in each. This takes a very long time to figure out. Donny Hurley\nIntro\n10 / 55\n\nWhy does it take so long? The training is repeatedly doing the\nsame procedure over and over and over\nand over again, until we get the\nparameters we think are best. Donny Hurley\nIntro\n11 / 55\n\nGathering Data\nMost of the data we want, needs to be labelled.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 10,
      "start_char": 3040,
      "end_char": 3471,
      "chunk_index": 6,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_7",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "This means, any data we put into the\nsystem is a pair of information\ny, x\nwhere y is the right answer for that and x is the collection of all features\nSo let\u2019s say we have a collection of images that are cats or dogs and we want to build\na model that can separate them. For a particular image there is a correct y, i.e. y is cat or dog and then x is all the\nfeatures of the image i.e. each x will be the colour of a pixel.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 12,
      "start_char": 3471,
      "end_char": 3893,
      "chunk_index": 7,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_8",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "each x will be the colour of a pixel. Donny Hurley\nIntro\n12 / 55\n\nDataset example\nWe (the human intelligence) must do a\nlot of work here. We need lots of training data and labels\nassociated with them. Example: MNIST Data set. To learn to recognise handwritten let-\nters the algorithm must be trained on\n60,000 samples, each of which must be\nlabelled correctly - by hand. Samples of data set, from wikimedia.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 12,
      "start_char": 3856,
      "end_char": 4263,
      "chunk_index": 8,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_9",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Samples of data set, from wikimedia. Donny Hurley\nIntro\n13 / 55\n\nHand-written Digit Recognition\nNow let\u2019s look at some potential applications and how we could try \u201dphrasing\u201d the\nproblem. Difficult to hand-craft rules about digits. Donny Hurley\nIntro\n14 / 55\n\nHand-written Digit Recognition\nxi =\n, yi = (0, 0, 0, 0, 1, 0, 0, 0, 0, 0)\nSo the image can be represented by the value of each pixel. Each pixel colour is called\na feature, they are independent variables..",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 13,
      "start_char": 4227,
      "end_char": 4691,
      "chunk_index": 9,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_10",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Represent input image as a vector xi = R784\nSuppose we have a target vector ti\nThis is supervised learning\nDiscrete, finite label set: a classification problem. Given a training set {(x1, y1), . . . , (xN, yN)}, the learning problem is to construct a\n\u201dgood\u201d function y(x) from these. y : R784 \u2192R10\nSome algorithms will not exactly return (0, 0, 0, 1, . . . etc. but may return probabilities\nlike (0.01, 0.03, 0.04, , 0.01, 0.88 . . . so the image is most likely a 4.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 15,
      "start_char": 4691,
      "end_char": 5157,
      "chunk_index": 10,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_11",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": ". . so the image is most likely a 4. Donny Hurley\nIntro\n15 / 55\n\nFace Detection\nClassification problem. ti \u2208{0, 1, 2}, non-face, frontal face, profile face. Of course this can be expanded into the face of a particular person so the possible ti\nset could be large. ti is the set of ALL possible results. We map/transform the image to a particular ti\nDonny Hurley\nIntro\n16 / 55\n\nStock Price Prediction\nProblems in which ti is continuous are called regression\nE.g.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 15,
      "start_char": 5121,
      "end_char": 5582,
      "chunk_index": 11,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_12",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "ti is stock price, xi contains company profit, debt, cash flow, gross sales,\nnumber of spam emails sent, . . . Donny Hurley\nIntro\n17 / 55\n\nSupervised Learning\nTechniques where we have training examples where we know the correct result (the y\nvalues).",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 17,
      "start_char": 5582,
      "end_char": 5832,
      "chunk_index": 12,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_13",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Different types of supervised learning are :\nClassification\nRegression\nSome algorithms :\nLinear/Logistic Regression\nNaive Bayes\nk-Nearest Neighbours\nSupport Vector Machines\nNeural Networks\nDecision Trees\nRandom Forests\nDonny Hurley\nIntro\n18 / 55\n\nClustering Images\nOnly xi is defined : unsupervised learning\nE.g.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 18,
      "start_char": 5832,
      "end_char": 6144,
      "chunk_index": 13,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_14",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "xi describes image, find groups of similar images\nDonny Hurley\nIntro\n19 / 55\n\nUnsupervised Learning\nTechniques where there is no \u201dright\u201d answer known but the algorithm tries to find\nsome structure/patterns in the data. Principal Component Analysis (Dimensionality Reduction)\nk-mean Clustering\nPageRank (Google)\nUnsupervised learning techniques will group things together but we do not necessarily\nknow what the groups are.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 19,
      "start_char": 6144,
      "end_char": 6566,
      "chunk_index": 14,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_15",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "The images example, after the grouping was done we were\nable to say \u201dThis group is pitching, this group is sliding \u201d etc. But we had no way of\nknowing this before it was run and the computer does not have labels for the data\nafter the algorithm. Donny Hurley\nIntro\n20 / 55\n\nOther Types\nThere are other types of Machine Learning, e.g. Reinforcement Learning, but we won\u2019t\nbe talking about them at all in this course. If you\u2019ve tried using ML-Agents in Unity,\nthese are an example of Reinforcement Learning.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 20,
      "start_char": 6566,
      "end_char": 7071,
      "chunk_index": 15,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_16",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "We won\u2019t be getting as far as Recurrent Neural Networks (often used for time-series\ndata, sometimes text data) or things like LSTM style models. Additionally we won\u2019t be doing anything with Transformer models (these are the\nbreakthrough that made GPTs work and it has been discovered to be very good for\nother things too - they are more complex for training and using)\nDonny Hurley\nIntro\n21 / 55\n\nDifferences\nWhat is the one big problem with supervised learning techniques?",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 21,
      "start_char": 7071,
      "end_char": 7544,
      "chunk_index": 16,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_17",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "All the training data has to be labelled, often manually, before the model can be built. Unsupervised learning: It is often easier to obtain unlabeled data - from a lab\ninstrument or a computer - than labeled data, which can require human intervention. E.g. Netflix do not have defined groups of people but they group people into clusters\nand make recommendations based on the cluster they are in.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 22,
      "start_char": 7544,
      "end_char": 7941,
      "chunk_index": 17,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_18",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Donny Hurley\nIntro\n22 / 55\n\nTypes of Machine Learning\nDonny Hurley\nIntro\n23 / 55\n\nMachine Learning\nOften referred to as narrow AI. We are only concerned with a specific task. So the goal of machine learning is not to develop general intelligence or a\nuniversal learning algorithm. Machine learning seeks an algorithm that learns a particular task well\nAnd should result in a system that probably carries out the task correctly on most\noccasions.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 22,
      "start_char": 7941,
      "end_char": 8386,
      "chunk_index": 18,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_19",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Donny Hurley\nIntro\n24 / 55\n\nMachine Learning\nAlmost every AI thing talk about at the moment are really examples of Machine\nLearning (ML) which is a subset of AI. ML is Narrow AI, it is designed for a specific task. Ask ChatGPT if it is an example of\nnarrow AI and it says\nYes, as an AI language model developed by OpenAI, I am an example of\nnarrow AI. I am designed for a specific task, which is generating human-like\ntext based on the input I receive from users.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 24,
      "start_char": 8386,
      "end_char": 8849,
      "chunk_index": 19,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_20",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "My capabilities are focused\non natural language understanding and generation, and I am not capable of\ngeneral intelligence or tasks outside of my specific domain. Donny Hurley\nIntro\n25 / 55\n\nAI - Machine Learning - Deep Learning\nOpenDataScience\nMedium\nNewGenApps\nDonny Hurley\nIntro\n26 / 55\n\nWhat is Machine Learning (ML)? We tell the system how to learn not what to do. Many of the learning algorithms have been around for decades going right back to\nthe 1960s.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 25,
      "start_char": 8849,
      "end_char": 9310,
      "chunk_index": 20,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_21",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Neural Networks had beginnings in the 1940s, Deep Learning is building upon\n\u201dbasic\u201d neural networks. Rosenblatt (1958) created the perceptron, the building blocks of Neural Networks. The problem was, they didn\u2019t work very well. Research stagnated as basic\nperceptrons were incapable of processing the exclusive-or circuit and that\ncomputers lacked sufficient power to process useful neural networks. This has been one of the problems with AI in general.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 27,
      "start_char": 9310,
      "end_char": 9763,
      "chunk_index": 21,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_22",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Something new is developed that seems full of promise but then turns out to not\nwork very well in practice. So AI goes back into hibernation... Donny Hurley\nIntro\n27 / 55\n\nAI Winter\nSource https://www.actuaries.digital/2018/09/05/history-of-ai-winters/\nDonny Hurley\nIntro\n28 / 55\n\nWhat has changed? Two major developments have brought all the machine learning algorithms back into\nfashion, especially Neural Networks/Deep Learning.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 27,
      "start_char": 9763,
      "end_char": 10194,
      "chunk_index": 22,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_23",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "What seemed like insurmountable problems in the past now maybe have solutions\nBig Data\nThe amount of data available now is orders of magnitude larger than the past. A \u201drule\u201d of ML, the more data the better. Neural Networks/Deep Learning in particular needs a lot of data to work properly. In the past the issue was trying to get more data, now it is often, how do we process\nall this information.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 29,
      "start_char": 10194,
      "end_char": 10590,
      "chunk_index": 23,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_24",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Compute Power\nIn particular GPUs\nPeople realised the efficiency and power of graphics cards (thanks gamers!) for doing\nparallel calculations of maths that ML relies on. These graphics cards can do matrix (Linear Algebra) operations much more\nefficiently than a CPU.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 29,
      "start_char": 10590,
      "end_char": 10855,
      "chunk_index": 24,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_25",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "(Tensor maths, Tensor cores, Tensor Flow)\nDonny Hurley\nIntro\n29 / 55\n\nTimeline\nI talked about more historical things on the earlier slides, what have been the big\nbreakthroughs that led us to where we are now\n2012: AlexNet paper and algorithm achieves breakthrough results in image\nrecognition in the ImageNet benchmark. This popularizes deep neural networks.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 29,
      "start_char": 10855,
      "end_char": 11214,
      "chunk_index": 25,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_26",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "This popularizes deep neural networks. Rest of 2010s: Further refinement of classification algorithms and improvements\nfor the ImageNet benchmarks and others\n2017: A team at Google Brain invent the transformer architecture. This is widely\nused for Text processing so led to LLMs, but also used by Nvidia for newer DLSS\nmodels. 2020s: GenAI took off this decade. Donny Hurley\nIntro\n30 / 55\n\nWhat do we need, to do Machine Learning?",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 30,
      "start_char": 11176,
      "end_char": 11606,
      "chunk_index": 26,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_27",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "The Task - define or limit the task\nThe Experience - This is the data, more data = more experience\nThe Performance measure - We need a good way to measure this. The Learning Algorithm - The recipe by which we will improve our performance. The Intelligence - the Network - the brain. Donny Hurley\nIntro\n31 / 55\n\nWhat\u2019s in the following image? How would you instruct a computer to recognise what\u2019s in the image? The computer doesn\u2019t need to be able to define what a dog is, in order to classify it.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 31,
      "start_char": 11606,
      "end_char": 12102,
      "chunk_index": 27,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_28",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "We give it lots of pictures of dogs, telling the computer: these are pictures of dogs. It\nis then up to the computer to \u201cdecide\u201d how to classify based on the algorithm. What\nparts does it pick out as important. Donny Hurley\nIntro\n32 / 55\n\nAlternatively\nCan a computer tell the difference between these two dogs? The computer doesn\u2019t need to be able to define who each dog is, in order to classify it. We give it lots of pictures of the dogs, telling the computer: these are pictures of two\ndifferent dogs.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 32,
      "start_char": 12102,
      "end_char": 12607,
      "chunk_index": 28,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_29",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "It is then up to the computer to \u201cdecide\u201d how to classify based on the\nalgorithm. Again, what parts does it pick out as important. An ML Model does not need to explain itself, so it may be taking completely different\ninformation than you expect e.g. https://venturebeat.com/business/\nwhen-ai-flags-the-ruler-not-the-tumor-and-other-arguments-for-abolishing-t\nDonny Hurley\nIntro\n33 / 55\n\nBad AI\nhttps://github.com/daviddao/awful-ai keeps a collection of Awful AI.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 33,
      "start_char": 12607,
      "end_char": 13069,
      "chunk_index": 29,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_30",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Donny Hurley\nIntro\n34 / 55\n\nMachine Learning Technologies\nRegression\nDonny Hurley\nDonny Hurley\nRegression\n35 / 55\n\nIndependent/Dependent Variables\nShort note on independent and dependent variables\nExample\nThe Value of a House Over Time\nHere we have two variables\n1 Value of House (V)\n2 Time (t)\nSince they both change, they are Variables\nValue is a Dependent variable, since the value of a house depends on the time you\nbought it\nTime is independent.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 34,
      "start_char": 13069,
      "end_char": 13519,
      "chunk_index": 30,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_31",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Plotting we typically leave the horizontal axis to repesent the independent variable\nand the vertical axis to represent the dependent variable\nDonny Hurley\nRegression\n36 / 55\n\nLinear Relationships\nIf this course, we will mostly look at linear relationships to make our predictions. This\nshould be revision on the next couple of slides! Definition\nA Linear relationship: a change in the value of x always produces the same\nproportionate change in the value of y.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 36,
      "start_char": 13519,
      "end_char": 13980,
      "chunk_index": 31,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_32",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Consider the following table:\nx\n1\n2\n3\n4\n5\n6\ny\n5\n8\n14\n17\n20\n44\na\nWhat is the value of y when x = 3? b\nWhat value of x will give y = 44? Donny Hurley\nRegression\n37 / 55\n\nOnce we are convinced that some relationship does exist, we can establish the\nprecise nature of that relationship and use it to predict values of one variable that\nwould correspond to any given values of the other. The exactness of the relationship can be seen in the diagram above. The plotted\npoints lie along an imaginary straight line.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 37,
      "start_char": 13980,
      "end_char": 14487,
      "chunk_index": 32,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_33",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "If we were to draw a straight line\nthrough them, we could use it for making exact predictions without even\nusing the formula. Donny Hurley\nRegression\n38 / 55\n\nCan you come up with a formula for the previous graph??? y = mx + c\ny = 3x + 2\nfor this particular line. We can now use this formula to \u201dpredict\u201d values.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 38,
      "start_char": 14487,
      "end_char": 14799,
      "chunk_index": 33,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_34",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "We can now use this formula to \u201dpredict\u201d values. This very very early taught equation of a line is the basis for regression and lots of\nmachine learning analysis\nDonny Hurley\nRegression\n39 / 55\n\nRegression\nThis is where we try to zone in on a continuous value(s) rather than trying to predict a\nclass or category. So, for example if we are trying to predict the distance to a pedestrian, from a vehicle,\nthis would be a regression problem.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 39,
      "start_char": 14751,
      "end_char": 15190,
      "chunk_index": 34,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_35",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Donny Hurley\nRegression\n40 / 55\n\nExample: How to Cook a Turkey\nThere is a rule of thumb when cooking a turkey. Put it in a pre-heated oven for 20 minutes per pound plus 20 minutes. Where did this idea come from? Is it correct? What could happen if it was wrong? Donny Hurley\nRegression\n41 / 55\n\nExample: How to Cook a Turkey\nThis is a linear model which is likely based on linear regression analysis of different\nweights of turkeys.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 40,
      "start_char": 15190,
      "end_char": 15622,
      "chunk_index": 35,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_36",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "The mathematical model is\nt = mw + c\nwhere t is the cooking time, w is the weight in pounds, m is the slope of the line and c\nis the y-intercept. Pief Panofsky disagreed and used the following model. t = w(2/3)/1.5\nLet\u2019s analyse this. Donny Hurley\nRegression\n42 / 55\n\nExample: How to Cook a Turkey\nFirst notice that for the linear model you\nhave to cook an imaginary turkey for 20\nminutes. Between 1 pound and 6 pounds, the\nmodels are very similar. There is 90\nminutes difference between them at 16\npounds.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 42,
      "start_char": 15622,
      "end_char": 16128,
      "chunk_index": 36,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_37",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Which is Safer? Which tastes better? Use a meat thermometer. Donny Hurley\nRegression\n43 / 55\n\nFitting a linear model\nWith the above example we see the result of a linear model but not how it was arrived\nat. To do that we have to collect some data, and fit the best line to it. In the above case this would mean cooking many turkeys of various weights and\ncooking them all using some calibrated method, such as a meat thermometer (or an\nexpert cook with experience).",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 43,
      "start_char": 16128,
      "end_char": 16593,
      "chunk_index": 37,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_38",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "We collect the \u201dright\u201d answer and use this to build the model i.e. figure out the\nparameters. Let\u2019s look at some data. Donny Hurley\nRegression\n44 / 55\n\nExample: How to Cook a Turkey\nGiven the data to the left, if you had a\nnine pound turkey, how long should you\ncook it for? Is this a task for supervised learning or\nunsupervised learning? If we assume a linear model, what are\nthe parameters that we need to learn?",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 44,
      "start_char": 16593,
      "end_char": 17008,
      "chunk_index": 38,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_39",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "t = mw + c\nDonny Hurley\nRegression\n45 / 55\n\nExample: How to Cook a Turkey\nWeight in Pounds\n1.1\n1.3\n1.7\n2\n2.1\n2.3\n... Time taken to cook\n39.4\n48.8\n55.8\n63.0\n69.5\n67.7\n... So w is our input variable/feature. t is our output and can be considered our regression target that we will use to train\nthe model. We need to learn m and c. We will also use the notation of M for number of training samples M=83 in this case. (w, t) one training example. \u0000w(i), t(i)\u0001\nthe i-th training example.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 45,
      "start_char": 17008,
      "end_char": 17490,
      "chunk_index": 39,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_40",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "\u0000w(i), t(i)\u0001\nthe i-th training example. Donny Hurley\nRegression\n46 / 55\n\nNotation\nYou will notice in the previous slides that we used variable names that either match the\nproblem or that are used regularly in Mathematics. However, we will not always be dealing with linear models so we are going to use\nnotation that can be scaled up to larger models.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 46,
      "start_char": 17451,
      "end_char": 17802,
      "chunk_index": 40,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_41",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "There isn\u2019t a set notation, so where possible I will use whatever is used in scikit-learn\nTherefore we will use w to represent parameters/weights that we are going to train. We will most often use y as our output and x as input. So our above equation will now be\ny = w0 + w1x\nDonny Hurley\nRegression\n47 / 55\n\nWarning on Use of Linear Regression\nLinear Regression is based on the assumption that the data points are scattered about\na line.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 47,
      "start_char": 17802,
      "end_char": 18240,
      "chunk_index": 41,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_42",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "However, often data points are scattered on a curve, example\nYou can still compute the coefficients (\u03b20 and \u03b21) which will give the line above. However, we can see it is an inappropriate fit as the line suggests the y-values keep\nincreasing while the curve shows that they will actually decrease after a point\nCriterion for Finding a Regression Line\nBefore finding a regression line for a set of data points, draw a scatterplot.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 48,
      "start_char": 18240,
      "end_char": 18668,
      "chunk_index": 42,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_43",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "If the data\npoints do not appear to be scattered about a line, do not determine a regression line. Donny Hurley\nRegression\n48 / 55\n\nOthers\nThis was an example of a Simple Linear Regression model. There is only one\nindependent feature, but of course the real world doesn\u2019t work like that. There can be polynomial regression\ny = w0 + w1x + w2x2 + . . . wnxn\nMultiple Linear Regression, where instead of x being a single feature it is a vector of\nfeatures x = (x1, x2, x3, x4, . . .)T\ny = w0 + w1x1 + w2x2 + w3x3 + . . .",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 48,
      "start_char": 18668,
      "end_char": 19185,
      "chunk_index": 43,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_44",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": ". .)T\ny = w0 + w1x1 + w2x2 + w3x3 + . . . We will look at those in some detail later, but keep in mind the weights (w\u2019s) are still\nlinear in both instances. We can also write the weights as a vector\nw = (w0, w1, w2, w3, w4, . . .)T\nWhat you know about Simple Linear Regression can be expanded to the other cases. Donny Hurley\nRegression\n49 / 55\n\nErrors\nThe first example I put up, had all the data points (training data) fall exactly on the\nline. This is not how it works really.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 49,
      "start_char": 19144,
      "end_char": 19623,
      "chunk_index": 44,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_45",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "This is not how it works really. The prediction is not going to be exactly the same\nas what we fed into our model to train. Think of the example I put up about the\nFriends vs Daily Minutes Online\nExample\nSo there is some amount of error on the model vs the real data. Donny Hurley\nRegression\n50 / 55\n\nSimple Linear Regression\nThe function will make a prediction for each observed data point. The observation is denoted by y and the prediction is denoted by \u02c6y.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 50,
      "start_char": 19591,
      "end_char": 20051,
      "chunk_index": 45,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_46",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Here\u2019s an idea, let\u2019s find the line that has the smallest amount of error and that will be\nthe model we want! Donny Hurley\nRegression\n51 / 55\n\nTraining the Model\nTraining the model means, find the best line that minimises the error for our training\nsample. This depends on the Loss function. Measuring the difference between what the model predicts the value should be and\nwhat the training sample actually was.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 51,
      "start_char": 20051,
      "end_char": 20462,
      "chunk_index": 46,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_47",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "There are multiple loss functions to choose and choosing the wrong one can be like\nchoosing the wrong type of model. Typical example is the mean squared error\nL = 1\n2m\nm\nX\ni=0\n(\u02c6yi \u2212yi)2\nwhere m is the number of samples, \u02c6yi is the prediction for the i-th sample and yi is the\ncorrect answer for that sample. Note : the\n1\n2m is just chosen for convenience as the Maths will look \u201dnicer\u201d. Even still\nminimising L without the fraction will give us the same values for w.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 52,
      "start_char": 20462,
      "end_char": 20930,
      "chunk_index": 47,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_48",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Donny Hurley\nRegression\n52 / 55\n\nTraining the model\nSo what we want are the particular parameters (say w0 and w1 in the simplest case\nwe\u2019ve seen) that give us the smallest value for L. This is actually quite easy for the linear regression examples we\u2019ve had so far, there\u2019s a\nknown formula, or we can use some algebra. But we want the idea to work for much more complicated models too, and with more\nparameters. The idea does work in further ML algorithms, get the parameters w0,w1,w2,. . .",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 52,
      "start_char": 20930,
      "end_char": 21420,
      "chunk_index": 48,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_49",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": ". . that\ngive us the smallest L, just the technique for finding the parameters might take a bit\nlonger in more complicated models. This is even true of deep learning (Neural\nNetworks). Usually we use a method called gradient descent to solve this (later!), that does\nmultiple iterations to find the best parameters. When we use the .fit() methods in\nscikit-learn and tensorflow this is what they do.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 53,
      "start_char": 21417,
      "end_char": 21816,
      "chunk_index": 49,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_50",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "Donny Hurley\nRegression\n53 / 55\n\nExtrapolation\nExtrapolation is a type of estimation, beyond the original observation range, the\nvalue of a variable on the basis of its relationship with another variable. Interpolation, which produces estimates between known observations\nExtrapolation is subject to greater uncertainty and a higher risk of producing\nmeaningless results. Donny Hurley\nRegression\n54 / 55\n\nBuilding a model\nWe have a set of data, that is a pair of information. We know the correct answer for\nthe set.",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 53,
      "start_char": 21816,
      "end_char": 22331,
      "chunk_index": 50,
      "total_chunks": 52
    },
    {
      "chunk_id": "db87e2885e77b83300b8835d423e201c_chunk_51",
      "doc_id": "db87e2885e77b83300b8835d423e201c",
      "content": "We know the correct answer for\nthe set. X is the set of all inputs (a matrix). y is the set of all corresponding response variables. A particular yi is the result for a particular xi. from\nsklearn\nimport\nlinear_model\nlr = linear_model . LinearRegression ()\nlr.fit(X, y)\npred = lr.predict(X)\nDonny Hurley\nRegression\n55 / 55",
      "source_doc_title": "Machine Learning Technologies Week 1 Introduction",
      "section_id": null,
      "page_number": 55,
      "start_char": 22292,
      "end_char": 22614,
      "chunk_index": 51,
      "total_chunks": 52
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_0",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Machine Learning Technologies\nGeneralisation\nValidation\nDonny Hurley\nATU Galway\nDonny Hurley\nGeneralisation\n1 / 46\n\nGeneralisation\nGeneralisation is the goal of ML\nWant good performance for new data\nHow good the generalisation is, is how well it performs on previously \u201cunseen\ndata\u201d i.e. data not used to train the set. Definition\nGeneralisation is the model\u2019s ability to give sensible outputs to sets of input that it has\nnever seen before.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 441,
      "chunk_index": 0,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_1",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Root-mean-squared (RMS) error could be used to measure: ERMS =\np\n2E(w\u2217)/N\nBack to previous idea of checking how far away pred is away from the original y, what\nis the problem? We are checking the Generalisation of the model by using data it was trained on, i.e. it\nis NOT previously unseen. Our model may be overfit to the training data. Donny Hurley\nGeneralisation\n2 / 46\n\nOverfitting\nSo you chose a model, and did your training and reduced the training loss to 0. Great right??? No!",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 2,
      "start_char": 441,
      "end_char": 925,
      "chunk_index": 1,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_2",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Great right??? No! This is the same as a student learning off the answers at that back of the book. They may still do well in the exam but if they are given different questions,\nthey\u2019re in trouble. They have not generalised their learning, they have memorised specific examples. In Machine Learning this is called Over-fitting to the data. Donny Hurley\nGeneralisation\n3 / 46\n\nUnderfitting\nThe opposite problem is under-fitting. You can\u2019t even seem to reduce your training loss to anything reasonable.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 3,
      "start_char": 907,
      "end_char": 1407,
      "chunk_index": 2,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_3",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "This is like the student not being smart enough to do the exercises. In machine learning this is generally a symptom of not having a complex enough\nmodel (Student not smart enough). Students not being smart enough is unlikely in the real world, as each of us is\ngiven a complex model to work with (The Brain). Other causes of under-fitting in machine learning are poor training mechanisms or\nan inappropriate loss function.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 4,
      "start_char": 1407,
      "end_char": 1830,
      "chunk_index": 3,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_4",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "This is more like the student who sits in the library all day long - looking at social\nmedia and can\u2019t figure out why they still can\u2019t do the examples. Donny Hurley\nGeneralisation\n4 / 46\n\nUnderfit/overfit\nDefinition\nUnderfitting occurs when a statistical model or machine learning algorithm cannot\ncapture the underlying trend of the data. Intuitively, underfitting occurs when the model or the algorithm does not fit the data\nwell enough. Underfitting is often a result of an excessively simple model.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 4,
      "start_char": 1830,
      "end_char": 2332,
      "chunk_index": 4,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_5",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Definition\nOverfitting occurs when a statistical model or machine learning algorithm captures the\nnoise of the data. Intuitively, overfitting occurs when the model or the algorithm fits\nthe training data too well. Overfitting is often a result of an excessively complicated model. Donny Hurley\nGeneralisation\n5 / 46\n\nTraining, Validation, and Test Sets\nTraining set: used to fit model parameters. Validation set: used during model development for hyperparameter tuning and\nmodel selection.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 5,
      "start_char": 2332,
      "end_char": 2821,
      "chunk_index": 5,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_6",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Test set: used once at the end to estimate generalisation performance. Using the test set during model tuning or selection leads to optimistic and misleading\nperformance estimates. Donny Hurley\nGeneralisation\n6 / 46\n\nDataset Splits (Diagram)\nTraining\nValidation\nTest\nDonny Hurley\nGeneralisation\n7 / 46\n\nBuilding model\nIt does not matter what type of models we are building the process is similar. Have a set of data. Pick base form of models to use.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 6,
      "start_char": 2821,
      "end_char": 3270,
      "chunk_index": 6,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_7",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Pick base form of models to use. Train the data on those models\nIf you are trying to choose precisely which model to use - this is where to do it\nEven if you are comparing different model types (Regression vs NN vs Random\nForest etc.) this is the stage to make the decisions. (Using a validation set!)\nTest the final chosen model. Note: no matter how good the model, almost always the more datapoints the better.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 8,
      "start_char": 3238,
      "end_char": 3650,
      "chunk_index": 7,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_8",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "(The more tuples of x, y the better, having too many independent variables/features\ncan cause issues.)\nDonny Hurley\nGeneralisation\n8 / 46\n\nBuilding a model\nWe have a set of data, that is a pair of information. We know the correct answer for\nthe set. X is the set of all inputs (a matrix). y is the set of all corresponding response variables. A particular y(i) is the result for a particular x(i). 1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 8,
      "start_char": 3650,
      "end_char": 4070,
      "chunk_index": 8,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_9",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "1 from\ns k l e a r n . l i n e a r m o d e l\nimport\nL i n e a r R e g r e s s i o n\n2\n3 l r = L i n e a r R e g r e s s i o n ()\n4 l r . f i t (X,\ny )\n5 pred = l r . p r e d i c t (X)\nDonny Hurley\nGeneralisation\n9 / 46\n\nTesting the performance of the model\n1 l r . f i t (X,\ny )\n2 pred = l r . p r e d i c t (X)\nIn the code, we have created a model called lr. lr was trained on the set of known information.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 9,
      "start_char": 4048,
      "end_char": 4455,
      "chunk_index": 9,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_10",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "lr was trained on the set of known information. Recall from regression, the predicted values are not exactly the same as what is\ninputted\nSo pred is not the exact same as y but they hopefully are close. So we could test performance by checking how far away pred is away from the\noriginal y\nDoes anyone see a problem with this? Donny Hurley\nGeneralisation\n10 / 46\n\nSo how do we actually test our model\nWe use a test set of data, that we know the answers.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 10,
      "start_char": 4408,
      "end_char": 4861,
      "chunk_index": 10,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_11",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "It does not matter if the response variable is a number (regression) or a quality\n(category/classifier), it\u2019s a similar idea. We may use different formulae to determine accuracy and performance. Our test set is kept separate from the training set. When it is used to check performance of the model, it is brand new information to the\nmodel. (while we still know the correct answers)\n1 l r . f i t ( X train ,\ny t r a i n )\n2 pred = l r .",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 11,
      "start_char": 4861,
      "end_char": 5298,
      "chunk_index": 11,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_12",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "f i t ( X train ,\ny t r a i n )\n2 pred = l r . p r e d i c t ( X t est )\n3 #Compare pred\nwith\ny t e s t\nCompare pred with the known results for the test set and it will give allow us to check\nperformance. Donny Hurley\nGeneralisation\n11 / 46\n\nTest Sets\nSometimes when trying to solve a problem using you are given an explicit Training Set\nand a separate Test Set. The Training set is used to fit the model (learn the parameters). The Test set is used to Evaluate the model (check its performance/generalisation).",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 11,
      "start_char": 5252,
      "end_char": 5763,
      "chunk_index": 12,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_13",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "This is fine and you use them that way. However, often you are just given one set of\ndata and it\u2019s your job to decide the best method to fit and train with the large set of\ndata.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 12,
      "start_char": 5763,
      "end_char": 5941,
      "chunk_index": 13,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_14",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n12 / 46\n\nTest Set\nThis is very very important\nThe Test Set is Only For the Final Eval-\nuation at the end after you have made\nall your decisions\nIf the Test Set is used for any decision\nmaking, it is now contaminated\nEven if it seems your lecturer is using\nthe test set in lectures/lab books, it is\nstill wrong to use it to make any choices\nin model selection.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 12,
      "start_char": 5941,
      "end_char": 6328,
      "chunk_index": 14,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_15",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n13 / 46\n\nTest Set\nSome Important Notes About the Test Set\nYou need to guard the test set carefully, from yourself, and use it only sparingly. The Test set should be kept apart and almost never used. It must not be used in trying to choose any hyper-parameter (polynomial, I\u2019m\ncoming to this) for a model\nIf it was used in selection, it would no longer be unseen data.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 13,
      "start_char": 6328,
      "end_char": 6723,
      "chunk_index": 15,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_16",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "It should never be used when trying decide between say a SVM or a Random\nForest model\nAs again, it would no longer be unseen data when making a choice. Use it only for overall evaluation. ML testing competitions keep the test set hidden and private. Donny Hurley\nGeneralisation\n14 / 46\n\nTrain/test Split\nLet\u2019s say we start with a big set of data that we have collected. This is a sample of\nsize m.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 14,
      "start_char": 6723,
      "end_char": 7120,
      "chunk_index": 16,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_17",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "This is a sample of\nsize m. For each item i in the data we know the x(i) vector (features/independent\nvariables)\nWe also know the equivalent y(i) response. We split the data into two sets, a training set and a test set\nAlmost always the training set is going to be significantly larger than the test set. 80/20, 75/25 are often good splits. Randomly select which items are in the training set and which are in the test set. Very Important. Fit the model on the training set. Evaluate the model on the test set.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 15,
      "start_char": 7093,
      "end_char": 7603,
      "chunk_index": 17,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_18",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Evaluate the model on the test set. Donny Hurley\nGeneralisation\n15 / 46\n\nTrain/test Split\n1 from\ns k l e a r n . m o d e l s e l e c t i o n\nimport\nt r a i n\nt e s t\ns p l i t\nis a method built into sklearn to split a dataset into training and test sets randomly.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 15,
      "start_char": 7568,
      "end_char": 7831,
      "chunk_index": 18,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_19",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "1 X train ,\nX test ,\ny t r a i n ,\ny t e s t = t r a i n\nt e s t\ns p l i t (X,\ny ,\nt e s t s i z e\n=0.25)\nwill make a test size of 25% of the data (which is the default anyway)\n1 X train ,\nX test ,\ny t r a i n ,\ny t e s t = t r a i n\nt e s t\ns p l i t (X,\ny ,\nt r a i n s i z e\n=0.8)\ncan specify it that way too if you prefer. The return values will be numpy arrays.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 16,
      "start_char": 7831,
      "end_char": 8197,
      "chunk_index": 19,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_20",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "The return values will be numpy arrays. Donny Hurley\nGeneralisation\n16 / 46\n\nFull model procedure\nDonny Hurley\nGeneralisation\n17 / 46\n\nDeciding the complexity of the model\nWhen dealing with relatively simple models, (that take only minutes-hours to\ntrain), then you can afford to try many different model sizes. But you can\u2019t just pick the one that gets the lowest training error, we\u2019ve already\nshown that can lead us wrong.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 16,
      "start_char": 8158,
      "end_char": 8582,
      "chunk_index": 20,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_21",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Instead we need a mechanism to check our loss on data that the training\nalgorithm doesn\u2019t see. The technique we will be looking at is called Hyper-parameter tuning. Donny Hurley\nGeneralisation\n18 / 46\n\nHyper Parameters\nDefinition\nHyper-parameters are parameters that are chosen rather than learned. Examples of Hyper-parameters are:\nthe degree of your model, e.g. linear, quadratic, n-degree polynomial. Which features to use in your model?",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 18,
      "start_char": 8582,
      "end_char": 9022,
      "chunk_index": 21,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_22",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Which features to use in your model? (Is every bit of information needed?)\nThe Learning rate \u03b1\nAmount of Regularisation\nThe number of layers/neurons in a Neural Network\nDonny Hurley\nGeneralisation\n19 / 46\n\nPolynomial Curve Fitting\nWhat form is y(x)? Try polynomials of degree M. y(x, w) = w0 + w1x + w2x2 + . . . + wMxM\nThis is the hypothesis space. Note w is a vector (that\u2019s why bold). w = (w0, w1, . . . , wM)\nHow do we measure success?",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 19,
      "start_char": 8986,
      "end_char": 9425,
      "chunk_index": 22,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_23",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "w = (w0, w1, . . . , wM)\nHow do we measure success? Sum of squared errors\nE(w) = 1\n2\nN\nX\nn=1\n[y(xn, w) \u2212tn]2\nor RMS\nAmong functions in the class, choose that which minimises this error. The degree of polynomial is a hyperparameter. We really want the degree that captures the trend of the data best. Donny Hurley\nGeneralisation\n20 / 46\n\nWhich degree of Polynomial? The green line is the \u201dideal\u201d model, the red line is the shape of the model built.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 20,
      "start_char": 9374,
      "end_char": 9821,
      "chunk_index": 23,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_24",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "If M = 9 then E(w\u2217) = 0: This is over-fitting\nDonny Hurley\nGeneralisation\n21 / 46\n\nPolynomial regression \u2013 overfitting vs underfitting\nOne of the biggest problems is either overfitting the model or underfitting the model\nOverfitting means the model is too sensitive to the variables you used to \u201dtrain\u201d\nthe model\nUnderfitting means the opposite, two few terms.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 21,
      "start_char": 9821,
      "end_char": 10181,
      "chunk_index": 24,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_25",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n22 / 46\n\nBetter picture\nBe aware of these possibilities when creating a regression model\nDonny Hurley\nGeneralisation\n23 / 46\n\nRMS\ny(x, w) = w0 + w1x + w2x2 + . . . + wMxM\nHere is the root-mean-squared for each M\n1 Seeing the performance of the training set vs the model (known information)\n2 Seeing the performance of the test set vs the model (previously unseen)\nThe point is, the model performs very very well at M = 9 for the training set.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 22,
      "start_char": 10181,
      "end_char": 10651,
      "chunk_index": 25,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_26",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "It\nperforms very very poorly at M = 9 for the test set. i.e. it is overfit at M = 9. Donny Hurley\nGeneralisation\n24 / 46\n\nSelecting the HyperParameter\nSo we need to have some method of selecting the hyperparameters. Note: There can be many different methods for selecting hyperparameters that can be\nquite complicated. In this level 8 course, I will only demonstrate one real method. I\nwill take shortcuts a little. In the future, if you go into this area, you will have more\nlearning to do.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 24,
      "start_char": 10651,
      "end_char": 11142,
      "chunk_index": 26,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_27",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n25 / 46\n\nValidation Set\nKeeping the Test Data separate:\nSplit training data into training set and validation set (70/30 split is often used)\nTrain different models (e.g. diff. order polynomials) on training set\nIterate through all possible hyper-parameters (e.g.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 25,
      "start_char": 11142,
      "end_char": 11432,
      "chunk_index": 27,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_28",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "polynomial M=1, then M=2,\nthen M=3)\n1\nFix the hyperparameter (M=1 to start)\n2\nTrain a model using the training set\n3\nValidate the model using the validation set\n4\nStore the result for that hyperparameter\n5\nRepeat from step 1 with the next hyperparameter\nChoose model (e.g. order of polynomial) with minimum error on validation set\n(not always done but often) Retrain the model on the complete training data\n(training set + validation set).",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 26,
      "start_char": 11432,
      "end_char": 11871,
      "chunk_index": 28,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_29",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n26 / 46\n\nTest Error/loss\nNow we need to be a little careful here. Hyperparameters are parameters of the model that are not trained, i.e. we choose\nthem. As we can see though with hyper-parameter tuning, we are not exactly training\nthem but we are not choosing them either. The tuning that takes place is in some way using the validation set data. It may not see the data directly but it is getting a signal from it. In effect the validation data is compromised.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 26,
      "start_char": 11871,
      "end_char": 12360,
      "chunk_index": 29,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_30",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "In effect the validation data is compromised. It doesn\u2019t give us a true picture of the generalisation of the algorithm. For this we need a test set so that we can calculate a test error, that is only used\nafter we have chosen our final model. Donny Hurley\nGeneralisation\n27 / 46\n\nGetting a Validation Set\nIf we have a lot of data, we can go with the above and just use a static validation set\nto choose our model.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 27,
      "start_char": 12315,
      "end_char": 12728,
      "chunk_index": 30,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_31",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "train test split can be used a second time here\u2019s an example:\n1 X train ,\nX test ,\ny t r a i n ,\ny t e s t = t r a i n\nt e s t\ns p l i t (X,\ny ,\nt e s t s i z e\n=0.2)\n2\n3 X train ,\nX val ,\ny t r a i n ,\ny v a l = t r a i n\nt e s t\ns p l i t ( X train ,\ny t r a i n ,\nt e s t s i z e =0.1)\n4\n5 # Or could\ndo\nX train sm ,\nX val ,\ny train sm ,\ny v a l = t r a i n\nt e s t\ns p l i t (\nX train ,\ny t r a i n ,\nt e s t s i z e =0.1)\nAt each step train the model using X train\nValidate each hyperparameter using X val\nMake the choice based off that\nRetrain the model using both X train and X val together\nEvaluate the model using X test\nDonny Hurley\nGeneralisation\n28 / 46\n\nValidation vs Testing\nYou choose your model using validation (even comparing different algorithm types, use\nvalidation).",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 28,
      "start_char": 12728,
      "end_char": 13515,
      "chunk_index": 31,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_32",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "You can evaluate your final model with the test set. Donny Hurley\nGeneralisation\n29 / 46\n\nCross-validation\nUsing a static Validation Set can introduce Bias. What if the validation set is not\nrepresentative? If we have limited data this is more likely to occur.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 29,
      "start_char": 13515,
      "end_char": 13775,
      "chunk_index": 32,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_33",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "The idea of Cross-Validation is to do multiple runs at validation\nCross-validation creates S groups of data, use S \u22121 to train, other to validate\nDo a number of runs with these different groups (depending on Cross-Validation\nmethod chosen)\nAverage over all the scores for the different training/validation splits\nChoose the hyperparameter that has the lowest average over all runs\nThere are many types of cross-validation as time is limited, here\u2019s one\nDonny Hurley\nGeneralisation\n30 / 46\n\nk-fold Cross Validation\nRandomly split your entire dataset into k\u201dfolds\u201d\nFor each k-fold in your dataset, build your model on k \u2013 1 folds of the dataset.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 30,
      "start_char": 13775,
      "end_char": 14418,
      "chunk_index": 33,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_34",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Then, evaluate the model to check the effectiveness for kth fold\nRecord the error you see on each of the predictions\nRepeat this until each of the k-folds has served as the validation set\nThe average of your k recorded errors is called the cross-validation error and will\nserve as your performance metric for the model\n(5 and 10 are usually often good choices for the number of folds - the default in\nsklearn is 5)\nNote\nThis is not feasible for more complicated models like Deep Learning, for those you have\nto just use a static validation set.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 31,
      "start_char": 14418,
      "end_char": 14962,
      "chunk_index": 34,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_35",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n31 / 46\n\n10-fold Cross Validation\nDonny Hurley\nGeneralisation\n32 / 46\n\nFinal procedure yet? Keeping the Test Data separate:\nIterate through all possible hyper-parameters (e.g. polynomial M=1, then M=2,\nthen M=3)\n1\nFix the hyperparameter (M=1 to start)\n2\nRandomly split the training data in k-folds\n1\nTrain a model using k \u22121 folds of the dataset. 2\nEvaluate the model using the remaining fold. 3\nRecord the error. 4\nRepeat until all folds have been a validation set.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 31,
      "start_char": 14962,
      "end_char": 15456,
      "chunk_index": 35,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_36",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "3\nTake the average error of all folds and store that result for the particular\nhyperparameter\n4\nRepeat from step 1 with the next hyperparameter\nChoose model (e.g. order of polynomial) with minimum (average) error on the\nfolds\n(not always done but often) Retrain the model on the complete training data\n(training set + validation set). You choose your model using validation, you can evaluate your final model with the\ntest set.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 33,
      "start_char": 15456,
      "end_char": 15883,
      "chunk_index": 36,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_37",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n33 / 46\n\nA procedure\n1 Split your labelled/known results into training set and test set\n2 Select your base model (linear regression, nearest neighbours etc.)\n3 Tune the hyper-parameters of the model using Cross Validation\n4 Train the model (with chosen parameters) with the training set\n5 Compare the different types of models using some form of validation - make final\ndecision.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 33,
      "start_char": 15883,
      "end_char": 16290,
      "chunk_index": 37,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_38",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "6 Evaluate the final model using the test set\nDonny Hurley\nGeneralisation\n34 / 46\n\nMultiple Hyperparameters\nIt is clear to see that if you have multiple hyper-parameters, it is difficult to\nchoose the optimum for each as you have to hold all other hyper-parameters\nsteady while you test one of them. What value do you hold them steady at? Well you have to try with every combination.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 34,
      "start_char": 16290,
      "end_char": 16673,
      "chunk_index": 38,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_39",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Well you have to try with every combination. (GridSearchCV can help with this\nand we\u2019ll see it later)\nWhen dealing with large data sets that take weeks to train you can see that this\nbecomes in-feasible. With small data sets in can be difficult to split into train-val-test and still have\nenough training data. There is no perfect solution. Donny Hurley\nGeneralisation\n35 / 46\n\nBias/variance\nWhenever we discuss model prediction, it\u2019s important to understand prediction errors.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 35,
      "start_char": 16629,
      "end_char": 17106,
      "chunk_index": 39,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_40",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "There is a tradeoff between a model\u2019s ability to minimise bias and variance. Definition\nBias is the difference between the average prediction of our model and the correct\nvalue which we are trying to predict. Model with high bias pays very little attention to\nthe training data and oversimplifies the model. It always leads to high error on training\nand test data. Definition\nVariance is the variability of model prediction for a given data point or a value which\ntells us spread of our data.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 36,
      "start_char": 17106,
      "end_char": 17598,
      "chunk_index": 40,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_41",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Model with high variance pays a lot of attention to training\ndata and does not generalise on the data which it hasn\u2019t seen before. As a result, such\nmodels perform very well on training data but have high error rates on test data. Donny Hurley\nGeneralisation\n36 / 46\n\nBias\u2013Variance Trade-off (Diagram)\nModel Complexity\nError\nBias\nVariance\nDonny Hurley\nGeneralisation\n37 / 46\n\nOverfit/underfit\nOverfitting is the case where the overall cost is really small, but the generalisation of\nthe model is unreliable.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 36,
      "start_char": 17598,
      "end_char": 18105,
      "chunk_index": 41,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_42",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "This is due to the model learning \u201dtoo much\u201d from the\ntraining data set. (low bias, high variance)\nUnderfitting is the case where the model has \u201dnot learned enough\u201d from the training\ndata, resulting in low generalisation and unreliable predictions. (high bias, low variance)\nBias-variance trade-off. What is the right measure? Depending on the model at hand, a performance that lies between overfitting and\nunderfitting is more desirable. This trade-off is the most integral aspect of Machine Learning model training.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 38,
      "start_char": 18105,
      "end_char": 18622,
      "chunk_index": 42,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_43",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Machine Learning models fulfil their purpose when they generalise well. Generalisation is bound by the two undesirable outcomes \u2014 high bias and high\nvariance. Detecting whether the model suffers from either one is the sole responsibility of\nthe model developer. Donny Hurley\nGeneralisation\n38 / 46\n\nConfidence\nOne of the major problems with over-fitting is that it does not just mis-classify\nnew data but it can often do so with very high confidence.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 38,
      "start_char": 18622,
      "end_char": 19072,
      "chunk_index": 43,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_44",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "When the model is of more modest complexity it may have certain samples in the\ntraining set that end up on the wrong side of the decision boundary\n(classification) or are a long way from the model (regression). However the model can report this. The model can say, I predict this but I am only x% sure (classification) or there is\nx-tolerance with this prediction (regression).",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 39,
      "start_char": 19072,
      "end_char": 19449,
      "chunk_index": 44,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_45",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n39 / 46\n\nSeeing Overfitting/underfitting\nOften people compare the score for the training set vs the score for the validation set\nto see if either of these are happening\n1 l r . f i t ( X train ,\ny t r a i n )\n2\n3 t r a i n s c o r e = l r . s c o r e ( X train ,\ny t r a i n )\n4 v a l s c o r e = l r . s c o r e ( X val ,\ny v a l )\nIf the training score is very poor, then the model is underfit\nAlmost always the training score will be better than the validation score.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 39,
      "start_char": 19449,
      "end_char": 19947,
      "chunk_index": 45,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_46",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "If there is a large difference between training score and validation score (training\nscore much better), than the model is overfit\nDonny Hurley\nGeneralisation\n40 / 46\n\nDealing with over/underfitting\nIt is a constant issue but there are some common techniques\nFor under-fitting you either need to train for longer or use a more complex model. It\nmay also require you to look at your training mechanisms and loss function. Are they\nappropriate to the problem? For over-fitting you could:\nMake the model less complex.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 40,
      "start_char": 19947,
      "end_char": 20461,
      "chunk_index": 46,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_47",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Decrease the number of features. Particularly if there is a large number of\nfeatures compared to the number of training samples. But you are also throwing\naway information with this method. Choose wisely what to throw out. Get more data. Often use regularisation (later we\u2019ll talk about this). Donny Hurley\nGeneralisation\n41 / 46\n\nRegularisation\nAnother term that applies in general to \u201cmost\u201d Machine Learning systems is the idea\nof Regularisation.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 41,
      "start_char": 20461,
      "end_char": 20909,
      "chunk_index": 47,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_48",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Definition\nRegularisation is a technique used for tuning the function by adding an additional\npenalty term in the error function. The additional term controls the excessively\nfluctuating function such that the coefficients don\u2019t take extreme values. This technique of keeping a check or reducing the value of error coefficients are called\nshrinkage methods or weight decay in case of neural networks.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 42,
      "start_char": 20909,
      "end_char": 21309,
      "chunk_index": 48,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_49",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Donny Hurley\nGeneralisation\n42 / 46\n\nRegularisation\nSome Points:\nRegularisation is a more general way of avoiding overfitting. It works be adding a term to the loss function which adds loss if the model is not\nof a preferred type. In particular we don\u2019t want the weights to become very large. A large weight can change by a large amount very quickly. As differences in the magnitude of weights can quickly spiral out of control and\nindividual weights influence can quickly exceed their importance.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 42,
      "start_char": 21309,
      "end_char": 21806,
      "chunk_index": 49,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_50",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "For this reason we add a penalty to the loss function that is related to the size of\nthe weights. Donny Hurley\nGeneralisation\n43 / 46\n\nMean Squared Error\nMSE(w) = 1\n2m\nm\nX\ni=1\n(y(i) \u2212fw(x(i)))2\nIt can happen that when \u201coptimising\u201d this Loss function, some of the weights\nmay become very large. That means the feature corresponding to these large weights will \u201cincrease\u201d in\ntheir importance\nThis can cause overfitting. So what can we do with these large weights?",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 43,
      "start_char": 21806,
      "end_char": 22267,
      "chunk_index": 50,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_51",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "So what can we do with these large weights? Donny Hurley\nGeneralisation\n44 / 46\n\nMean Squared Error - l2 (Ridge Regression)\nWe add a regularisation term (\u03bb) to our Loss function that penalises large weights\nL(w) = MSE(w) + \u03bb\nn\nX\nj=1\nw2\nj\nThis means that if any particular wj is large, the overall Loss will be large too. This is known as the L2 norm. (squared magnitude)\nIt does not matter if we are using MSE or any other loss function, we add on the\nregularisation term to the end.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 44,
      "start_char": 22224,
      "end_char": 22707,
      "chunk_index": 51,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_52",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Here is the MSE with regularisation fully written out\nL(w) = 1\n2m\nm\nX\ni=1\n(y(i) \u2212fw(x(i)))2 + \u03bb\nn\nX\nj=1\nw2\nj\n(some places may write this slightly differently), where n is the number of features and\nm is sample size. Notice we do not include the bias term (w0)\nDonny Hurley\nGeneralisation\n45 / 46\n\nL2 Regularisation (Regularization)\nThe effect of regularisation is to make it so the model prefers to learn small weights,\nall other things being equal.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 45,
      "start_char": 22707,
      "end_char": 23156,
      "chunk_index": 52,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_53",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Large weights will only be allowed if they considerably improve the first part of the cost\nfunction. The relative importance of the two elements of the compromise depends on the value\nof \u03bb:\nwhen \u03bb is small we prefer to minimize the original cost function\nwhen \u03bb is large we prefer small weights. Large \u03bb moves in the direction of underfitting, and small \u03bb moves in the direction of\noverfitting. So it is another hyperparameter that we have to choose - perhaps via cross-validation.",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 46,
      "start_char": 23156,
      "end_char": 23637,
      "chunk_index": 53,
      "total_chunks": 55
    },
    {
      "chunk_id": "7e13d7ec9c09fa3e6c40af0df589fc25_chunk_54",
      "doc_id": "7e13d7ec9c09fa3e6c40af0df589fc25",
      "content": "Additionally Gradient Descent applies just as it does without regularisation, it just has\nan extra term when calculating the updates/gradients. Donny Hurley\nGeneralisation\n46 / 46",
      "source_doc_title": "Machine Learning Technologies Generalisation Validation",
      "section_id": null,
      "page_number": 46,
      "start_char": 23637,
      "end_char": 23816,
      "chunk_index": 54,
      "total_chunks": 55
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_0",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Machine Learning Technologies\nClassification\nLogistic Regression\nDonny Hurley\nATU Galway\nDonny Hurley\nClassification\n1 / 24\n\nClassification\nNow some different problems:\nWith regression, we assumed we were taking input and producing a quantity: what\nnumber (continuous value) do we think y will be (close to) for a particular input? Many ML problems have a more limited set of outputs. Often the question is\nmore like: which group is this input (mostly likely) part of? These are classification problems.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 503,
      "chunk_index": 0,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_1",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "These are classification problems. Our outputs will be restricted to the possible classes for our problem. i.e. for n\nsamples with K categories, we predict a vector of n values chosen from C1, C2, . . . , Ck. Our inputs will stay the same: an n \u00d7 m matrix of numbers (for n samples and m\nfeatures). Donny Hurley\nClassification\n2 / 24\n\nClassification\nSome problems that can be seen as classification:\nIs this email spam? Is this credit card charge fraud?",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 2,
      "start_char": 469,
      "end_char": 922,
      "chunk_index": 1,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_2",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Is this credit card charge fraud? (2 categories)\nWhat (English) word did the user say into the microphone? (\u2248470k categories)\nWhat day will ATU Galway open next year? (<<365 categories) Or is that\nregression? How many people are in this picture? Who are the people? Given an RGB colour value, what colour word describes it? Given weather observations, can you guess what city are we looking at? Donny Hurley\nClassification\n3 / 24\n\nModelling\nThere are many ways to model classification.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 3,
      "start_char": 889,
      "end_char": 1374,
      "chunk_index": 2,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_3",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "All do the same basic job:\n1\ntrain on known data\n2\nthen make predictions about correct category for other inputs. 1 model = SomeClassifierModel ( . . . . )\n2 model . f i t ( X train ,\ny t r a i n )\n3 y p r e d i c t e d = model . p r e d i c t ( X test )\n4 somehow compare results ( y t e s t ,\ny p r e d i c t e d )\n(Often use validation to pick the best form of the Classifier, previously used\n1 model = L i n e a r R e g r e s s i o n ( . . . .",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 4,
      "start_char": 1374,
      "end_char": 1821,
      "chunk_index": 3,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_4",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": ". . . )\nit all takes the same basic idea in code)\nDonny Hurley\nClassification\n4 / 24\n\nEvaluating Classification Performance\nFor evaluating Classification algorithms often people just see how accurate the\npredictions are on the test set\n1 y pred = model . p r e d i c t ( X test )\n2 from\ns k l e a r n . m e t r i c s\nimport\na c c u r a c y s c o r e\n3 a c c u r a c y s c o r e ( y t e s t ,\ny pred )\nWill give a score (< 1) which is the fraction of how many predictions were correct.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 4,
      "start_char": 1816,
      "end_char": 2300,
      "chunk_index": 4,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_5",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "This stores the y predictions separately (which may be useful later) but the model has\na method itself that can be used if you just want the accuracy score\n1 model . s c o r e ( X test ,\ny t e s t )\nThis one line does the predicting and comparing of predictions in the same step. We will come back to this later, about alternatives.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 5,
      "start_char": 2300,
      "end_char": 2632,
      "chunk_index": 5,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_6",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Donny Hurley\nMetrics\n5 / 24\n\nBinary Classification\nDefinition\nA Binary classification problem has:\nNegative Class\nPositive Class\ny \u2208[0, 1] or y \u2208[\u22121, 1]\nWe\u2019ll consider Multi-class problems later.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 5,
      "start_char": 2632,
      "end_char": 2827,
      "chunk_index": 6,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_7",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Some models can directly address multi-class\n(Na\u00a8\u0131ve Bayes, k-nearest neighbours) while some are inherently binary but can be\n\u201dextended\u201d (SVM, Logistic Regression)\nDonny Hurley\nBinary Classification\n6 / 24\n\nCategorical Response Variable\nMethods for use when the response variable (\u02c6y) has only two possible outcomes: a\ncustomer buys or does not buy a product, a patient lives or dies, or a candidate\naccepts or declines a job offer.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 6,
      "start_char": 2827,
      "end_char": 3259,
      "chunk_index": 7,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_8",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "In general, we call the two outcomes of the response variable \u201dsuccess\u201d and \u201dfailure\u201d\nand represent them by 1 and 0. The mean is then the proportion of 1s. Donny Hurley\nLogistic Regression\n7 / 24\n\nLogistic Regression\nTo start on classification, do we really need a different algorithm? Let\u2019s see if we can\n\u201dtweak\u201d what we know about regression. Note:\nLogistic Regression is a confusing name because we are not trying to predict a\ncontinuous value.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 7,
      "start_char": 3259,
      "end_char": 3706,
      "chunk_index": 8,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_9",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "We do predict a continuous value but we then threshold it to take on discrete\n(non-continuous) values. Logistic Regression, in the end, is actually a classification algorithm. Donny Hurley\nLogistic Regression\n8 / 24\n\nLogistic Regression\nDespite its name, Logistic Regression is a classification ML model. Something is either\nin category 0 or 1 and the predicted value is between 0 and 1. The predicted value is a\nprobability\nThe response variable y is either 0 or 1.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 8,
      "start_char": 3706,
      "end_char": 4172,
      "chunk_index": 9,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_10",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "By our standard formulae for regression you\nwill recall it is\n\u02c6y = w0 + w1x1\nThe estimation of \u02c6y has a good chance of being < 0 or > 1. We want an estimation\nfor p between 0 and 1. Donny Hurley\nLogistic Regression\n9 / 24\n\nLogistic Regression\nAdditionally we get a plot (only one fea-\nture) like\nThe picture clearly is not going to be\nvery accurate.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 9,
      "start_char": 4172,
      "end_char": 4521,
      "chunk_index": 10,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_11",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "We would much rather a\npicture like\nDonny Hurley\nLogistic Regression\n10 / 24\n\nModel for Logistic Regression\nSigmoid function, also called the logistic function, takes any real-valued number and\nmaps it to a value between 0 and 1.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 10,
      "start_char": 4521,
      "end_char": 4750,
      "chunk_index": 11,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_12",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "If the curve tends towards +\u221e, y goes towards 1\nand if it tends towards \u2212\u221e, y goes towards 0\nf (x) =\n1\n1 + e\u2212x\nNote: Sigmoid function has regularly\nbeen used in the past for Neural Net-\nworks, Deep Learning, the main one in\nthe past\nWe have the function that gives us the shape we want, how do we relate this to our\n\u02c6y = w0 + w1x?",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 11,
      "start_char": 4750,
      "end_char": 5080,
      "chunk_index": 12,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_13",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Donny Hurley\nLogistic Regression\n11 / 24\n\nModel for Logistic Regression\nDefinition\nLogit Function\nlogit(p) = log\n\u0012\np\n1 \u2212p\n\u0013\nwhere log is the natural logarithm (probably ln on your calculator, log with base e\n(exponential)). Inverse logit is the sigmoid function and\nexp(x)\n1 + exp(x) =\n1\n1 + exp(\u2212x)\nDonny Hurley\nLogistic Regression\n12 / 24\n\nLogistic Regression\nWe use the sigmoid function\n1\n1 + exp(\u2212x)\nto bind our values between 0 and 1, then (log means natural log)\nlog\n\u0012\ny\n1 \u2212y\n\u0013\n= w0 + w1x1 + . . . + wnxn.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 11,
      "start_char": 5080,
      "end_char": 5591,
      "chunk_index": 13,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_14",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": ". . + wnxn. where x describes the features of a datapoint. So\nfw(x) = \u02c6y =\n1\n1 + e\u2212(w0+w1x1+...+wnxn)\nDonny Hurley\nLogistic Regression\n13 / 24\n\nLogistic Regression\nDespite its name, Logistic Regression is a classification ML model. Something is either in category 0 or 1 and the predicted value is between 0 and 1.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 13,
      "start_char": 5580,
      "end_char": 5894,
      "chunk_index": 14,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_15",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "The\npredicted value is a probability\nDonny Hurley\nLogistic Regression\n14 / 24\n\nDecision Boundary\nWe expect our classifier to give us a set of outputs or classes based on probability\nwhen we pass the inputs through a prediction function and returns a probability score\nbetween 0 and 1. Example\nWe have 2 classes, cats and dogs (1 - dog , 0 - cats). We basically decide with a\nthreshold value above which we classify values into Class 1 and of the value goes below\nthe threshold then we classify it in Class 2.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 14,
      "start_char": 5894,
      "end_char": 6402,
      "chunk_index": 15,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_16",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Chosen the threshold as 0.5:\nif the\nprediction function returned a value of\n0.7 then we would classify this obser-\nvation as Class 1(DOG). If our predic-\ntion returned a value of 0.2 then we\nwould classify the observation as Class\n2(CAT). We can choose a different boundary to adjust for FP vs FN. Donny Hurley\nLogistic Regression\n15 / 24\n\nThreshold\nThe end result of our ML system (for a binary classifier) is either the number 0 or\n1.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 15,
      "start_char": 6402,
      "end_char": 6838,
      "chunk_index": 16,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_17",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "ML systems will often only output the class it thinks is most likely with this\nthreshold, i.e. if one side of decision boundary always output 0, if the other\noutput 1\nA lot of ML libraries that deal with Logistic Regression you can retrieve the\n\u201cprobability\u201d for its result, however, some may just output 0 or 1. Donny Hurley\nLogistic Regression\n16 / 24\n\nLogistic Regression\nClearly this is a binary classification system, so what happens if we have multiple\ncategories?",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 16,
      "start_char": 6838,
      "end_char": 7308,
      "chunk_index": 17,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_18",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "For now we\u2019ll just take the scikit learn method as \u201cmagic\u201d in how it deals with\nmultiple categories. I\u2019ll explain how this works later. A version of Logistic Regression is very often applied on the last layer of a neural\nnetwork so understanding it will help when we get to that later. Donny Hurley\nLogistic Regression\n17 / 24\n\nLoss Function\nTo train a logistic regression model, we have to figure out what weights (w) minimise\nthe cost/loss.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 17,
      "start_char": 7308,
      "end_char": 7750,
      "chunk_index": 18,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_19",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "We have to choose the cost function\nMean Squared Error performs very poorly for Logistic Regression\n1\n2m\nm\nX\ni=1\n\u0010\ny(i) \u2212fw(x(i))\n\u00112\nit would end up being a non-convex function with many local minimum (so difficult to\nfind the global minimum) instead use this Cost function\nL(fw(x), y) =\n(\n\u2212log (fw(x))\nif y = 1\n\u2212log (1 \u2212fw(x))\nif y = 0\nthe loss for one datapoint.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 18,
      "start_char": 7750,
      "end_char": 8114,
      "chunk_index": 19,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_20",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Donny Hurley\nLogistic Regression\n18 / 24\n\nLoss Function (Log-Loss)\nThis is not differentiable as stated so let\u2019s combine it to\nL(fw(x), y) = \u2212y log (fw(x)) \u2212(1 \u2212y) log (1 \u2212fw(x))\nOf course we need to do this over all samples. Therefore we use the following as our\nLoss Function for logistic regression\nL(w) = \u22121\nm\n\" m\nX\ni=1\n\u2212y(i) log\n\u0010\nfw(x(i))\n\u0011\n\u2212(1 \u2212y(i)) log\n\u0010\n1 \u2212fw(x(i))\n\u0011#\nThe function is convex which greatly simplifies minimisation.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 18,
      "start_char": 8114,
      "end_char": 8554,
      "chunk_index": 20,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_21",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "We need to find the\noptimum parameters w that minimises the loss on the training set. Then we can use\nthe model to predict new outputs on unseen data. The algorithm to solve the problem and find the w is the same as others and we\u2019ll,\nagain, see it later. Donny Hurley\nLogistic Regression\n19 / 24\n\nProgramming\n1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 19,
      "start_char": 8554,
      "end_char": 8885,
      "chunk_index": 21,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_22",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "l i n e a r m o d e l\nimport\nL o g i s t i c R e g r e s s i o n\nIt has some options, but we\u2019ll mostly just keep the default\n1 p e n a l t y\n:\n{ \u2019 l 1 \u2019 ,\n\u2019 l 2 \u2019 ,\n\u2019 e l a s t i c n e t \u2019 , None } ,\nd e f a u l t=\u2019 l 2 \u2019\nis a hyperparameter for it, the default is good - I\u2019m just pointing it out, I\u2019ll explain\nlater. 1 m ax i te r :\nint ,\nd e f a u l t =100\nI\u2019ve mentioned a few times about how the learning is doing things over and over again. Here we can control the max number of times it will do this process.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 20,
      "start_char": 8885,
      "end_char": 9399,
      "chunk_index": 22,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_23",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Sometimes you\nmight need more. Donny Hurley\nLogistic Regression\n20 / 24\n\nProgramming\nNow to talk about the outputs. Like LinearRegression it has\n1 model . c o e f\n2 model . i n t e r c e p t\n3 model . s c o r e ( X test ,\ny t e s t ) # Gives\nthe \u201d accuracy \u201d\ns c o r e\nof\nthe\nmodel\n4 y pred = model . p r e d i c t ( X test )\nA new one it has is\n1 model . c l a s s e s\nThis gives you a list of class labels known to the model. e.g. your y\u2019s are\n1 y = np .",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 20,
      "start_char": 9399,
      "end_char": 9855,
      "chunk_index": 23,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_24",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "e.g. your y\u2019s are\n1 y = np . a r r a y ( [ 0 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 0 , 2 , 1 ] ) #or\na pandas\ns e r i e s\ne q u i v a l e n t\nthen model.classes will be [0, 1, 2]. If your y\u2019s are\n1 y = np . a r r a y ( [ \u201d cat \u201d , \u201ddog\u201d , \u201d pikachu \u201d , \u201ddog\u201d , \u201ddog\u201d , \u201ddog\u201d , \u201d cat \u201d , \u201d pikachu \u201d ] )\nthen model.classes will be [\u201dcat\u201d,\u201ddog\u201d,\u201dpikachu\u201d] instead. Donny Hurley\nLogistic Regression\n21 / 24\n\nProgramming\nA new method it has is\n1 model .",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 21,
      "start_char": 9827,
      "end_char": 10268,
      "chunk_index": 24,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_25",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "p r e d i c t p r o b a ( X test )\nThis one gives probability estimates for all the classes. Maybe you want to know how\nconfident the model is in its predictions - but be aware, models can be over confident! The .predict I mentioned on the previous slide uses thresholds to make the decisions. You may also want the probabilities for some performance metrics when trying to make\ndecisions about models.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 22,
      "start_char": 10268,
      "end_char": 10670,
      "chunk_index": 25,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_26",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "Donny Hurley\nLogistic Regression\n22 / 24\n\nTransform Data\nSometimes the data we have is not in the best form for the model we are training, so\nwe need to change it. However\nWe don\u2019t alter the entire data set\nThis would mean any future data being evaluated by the model would also first\nhave to be transformed\nWe build the transform into the model using make pipeline\n1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 22,
      "start_char": 10670,
      "end_char": 11058,
      "chunk_index": 26,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_27",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "p i p e l i n e\nimport\nm a k e p i p e l i n e\n2\n3 model = m a k e p i p e l i n e (\n4\nMinMaxScaler () ,\n5\nL o g i s t i c R e g r e s s i o n ()\n6 )\n7 model . f i t ( X train ,\ny t r a i n )\n8 model . s c o r e ( X test ,\ny t e s t )\nAny data that goes into this pipeline has the first step: the data is transformed, then\nthe transformed data goes into the LogisticRegression step. We build the\ntransformation into the model. We can chain more things together in a pipeline.",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 23,
      "start_char": 11058,
      "end_char": 11533,
      "chunk_index": 27,
      "total_chunks": 29
    },
    {
      "chunk_id": "26f0f95a5aaadc44bd2e70497e156266_chunk_28",
      "doc_id": "26f0f95a5aaadc44bd2e70497e156266",
      "content": "We can chain more things together in a pipeline. Donny Hurley\nLogistic Regression\n23 / 24\n\nTransform Data\nThe example I had on the previous slide was MinMaxScaler which is a very commonly\nused transformation step, https://scikit-learn.org/stable/modules/\ngenerated/sklearn.preprocessing.MinMaxScaler.html. It makes all the data in the same range, by default the minimum value is changed to 0,\nthe maximum to 1 and everything else scaled accordingly. Donny Hurley\nLogistic Regression\n24 / 24",
      "source_doc_title": "Machine Learning Technologies Classification Logistic Regression",
      "section_id": null,
      "page_number": 23,
      "start_char": 11485,
      "end_char": 11975,
      "chunk_index": 28,
      "total_chunks": 29
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_0",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Machine Learning Technologies\nk-Nearest Neighbours\nMetrics\nDonny Hurley\nATU Galway\nDonny Hurley\nMetrics\n1 / 49\n\nLazy and Eager Learning\nLazy: wait for query before generalizing\nkNearest Neighbour, Case based reasoning\nEager: generalize before seeing query\nRadial basis function networks, ID3, Backpropagation, NaiveBayes, ... Does it matter?",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 341,
      "chunk_index": 0,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_1",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Does it matter? Eager learner must create global approximation, but Lazy learner can create many\nlocal approximations\nif they use same H, lazy can represent more complex fns (e.g., consider H =\nlinear functions)\nDonny Hurley\nkNN\n2 / 49\n\nk-Nearest Neighbours\nDonny Hurley\nkNN\n3 / 49\n\nkNN Basics\nkNN is basically a two-part algorithm\n1 Retrieve the k most similar recorded cases\n2 \u201cAverage\u201d across these k cases (regression)\ntake the max of k votes for ordinal types (classification)\nso kNN can be used for either regression or classification.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 2,
      "start_char": 326,
      "end_char": 867,
      "chunk_index": 1,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_2",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "However, let\u2019s ignore\nregression and stick to classification! Donny Hurley\nkNN\n4 / 49\n\nDistance\nMay require examining all instances\nDonny Hurley\nkNN\n5 / 49\n\nVoronoi Diagram\nWe typically need a notion of \u201ddistance\u201d to calculate the \u201dnearest\u201d neighbour. Often\nuse Euclidean distance i.e. (with p predictor values, t is a target and s is a source, t1 is\nthe first attribute for t, t2 etc. d(t, s) =\nq\n(t1 \u2212s1)2 + . . . + (tp \u2212sp)2\nCaution: Scale matters.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 4,
      "start_char": 867,
      "end_char": 1318,
      "chunk_index": 2,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_3",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": ". . + (tp \u2212sp)2\nCaution: Scale matters. If one predictor has is a much larger value than another, it will\ncontribute more to the distance measurement. Donny Hurley\nkNN\n6 / 49\n\nFormalising everything\n1 If you have a training set of data (x1, x2, . . . , xm). 2 Given a target xj. 3 Calculate all d(xj, xi) and select the k smallest ones. 4 Classify/Approximate xj based on the k nearest xi you found.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 6,
      "start_char": 1279,
      "end_char": 1678,
      "chunk_index": 3,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_4",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Donny Hurley\nkNN\n7 / 49\n\nNearest Neighbour\nWhat if the prediction is made by just looking at which training point is it closest to? Donny Hurley\nkNN\n8 / 49\n\nNearest Neighbour\nNot too bad. Looks like overfitting the training data. Donny Hurley\nkNN\n9 / 49\n\nNearest Neighbour\nWith the validation data, can see noise. Donny Hurley\nkNN\n10 / 49\n\nk Nearest Neighbour\nInstead of looking at the one nearest training point, we could look at k and have\na vote: a k-nearest neighbours classifier. e.g.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 7,
      "start_char": 1678,
      "end_char": 2167,
      "chunk_index": 4,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_5",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "e.g. with k = 5, we might want to predict a category for a point and find that the\nnearest five training points are green, green, red, green, red. Take the most\ncommon and predict green. Donny Hurley\nkNN\n11 / 49\n\nk Nearest Neighbour\nSmooths out some of the overfitting\nDonny Hurley\nkNN\n12 / 49\n\nsklearn\nOf course we want to do this by programming. It follows the exact same procedure as\nthe others. Model, fit, evaluate. 1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 11,
      "start_char": 2163,
      "end_char": 2606,
      "chunk_index": 5,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_6",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Model, fit, evaluate. 1 from\ns k l e a r n . neighbors\nimport\nK N e i g h b o r s C l a s s i f i e r\n2 model = K N e i g h b o r s C l a s s i f i e r ( n n e i g h b o r s =5)\n3 model . f i t ( X train ,\ny t r a i n )\n4 p r i n t ( model . s c o r e ( X val ,\ny v a l ) )\n5\n6 0.944\nwhich is quite a good result for such a simple technique\nDonny Hurley\nkNN\n13 / 49\n\nChoosing k\nThere\u2019s tradeoff. If k is too small, you\u2019ll overfit the training data. If it\u2019s too large,\nyou\u2019ll underfit reality. Any guesses?",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 13,
      "start_char": 2562,
      "end_char": 3067,
      "chunk_index": 6,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_7",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Any guesses? Cross-validation. Experiment. So you can use k-fold cross validation to choose the k in k-nearest neighbours....is that\nconfusing? They are two different k\u2019s!",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 14,
      "start_char": 3055,
      "end_char": 3226,
      "chunk_index": 7,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_8",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "They are two different k\u2019s! We are choosing the hyperparameter k for k-nearest neighbours and you apply say a\n10-fold cross validation procedure to choose it\nDonny Hurley\nkNN\n14 / 49\n\nkNN\nkNN can create a complicated boundary shape (both with\nKNeighborsClassifier(9)):\nDonny Hurley\nkNN\n15 / 49\n\nCurse of Dimensionality\nImagine instances described by 20 attributes, but only 2 are relevant to target\nfunction\nCurse of dimensionality: nearest nbr is easily mislead when highdimensional X\nHigh-dimensionality increases sparcity.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 14,
      "start_char": 3199,
      "end_char": 3724,
      "chunk_index": 8,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_9",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "The more features, the further the distance\naparts are going to be\nDonny Hurley\nkNN\n16 / 49\n\nRemark on Nearest Neighbours\nOne further remark on nearest neighbours\nSince the calculations are done when a new data point is attempting to be classified,\nthe model must store all the training information. (Lazy)\nStoring all the training information can be quite wasteful, better generalisation models\nwould not have to store the full training set.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 16,
      "start_char": 3724,
      "end_char": 4166,
      "chunk_index": 9,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_10",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "k-nearest neighbours is very quick to train (it does not do anything except store the\nset), however prediction is slow\nIt also will take up more memory than Eager models. Eager models just store the\nweights, Lazy ones store the entire dataset. Donny Hurley\nkNN\n17 / 49\n\nFurther dimensions\nIt\u2019s easy to show examples that are 2D points: they make nice figures and are\neasy to visualize, but they aren\u2019t so realistic.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 17,
      "start_char": 4166,
      "end_char": 4581,
      "chunk_index": 10,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_11",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Let\u2019s look at some more realistic data: there are lots of example data sets out\nthere for ML experimentation: mldata.org, UCI Machine Learning Repository,\nKDnuggets dataset list. Scikit-learn has some data sets built-in. (like the diabetes and iris datasets that\nwe\u2019ve seen)\nDonny Hurley\nkNN\n18 / 49\n\nSklearn datasets\n1 from\ns k l e a r n . d a t a s e t s\nimport\nl o a d b r e a s t c a n c e r\n2 bc = l o a d b r e a s t c a n c e r ()\n3 p r i n t ( bc . data . shape )\n4 p r i n t ( bc . t a r g e t . shape ,\nnp .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 18,
      "start_char": 4581,
      "end_char": 5098,
      "chunk_index": 11,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_12",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "t a r g e t . shape ,\nnp . unique ( bc . t a r g e t ) )\n5 p r i n t ( bc . feature names )\n6 X = bc . data\n7 y = bc .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 19,
      "start_char": 5072,
      "end_char": 5190,
      "chunk_index": 12,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_13",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "feature names )\n6 X = bc . data\n7 y = bc . t a r g e t\nDonny Hurley\nkNN\n19 / 49\n\nSklearn datasets\n1 (569 ,\n30)\n2 (569 ,)\n[0\n1]\n3 [ \u2019mean\nr a d i u s \u2019\n\u2019mean\nt e x t u r e \u2019\n\u2019mean\np e r i m e t e r \u2019\n\u2019mean area \u2019\n4\n\u2019mean smoothness \u2019\n\u2019mean compactness \u2019\n\u2019mean\nc o n c a v i t y \u2019\n5\n\u2019mean concave\np o i n t s \u2019\n\u2019mean symmetry \u2019\n\u2019mean\nf r a c t a l\ndimension \u2019\n6\n\u2019 r a d i u s\ne r r o r \u2019\n\u2019 t e x t u r e\ne r r o r \u2019\n\u2019 p e r i m e t e r\ne r r o r \u2019\n\u2019 area\ne r r o r \u2019\n7\n\u2019 smoothness\ne r r o r \u2019\n\u2019 compactness\ne r r o r \u2019\n\u2019 c o n c a v i t y\ne r r o r \u2019\n8\n\u2019 concave\np o i n t s\ne r r o r \u2019\n\u2019 symmetry\ne r r o r \u2019\n\u2019 f r a c t a l\ndimension\ne r r o r \u2019\n9\n\u2019 worst\nr a d i u s \u2019\n\u2019 worst\nt e x t u r e \u2019\n\u2019 worst\np e r i m e t e r \u2019\n\u2019 worst\narea \u2019\n10\n\u2019 worst\nsmoothness \u2019\n\u2019 worst\ncompactness \u2019\n\u2019 worst\nc o n c a v i t y \u2019\n11\n\u2019 worst\nconcave\np o i n t s \u2019\n\u2019 worst\nsymmetry \u2019\n\u2019 worst\nf r a c t a l\ndimension \u2019 ]\nDonny Hurley\nkNN\n20 / 49\n\nSklearn datasets\nApply a randomly-chosen classifier and it does quite well\n1 model = GaussianNB ()\n2 model .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 19,
      "start_char": 5148,
      "end_char": 6182,
      "chunk_index": 13,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_14",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "f i t ( X train ,\ny t r a i n )\n3 p r i n t ( model . s c o r e ( X val ,\ny v a l ) )\n4 p r i n t ( c l a s s i f i c a t i o n r e p o r t ( y val ,\nmodel . p r e d i c t ( X val ) ) )\n5\n6 0.944055944056\n7\np r e c i s i o n\nr e c a l l\nf1=s c o r e\nsupport\n8\n9\n0\n0.94\n0.91\n0.93\n55\n10\n1\n0.94\n0.97\n0.96\n88\n11\n12 avg /\nt o t a l\n0.94\n0.94\n0.94\n143\nAlthough the 5.6% inaccuracy may be too much for this type of information. Ignore\nthis and just think of modelling.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 21,
      "start_char": 6182,
      "end_char": 6643,
      "chunk_index": 14,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_15",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Ignore\nthis and just think of modelling. Donny Hurley\nkNN\n21 / 49\n\nSklearn datasets - kNN\nkNN seems to do worse\n1 model = K N e i g h b o r s C l a s s i f i e r ( n n e i g h b o r s =9)\n2 model . f i t ( X train ,\ny t r a i n )\n3 p r i n t ( model . s c o r e ( X val ,\ny v a l ) )\n4\n5 0.923076923077\nAny ideas why? Ask questions about the dataset, how it fits into the model and how the model reacts.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 21,
      "start_char": 6603,
      "end_char": 7006,
      "chunk_index": 15,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_16",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Remember I said about distance metrics being heavily influenced by larger values, so\nwe may need some form of scaling. Donny Hurley\nkNN\n22 / 49\n\nSklearn datasets - kNN\nThe scales are completely different. Measuring \u201dnearest\u201d counts \u201dmean radius\u201d as\nmuch more important than \u201dworst concavity\u201d. 1 bc df = pd . DataFrame ( bc . data ,\ncolumns=bc . feature names )\n2 p r i n t ( bc df [ [ \u2019mean\nr a d i u s \u2019 ,\n\u2019 t e x t u r e\ne r r o r \u2019 ,\n\u2019 worst\nc o n c a v i t y \u2019 ] ] .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 22,
      "start_char": 7006,
      "end_char": 7476,
      "chunk_index": 16,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_17",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "d e s c r i b e () )\n3\n4\nmean\nr a d i u s\nt e x t u r e\ne r r o r\nworst\nc o n c a v i t y\n5 count\n569.000000\n569.000000\n569.000000\n6 mean\n14.127292\n1.216853\n0.272188\n7 std\n3.524049\n0.551648\n0.208624\n8 min\n6.981000\n0.360200\n0.000000\n9 25%\n11.700000\n0.833900\n0.114500\n10 50%\n13.370000\n1.108000\n0.226700\n11 75%\n15.780000\n1.474000\n0.382900\n12 max\n28.110000\n4.885000\n1.252000\nDonny Hurley\nkNN\n23 / 49\n\nFeature Scaling\nAt the very least, we can bring the features into a similar range.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 23,
      "start_char": 7476,
      "end_char": 7955,
      "chunk_index": 17,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_18",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "This is a common transformation step at the start of a model: scale each feature\nseparately so they have the same \u201dsize\u201d. That could mean scaling your data so:\nthe smallest value is 0 and the largest is 1. the mean is 0 and standard deviation is 1. something else that makes sense for your data. Donny Hurley\nkNN\n24 / 49\n\nFeature Scaling\nsklearn of course has a transformer\n1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 24,
      "start_char": 7955,
      "end_char": 8351,
      "chunk_index": 18,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_19",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "p r e p r o c e s s i n g\nimport\nMinMaxScaler ,\nStandardScaler\n2 model = m a k e p i p e l i n e (\n3\nStandardScaler () ,\n4\nK N e i g h b o r s C l a s s i f i e r ( n n e i g h b o r s =9)\n5 )\n6 model . f i t ( X train ,\ny t r a i n )\n7 p r i n t ( model . s c o r e ( X val ,\ny v a l ) )\n8\n9 0.958041958042\nAn improvement on both GaussianNB and the previous KNeighborsClassifier\nDonny Hurley\nkNN\n25 / 49\n\nEvaluating Machine Learning Models\nHow predictive is the model we learned?",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 25,
      "start_char": 8351,
      "end_char": 8831,
      "chunk_index": 19,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_20",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "For regression, usually\nMean Squared Error (MSE) : What we used for linear regression\nR2\np-values (Hypothesis testing)\nF-stats (ANOVA)\nAIC/BIC\nFor classification, many options\nROC curve\nLift Chart\nAccuracy score\nF1 Score, Recall, Precision\nDonny Hurley\nMetrics\n26 / 49\n\nChoosing a metric\nThe procedure of systematically choosing a set of predictors that have a\nsignificant relationship with the response variable is called variable selection.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 26,
      "start_char": 8831,
      "end_char": 9273,
      "chunk_index": 20,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_21",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "But which metric (F-stats, p-values [hypothesis testing], R2, AIC/BIC) should we\nuse to determine the significance of a set of predictors? Rather than relying on a single metric, we should use multiple metrics in\nconjunction and double check with common sense! Donny Hurley\nMetrics\n27 / 49\n\nClassification Evaluation\nFor evaluating Classification algorithms often people just see how accurate the\npredictions are on the test set\n1 y pred = model . p r e d i c t ( X val )\n2 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 27,
      "start_char": 9273,
      "end_char": 9767,
      "chunk_index": 21,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_22",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "p r e d i c t ( X val )\n2 from\ns k l e a r n . m e t r i c s\nimport\na c c u r a c y s c o r e\n3 a c c u r a c y s c o r e ( y val ,\ny pred )\nWill give a score (\u22641) which is the fraction of how many predictions were correct. This stores the y predictions separately (which may be useful later) but the model has\na method itself that can be used if you just want the accuracy score\n1 model . s c o r e ( X val ,\ny v a l )\nThis one line does the predicting and comparing of predictions in the same step.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 28,
      "start_char": 9721,
      "end_char": 10221,
      "chunk_index": 22,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_23",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Donny Hurley\nMetrics\n28 / 49\n\nAccuracy Score\nAccuracy score is a sometimes called a very harsh metric and it does not tell us how a\nclassifier performs for particular classes. The number digit classifier has 10 possible classes, i.e. 0,1,2,3,4,5,6,7,8,9. Maybe we\njust care about the performance of the number 7. Also, is it over-predicting some of the classes? (Is it falsely identifying 1\u2019s as 7, i.e.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 28,
      "start_char": 10221,
      "end_char": 10624,
      "chunk_index": 23,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_24",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "(Is it falsely identifying 1\u2019s as 7, i.e. is it\na false-positive for 7 and a false-negative for 1)\nDonny Hurley\nMetrics\n29 / 49\n\nPrecision/Recall\nFor each category we can measure two important metrics, Precision and Recall\nPrecision: how many selected were correct. Recall: how many correct were found? A Classification Report can be outputted from\nour test data\n1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 29,
      "start_char": 10583,
      "end_char": 10968,
      "chunk_index": 24,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_25",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "m e t r i c s\nimport\nc l a s s i f i c a t i o n r e p o r t\n2 p r i n t ( c l a s s i f i c a t i o n r e p o r t ( y val ,\ny pred ) )\n3\n4\np r e c i s i o n\nr e c a l l\nf1=s c o r e\nsupport\n5\n6\n0\n1.00\n1.00\n1.00\n54\n7\n1\n0.89\n0.94\n0.92\n35\n8\n2\n0.94\n0.89\n0.91\n36\n9\n10 avg /\nt o t a l\n0.95\n0.95\n0.95\n125\nDonny Hurley\nMetrics\n30 / 49\n\nPrecision/Recall\n1\np r e c i s i o n\nr e c a l l\nf1=s c o r e\nsupport\n2\n0\n1.00\n1.00\n1.00\n54\n3\n1\n0.89\n0.94\n0.92\n35\n4\n2\n0.94\n0.89\n0.91\n36\nThe above is showing the performance for 3 categories (0, 1, 2).",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 30,
      "start_char": 10968,
      "end_char": 11497,
      "chunk_index": 25,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_26",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "The support is the\nnumber of occurrences of each class in y true. Category 1 has precision 0.89. This means that 89% of items predicted to be in\nCategory 1 were correct. The other 11% were incorrectly identified as Category 1\n(false-positive for category 1)\nCategory 1 has recall 0.94. This means that 94% of the items that should\u2019ve been\npredicted in Category 1 were found.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 31,
      "start_char": 11497,
      "end_char": 11871,
      "chunk_index": 26,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_27",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "The other 6% were incorrectly identified in some\nother category (false-negative for category 1)\nDonny Hurley\nMetrics\n31 / 49\n\nPrecision is defined as the number of relevant domains retrieved, divided by the\ntotal number of domains (in that category) retrieved. Precision = #relevant-retrieved\n#total-documents\nRecall is defined as the number of relevant domains retrieved divided by the total\nnumber of relevant domains (in that category).",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 31,
      "start_char": 11871,
      "end_char": 12310,
      "chunk_index": 27,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_28",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Recall = #relevant-retrieved\n#total-relevant\nsklearn says\nprecision =\ntp\ntp + fp\nrecall =\ntp\ntp + fn\nwhere tp is number of true positives, fp is number of false positives and fn is\nnumber of false negatives\nDonny Hurley\nMetrics\n32 / 49\n\nCancer Example\nExample\nSuppose we build a model for cancer diagnosis and we have a validation sample of 100\npatients.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 32,
      "start_char": 12310,
      "end_char": 12664,
      "chunk_index": 28,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_29",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "We have the following table describing the performance of our model (this is\ncalled a confusion matrix, I will come back to this):\nWe can read precision from the columns.. Precision for Cancer Yes =\n25\n25+5 = 0.83. Precision for Cancer No =\n65\n65+5 = 0.928. We can read recall from the rows... Recall for Cancer Yes =\n25\n25+5 = 0.83. Recall for\nCancer No =\n65\n65+5 = 0.928.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 33,
      "start_char": 12664,
      "end_char": 13037,
      "chunk_index": 29,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_30",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Recall for\nCancer No =\n65\n65+5 = 0.928. Donny Hurley\nMetrics\n33 / 49\n\nAccuracy Revisited\nNote: Accuracy works quite well when the class distribution is similar but very often\nour data is imbalanced (cancer analysis, we hopefully have many more patients without\ncancer than with - imbalanced set).",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 33,
      "start_char": 12998,
      "end_char": 13294,
      "chunk_index": 30,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_31",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Accuracy =\nTP + TN\nTP + FP + TN + FN\nAccuracy is used when the True Positives and True negatives are more important\nExample\nThe accuracy in this case is = 90% which is a high enough number for the model to\nbe considered as \u2019accurate\u2019. However, there are 5 patients who actually have cancer\nand the model predicted that they don\u2019t have it. Obviously, this is too high a cost. Our model should try to minimize these False Negatives.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 34,
      "start_char": 13294,
      "end_char": 13724,
      "chunk_index": 31,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_32",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Donny Hurley\nMetrics\n34 / 49\n\nF1-Score\nF1-score is a better metric when there are imbalanced classes. F1-score: The harmonic mean of Precision and Recall. F1-Score gives a better measure of the incorrectly classified cases than the Accuracy.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 34,
      "start_char": 13724,
      "end_char": 13965,
      "chunk_index": 32,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_33",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "IF we let P and R stand for precision and recall respectively then\nF1 = 2 P.R\nP + R\n1\np r e c i s i o n\nr e c a l l\nf1=s c o r e\nsupport\n2\n0\n1.00\n1.00\n1.00\n54\n3\n1\n0.89\n0.94\n0.92\n35\n4\n2\n0.94\n0.89\n0.91\n36\nDonny Hurley\nMetrics\n35 / 49\n\nExtreme Example\nOf all women who receive regular mammograms, about 10 percent will get called back\nfor further testing and of those, only about 0.5 percent will be found to have cancer.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 35,
      "start_char": 13965,
      "end_char": 14383,
      "chunk_index": 33,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_34",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "So for the women that are called back 99.5% of them will not have cancer.So if my\nmodel is\n1 def\nhas cancer (X) :\n2\nr e t u r n\nf a l s e\nThen my model will be 99.5% accurate for these women! But it misses all the ones\nthat have cancer. Donny Hurley\nMetrics\n36 / 49\n\nConfusion Matrix\nAnother way of evaluating the the accuracy of a classification is to compute a confuse\nmatrix.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 36,
      "start_char": 14383,
      "end_char": 14761,
      "chunk_index": 34,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_35",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "By definition a confusion matrix C is such that Ci,j is equal to the number of\nobservations known to be in group i but predicted to be in group j. The diagonals are therefore the \u201dcorrect\u201d predictions, while the off diagonals is where\nsomething has been miscategorised\n2-class problems are a particular case\nevent\nno-event\nevent\ntrue positive\nfalse positive\nno-event\nfalse negative\ntrue negative\nDonny Hurley\nMetrics\n37 / 49\n\nConfusion Matrix\nAs an example in sklearn\n1 from\ns k l e a r n .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 37,
      "start_char": 14761,
      "end_char": 15251,
      "chunk_index": 35,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_36",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "m e t r i c s\nimport\nc o n f u s i o n m a t r i x\n2 >>> y t r u e = [ 2 ,\n0 ,\n2 ,\n2 ,\n0 ,\n1]\n3 >>> y pred = [ 0 ,\n0 ,\n2 ,\n2 ,\n0 ,\n2]\n4 >>> c o n f u s i o n m a t r i x ( y true ,\ny pred )\n5 a r r a y ( [ [ 2 ,\n0 ,\n0 ] ,\n6\n[ 0 ,\n0 ,\n1 ] ,\n7\n[ 1 ,\n0 ,\n2 ] ] )\nWe can use confusion matrices to see where exactly our misclassifications are\nhappening and if necessary tune our ML algorithm. Precision and Recall are read from\na confusion matrix.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 38,
      "start_char": 15251,
      "end_char": 15693,
      "chunk_index": 36,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_37",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Donny Hurley\nMetrics\n38 / 49\n\nColour Data Set\nblack\nblue\nbrown\ngreen\ngrey\norange\npink\npurple\nred\nwhite\nyellow\nblack\n68\n0\n1\n0\n2\n0\n0\n1\n1\n0\n0\nblue\n3\n84\n0\n4\n7\n0\n0\n8\n0\n0\n0\nbrown\n5\n0\n18\n4\n5\n1\n1\n0\n18\n0\n2\ngreen\n4\n4\n3\n135\n8\n0\n0\n0\n0\n0\n1\ngrey\n4\n4\n0\n7\n62\n1\n1\n2\n3\n0\n0\norange\n0\n0\n1\n2\n0\n16\n2\n0\n7\n0\n2\npink\n0\n0\n0\n0\n0\n0\n41\n15\n6\n0\n0\npurple\n5\n5\n0\n1\n3\n0\n2\n103\n5\n0\n0\nred\n2\n6\n1\n6\n5\n3\n10\n3\n26\n0\n2\nwhite\n2\n0\n1\n0\n4\n0\n2\n0\n0\n0\n0\nyellow\n0\n1\n1\n4\n4\n0\n2\n0\n0\n0\n17\nWe can read the precision for black off by the black column (predicted were correct).",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 38,
      "start_char": 15693,
      "end_char": 16209,
      "chunk_index": 37,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_38",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "68 predicted were correct, while 68 + 3 + 5 + 4 + 4 + 0 + 0 + 5 + 2 + 2 + 0 = 93 were\npredicted in total to be black. So precision was 68\n93 = 0.7311. We can read the recall for purple by the purple row (correct were found). 103 were found, while 5 + 5 + 0 + 1 + 3 + 0 + 2 + 103 + 5 + 0 + 0 = 124 should have\nbeen found in total. So recall for purple was 103\n124 = 0.83. Donny Hurley\nMetrics\n39 / 49\n\nOptimising to these scores? Written a lot about how we want to get the best metrics: f1-score, accuracy etc.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 39,
      "start_char": 16209,
      "end_char": 16718,
      "chunk_index": 38,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_39",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "However, we can\u2019t actually do this directly! Metrics on a dataset is what we care about (performance)\nWe typically cannot directly optimize for the metrics\nOur loss/cost function should reflect the problem we are solving. We then hope it\nwill yield models that will do well on our dataset\nDonny Hurley\nMetrics\n40 / 49\n\nFeature Engineering\nNot surprisingly, the kind of X values we have for each training point have a huge\neffect on what we can learn/predict. The components of each X value are the features.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 40,
      "start_char": 16718,
      "end_char": 17225,
      "chunk_index": 39,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_40",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "The components of each X value are the features. A combination of understanding our data and the model can help us be much\nmore successful. Donny Hurley\nMetrics\n41 / 49\n\nFeature Engineering\nWe have seen:\nJust use the original features. Calculate powers of the original features (PolynomialFeatures). Scale features to tidy their min/max (MinMaxScaler) or mean/?stddev\n(StandardScaler). Calculate anything else we want/need (FunctionTransformer).",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 41,
      "start_char": 17177,
      "end_char": 17622,
      "chunk_index": 40,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_41",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Donny Hurley\nMetrics\n42 / 49\n\nFeature Engineering\nWe can design whatever features we think will give the model something to work with. As an example:\nDonny Hurley\nMetrics\n43 / 49\n\nFeature Engineering\nWith that picture and a kNN model:\n1 model = K N e i g h b o r s C l a s s i f i e r (7)\n2 model . f i t ( X train ,\ny t r a i n )\n3 p r i n t ( model .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 42,
      "start_char": 17622,
      "end_char": 17974,
      "chunk_index": 41,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_42",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "s c o r e ( X val ,\ny v a l ) )\n4\n5 0.805309734513\nDonny Hurley\nMetrics\n44 / 49\n\nFeature Engineering\nIt doesn\u2019t have enough training data to really capture what\u2019s going on, and overfits the\ntraining data. Donny Hurley\nMetrics\n45 / 49\n\nFeature Engineering\nMaybe if it has a feature of how far away the point is from (0, 0) it would improve the\nresults. Add a transformer:\n1 def\na d d r a d i u s (X) :\n2\nX0 = X [ : ,\n0]\n3\nX1 = X [ : ,\n1]\n4\nR = np . l i n a l g .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 44,
      "start_char": 17974,
      "end_char": 18435,
      "chunk_index": 42,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_43",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "l i n a l g . norm (X,\na x i s =1) # Euclidean\nnorm\n5\nr e t u r n\nnp . stack (( X0 , X1 , R) ,\na x i s =1)\n6\n7 model = m a k e p i p e l i n e (\n8\nFunctionTransformer ( a d d r a d i u s ) ,\n9\nGaussianNB () )\n10 model . f i t ( X train ,\ny t r a i n )\n11 p r i n t ( model .",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 46,
      "start_char": 18422,
      "end_char": 18696,
      "chunk_index": 43,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_44",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "s c o r e ( X val ,\ny v a l ) )\n12\n13 0.893805309735\nDonny Hurley\nMetrics\n46 / 49\n\nFeature Engineering\nThe model does better with the right features\nDonny Hurley\nMetrics\n47 / 49\n\nFeature Engineering\nOther problems that may arise is having\nMore features than train+test data\nToo large of a data set (processor/memory)\nSometimes we have to group common features together\nExample\nWe observe multiple weather values (rainfall, temperature) every day.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 46,
      "start_char": 18696,
      "end_char": 19142,
      "chunk_index": 44,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_45",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "This gives us\n365 points (times the number of weather features) for one year. It may make sense to\naverage these values for each month and cut down the number of points to 12 for a\nyear (times the number of weather features). This comes from knowing the data. We could also average over each day in a month, 1st of month, 2nd of month etc. so\ndown to 31 points....however this would make no sense for grouping together.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 48,
      "start_char": 19142,
      "end_char": 19561,
      "chunk_index": 45,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_46",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Donny Hurley\nMetrics\n48 / 49\n\nHigh-Dimensional Data kNN (Source Wikipedia)\nFor high-dimensional data (e.g., with number of dimensions more than 10)\ndimension reduction is usually performed prior to applying the k-NN algorithm in\norder to avoid the effects of the curse of dimensionality. The curse of dimensionality in the k-NN context basically means that Euclidean\ndistance is unhelpful in high dimensions because all vectors are almost equidistant\nto the search query vector.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 48,
      "start_char": 19561,
      "end_char": 20039,
      "chunk_index": 46,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_47",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "Feature extraction and dimension reduction can be combined in one step using\nprincipal component analysis (PCA), linear discriminant analysis (LDA), or\ncanonical correlation analysis (CCA) techniques as a pre-processing step, followed\nby clustering by k-NN on feature vectors in reduced-dimension space. This process\nis also called low-dimensional embedding. For very-high-dimensional datasets (e.g.",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 49,
      "start_char": 20039,
      "end_char": 20438,
      "chunk_index": 47,
      "total_chunks": 49
    },
    {
      "chunk_id": "e6ed75a24850addc063603e06325bd9b_chunk_48",
      "doc_id": "e6ed75a24850addc063603e06325bd9b",
      "content": "For very-high-dimensional datasets (e.g. when performing a similarity search on\nlive video streams, DNA data or high-dimensional time series) running a fast\napproximate k-NN search using locality sensitive hashing, \u201drandom projections\u201d,\n\u201dsketches\u201d or other high-dimensional similarity search techniques from the VLDB\ntoolbox might be the only feasible option. Donny Hurley\nMetrics\n49 / 49",
      "source_doc_title": "Machine Learning Technologies k-Nearest Neighbours Metrics",
      "section_id": null,
      "page_number": 49,
      "start_char": 20398,
      "end_char": 20786,
      "chunk_index": 48,
      "total_chunks": 49
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_0",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Machine Learning\nDecision Trees\nRandom Forests\nDonny Hurley\nATU\nDonny Hurley\nDecision Trees\n1 / 43\n\nDecision Trees: Motivation\nAll classification models implement decision rules. A decision tree makes these rules explicit and interpretable. Instead of us writing conditions, the model learns them from data.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 307,
      "chunk_index": 0,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_1",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n2 / 43\n\nML as Learned If / Else Rules\nif ???:\nreturn \u2019red\u2019\nelif ???:\nreturn \u2019orange \u2019\nFor example, in digit recognition:\nif img [4 ,8] < 30 and img [32 ,18] > 10:\nreturn \u20191\u2019\nDecision trees learn these rules automatically. Donny Hurley\nDecision Trees\n3 / 43\n\nDecision Trees: Core Idea\nStart with all training data. Choose a feature and a threshold. Split the data into two groups. Repeat recursively on each branch. Stop when further splitting is unnecessary.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 2,
      "start_char": 307,
      "end_char": 793,
      "chunk_index": 1,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_2",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Stop when further splitting is unnecessary. The result is a tree of decisions. Donny Hurley\nDecision Trees\n4 / 43\n\nExample\nSimple rules:\nIf x is small, predict purple. Else if y is small, predict red. Else predict green. Donny Hurley\nDecision Trees\n5 / 43\n\nTree Terminology\nRoot node: first split\nInternal nodes: decision points\nLeaf nodes: final predictions\nDonny Hurley\nDecision Trees\n6 / 43\n\nDecision trees are a type of classifier network.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 4,
      "start_char": 750,
      "end_char": 1193,
      "chunk_index": 2,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_3",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "They are very easy to interpret which means that if causal reasoning is important\nthen they can be one of the better ways to go. This works because it is very easy to trace our way back up the tree to determine\nwhy each decision was made. In general, trees are upside down with the root at the top and the leaves at the\nbottom. Nodes that are neither the root or a leaf are called internal nodes. If we get to a\nleaf node, a classification has been made.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 7,
      "start_char": 1193,
      "end_char": 1647,
      "chunk_index": 3,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_4",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n7 / 43\n\nDecision Trees I\nWe can use scikit-learn to do the work\nfrom\nsklearn.tree\nimport\nDecisionTreeClassifier\nmodel = DecisionTreeClassifier (max_depth =2)\nmodel.fit(X_train , y_train)\nGiving these decision boundaries\nDonny Hurley\nDecision Trees\n8 / 43\n\nDecision Trees II\nYou\u2019ll notice max depth is a hyperparameter. Increasing that will make things more\ncomplex, with the usual risk of overfitting.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 7,
      "start_char": 1647,
      "end_char": 2076,
      "chunk_index": 4,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_5",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "With this \u201dtoy\u201d example and depth four we\nget the following decision boundaries\nAnother option: how many nodes must be on a branch before it\u2019s too small to split?",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 9,
      "start_char": 2076,
      "end_char": 2238,
      "chunk_index": 5,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_6",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "model = DecisionTreeClassifier ( min_samples_split =30)\nDonny Hurley\nDecision Trees\n9 / 43\n\nControlling Tree Complexity\nImportant hyperparameters:\nmax depth \u2013 how many decisions are chained\nmin samples leaf \u2013 minimum samples at a leaf\nmin samples split - minimum number of samples required to split an internal\nnode\nYou can of course combine them and set multiple hyperparameters.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 9,
      "start_char": 2238,
      "end_char": 2618,
      "chunk_index": 6,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_7",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Deep trees: low bias, high variance\nShallow trees: higher bias, lower variance\nDonny Hurley\nDecision Trees\n10 / 43\n\nVisualising\nScikit-learn can visualise the decision trees for us https://scikit-learn.org/\nstable/modules/generated/sklearn.tree.export_graphviz.html exporting to\ngraphviz. For the two max depths we get\nDonny Hurley\nDecision Trees\n11 / 43\n\nHow Does a Tree Decide Where to Split? At each node, the algorithm chooses the split that produces the purest child nodes.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 10,
      "start_char": 2618,
      "end_char": 3096,
      "chunk_index": 7,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_8",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Purity measures how mixed the class labels are after the split. Common impurity measures:\nGini impurity (default in scikit-learn)\nEntropy\nBoth are a measure of how \u201dpure\u201d a branch is. If we can split so that 100% of\nvalues on each side are in one category, then that\u2019s good. If we split so that every\noutcome is equally likely on each side, that\u2019s bad. Create the decision that\nmaximizes \u201dgoodness\u201d. Either method seems to produce similar trees.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 12,
      "start_char": 3096,
      "end_char": 3541,
      "chunk_index": 8,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_9",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Either method seems to produce similar trees. In either case, we create a decision,\nand then recurse until we have to stop. Donny Hurley\nDecision Trees\n12 / 43\n\nGini Impurity\nGini impurity measures how mixed the classes are at a node. G = 1 \u2212\nc\nX\ni=1\np2\ni\nwhere pi is the proportion of class i at the node. G = 0 \u21d2node is perfectly pure\nEqual class proportions \u21d2high Gini\nDonny Hurley\nDecision Trees\n13 / 43\n\nReading Gini from a Tree\nWhen viewing a tree (e.g.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 12,
      "start_char": 3496,
      "end_char": 3955,
      "chunk_index": 9,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_10",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Graphviz output):\nGini is displayed at each node\nLeaf nodes with only one class have Gini = 0\nInternal nodes with mixed labels have G > 0\nDonny Hurley\nDecision Trees\n14 / 43\n\nTree examples\nThe default for sklearn.tree.DecisionTreeClassifier criterion is \u2019gini\u2019 but you can set it\nto \u2019entropy\u2019 like the following examples.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 14,
      "start_char": 3955,
      "end_char": 4276,
      "chunk_index": 10,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_11",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n15 / 43\n\nDepth of 2\nDonny Hurley\nDecision Trees\n16 / 43\n\nDepth of 3\nDonny Hurley\nDecision Trees\n17 / 43\n\nDepth of 4\nDonny Hurley\nDecision Trees\n18 / 43\n\nDepth of 6\nDonny Hurley\nDecision Trees\n19 / 43\n\nScores\nTree depth 1 Training Score 0.694444 Validation Score: 0.562500\nTree depth 2 Training Score 0.888889 Validation Score: 0.937500\nTree depth 3 Training Score 0.888889 Validation Score: 0.937500\nTree depth 4 Training Score 0.944444 Validation Score: 1.000000\nTree depth 5 Training Score 0.972222 Validation Score: 0.937500\nTree depth 6 Training Score 1.000000 Validation Score: 0.937500\nKeep in mind that there are only 32 points in our training set and 16 points in our\nvalidation set so more data would certainly help the situation.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 15,
      "start_char": 4276,
      "end_char": 5043,
      "chunk_index": 11,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_12",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n20 / 43\n\nTraining vs Validation Performance\nAs tree depth increases:\nTraining accuracy increases\nValidation accuracy may decrease\nThis is overfitting.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 20,
      "start_char": 5043,
      "end_char": 5221,
      "chunk_index": 12,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_13",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n21 / 43\n\nAdvantages of Decision Trees\nHighly interpretable\nMinimal preprocessing required\nHandle numeric and categorical data\nCapture nonlinear interactions\nUseful when explanation matters\nDonny Hurley\nDecision Trees\n22 / 43\n\nDisadvantages of Decision Trees\nHigh variance\nSensitive to small data changes\nSingle trees rarely generalise optimally\nDonny Hurley\nDecision Trees\n23 / 43\n\nEnsembles\nIt can sometimes be hard to balance the complexity of the classifier with\nover-fitting.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 21,
      "start_char": 5221,
      "end_char": 5728,
      "chunk_index": 13,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_14",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Sometimes, we\u2019ll find that no single classifier will do a great job on\nthe problem. It can be easy to produce a classifier that makes predictions better than chance,\nbut still not great. Often useful to have many simple classifiers, with some differences in their\nconstruction so they produce different results and let them vote: an ensemble\nAn ensemble can often produce good results, even though the individual classifiers\nare quite bad.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 24,
      "start_char": 5728,
      "end_char": 6167,
      "chunk_index": 14,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_15",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Combining many weak models can give a much stronger and robust result that\ngeneralises better. The wisdom of the crowd\nDonny Hurley\nDecision Trees\n24 / 43\n\nEnsembles\nUsually, all of the models will be of the same type, but they don\u2019t have to be. A\nVotingClassifier will let you combine any collection of models you want, train them\ntogether, and vote on predictions.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 24,
      "start_char": 6167,
      "end_char": 6533,
      "chunk_index": 15,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_16",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "model = VotingClassifier ([\n(\u2019nb\u2019, GaussianNB ()),\n(\u2019knn\u2019, KNeighborsClassifier (5)),\n(\u2019svm\u2019, SVC(kernel=\u2019linear \u2019, C=0.1)) ,\n(\u2019tree1 \u2019, DecisionTreeClassifier (max_depth =4)),\n(\u2019tree2 \u2019, DecisionTreeClassifier ( min_samples_leaf =10)) ,\n])\nmodel.fit(X_train , y_train)\nprint(model.score(X_train , y_train ))\nprint(model.score(X_valid , y_valid ))\nDonny Hurley\nDecision Trees\n25 / 43\n\nRandom Forests\nA random forest is an ensemble of decision trees where:\nEach tree is trained on a bootstrap sample\nEach split considers only a random subset of features\nPredictions are aggregated by voting\nDonny Hurley\nDecision Trees\n26 / 43\n\nRandom Forest in scikit-learn\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\nmodel = RandomForestClassifier (\nn_estimators =100 ,\nmax_depth =5,\nmin_samples_leaf =10\n)\nmodel.fit(X_train , y_train)\nDonny Hurley\nDecision Trees\n27 / 43\n\nRandom Forests\nEach decision tree will be constructed as before, but.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 25,
      "start_char": 6533,
      "end_char": 7467,
      "chunk_index": 16,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_17",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": ". . individual trees will likely be simpler (lower max depth, fewer leaves, etc);\nwhen training, use only a sample of the training data (chosen randomly with\nreplacement, a bootstrap sample);\nfor each decision, consider only a random subset of the features. . . . and repeat many times. Donny Hurley\nDecision Trees\n28 / 43\n\nRandom Forest 10 times\nDonny Hurley\nDecision Trees\n29 / 43\n\nHyperparameters That Matter\nFocus on:\nmax depth\nmin samples leaf\nn estimators\nMore trees \u21d2lower variance but higher computation cost.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 28,
      "start_char": 7467,
      "end_char": 7984,
      "chunk_index": 17,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_18",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n30 / 43\n\nRandom Forests I\nOriginal toy data. When constructing a random forest, we get the same options as a\ndecision tree about the tree shape, plus the number of estimators (and some other\nchoices about the randomness of the construction).",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 30,
      "start_char": 7984,
      "end_char": 8253,
      "chunk_index": 18,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_19",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "from\nsklearn.ensemble\nimport\nRandomForestClassifier\nmodel = RandomForestClassifier (n_estimators =100,\nmax_depth =3, min_samples_leaf =10)\nmodel.fit(X_train , y_train)\nprint(model.score(X_train , y_train ))\nprint(model.score(X_valid , y_valid ))\n0.9653333333333334\n0.92\nDonny Hurley\nDecision Trees\n31 / 43\n\nRandom Forests II\nWe get a reasonable-looking decision boundary\nDonny Hurley\nDecision Trees\n32 / 43\n\nRandom Forests III\nA few of the decision trees from the forest:\nDonny Hurley\nDecision Trees\n33 / 43\n\nRandom Forests IV\nNone of the individual trees make great decisions, but together they do.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 31,
      "start_char": 8253,
      "end_char": 8852,
      "chunk_index": 19,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_20",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "The voting among trees can also help with over-fitting: since each was trained with a\nsubset of the data, it\u2019s less of a problem. Donny Hurley\nDecision Trees\n34 / 43\n\nRandom Forests V\nLet\u2019s try some data from make moons.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 34,
      "start_char": 8852,
      "end_char": 9072,
      "chunk_index": 20,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_21",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "We can say very little about tree size but still\nget good results:\nmodel = RandomForestClassifier (n_estimators =400, max_depth =7)\nmodel.fit(X_train , y_train)\nprint(model.score(X_train , y_train ))\nprint(model.score(X_valid , y_valid ))\n1.0\n1.0\nDonny Hurley\nDecision Trees\n35 / 43\n\nRandom Forests VI\nThere is still maybe some over-fitting, but it\u2019s not affecting the scores.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 35,
      "start_char": 9072,
      "end_char": 9448,
      "chunk_index": 21,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_22",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n36 / 43\n\nRandom Forest Algorithm\n1 Draw a random bootstrap sample of size n (randomly choose n samples from the\ntraining set with replacement i.e. there can be duplicates). 2 Grow a decision tree from the bootstrap sample. At each node:\nRandomly select d features without replacement i.e. no duplicates. Split the node using the feature that provides the best split according to the\nobjective function, for instance, by maximising the information gain. 3 Repeat the steps 1 and 2 k times.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 36,
      "start_char": 9448,
      "end_char": 9964,
      "chunk_index": 22,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_23",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "3 Repeat the steps 1 and 2 k times. 4 Aggregate the prediction by each tree to assign the class label by majority vote.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 37,
      "start_char": 9929,
      "end_char": 10048,
      "chunk_index": 23,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_24",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "Donny Hurley\nDecision Trees\n37 / 43\n\nBias\u2013Variance Summary\nDeep single tree: low bias, high variance\nShallow tree: higher bias, lower variance\nRandom forest: low variance, good generalisation\nDonny Hurley\nDecision Trees\n38 / 43\n\nWhen to Use Trees and Random Forests\nThey work particularly well for:\nTabular data\nMixed feature types\nNonlinear relationships\nSmall to medium datasets\nMessy or noisy data\nDonny Hurley\nDecision Trees\n39 / 43\n\nTrees vs Neural Networks\nTrees: tabular, interpretable, low preprocessing\nNeural networks: images, text, audio, large datasets\nTrees are often strong baselines\nNeural networks are rarely the best choice for tabular data\nDonny Hurley\nDecision Trees\n40 / 43\n\nOut-of-Bag Estimation\nEach tree leaves out \u22481/3 of training samples\nThese are called out-of-bag samples\nCan estimate generalisation internally\nUseful, but not a replacement for validation\nDonny Hurley\nDecision Trees\n41 / 43\n\nReproducibility with Random Forests and Cross Validation\nRandom forests contain randomness:\nBootstrap sampling of training data\nRandom subset of features at each split\nTo ensure consistent cross-validation results, fix the random seed:\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\nfrom\nsklearn.",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 37,
      "start_char": 10048,
      "end_char": 11269,
      "chunk_index": 24,
      "total_chunks": 26
    },
    {
      "chunk_id": "10a9fe4a373bcfb421d517dd5d832139_chunk_25",
      "doc_id": "10a9fe4a373bcfb421d517dd5d832139",
      "content": "model_selection\nimport\ncross_val_score\nmodel = RandomForestClassifier (\nn_estimators =100 ,\nrandom_state =42\n)\n%scores = cross_val_score (model , X_train , y_train)\nSetting random state ensures reproducible results across runs. Donny Hurley\nDecision Trees\n42 / 43\n\nSummary\nDecision trees are interpretable but unstable\nRandom forests reduce variance via averaging\nExcellent choice for tabular data\nStrong baselines before neural networks\nDonny Hurley\nDecision Trees\n43 / 43",
      "source_doc_title": "Machine Learning Decision Trees Random Forests",
      "section_id": null,
      "page_number": 42,
      "start_char": 11269,
      "end_char": 11742,
      "chunk_index": 25,
      "total_chunks": 26
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_0",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "MOBILE APPLICATIONS DEVELOPMENT 2\nIHAB SALAWDEH\nDEPARTMENT OF COMPUTER SCIENCE & APPLIED PHYSICS \n\nCUSTOM TYPES AND CLASSES\n\nINTRODUCTION\n\uf0a1Variables\n\uf0a1Classes\n\uf0a1Access Modifiers \n\uf0a1Interfaces \n\uf0a1Enums\n\nVARIABLES\n\uf0a1var (ES5 and earlier versions)\n\uf0a1let (ES6 and later versions)\n\uf0a1const\n\nVARIABLES\n\uf0a1A variable, by definition, is \u201ca named space in the memory\u201d that stores values\n\uf0a1Naming:\n\uf0a1\nVariable names can contain alphabets and numeric digits.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 435,
      "chunk_index": 0,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_1",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1\nThey cannot contain spaces and special characters, except the underscore (_) and the dollar ($) sign. \uf0a1\nVariable names cannot begin with a digit.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 5,
      "start_char": 435,
      "end_char": 582,
      "chunk_index": 1,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_2",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1\nVariable names cannot begin with a digit. VAR EXAMPLE\n\nLET EXAMPLE\n\uf0a1\nThe variables declared using the 'let' keyword have the block scope\n\uf0a1\nYou can't re-declare the variables that are declared using the 'let' keyword\n\uf0a1\nVariables with the same name but if they are in different blocks, are considered as different variables\n\nEXAMPLES TRANSPILED\n\nCONST\n\uf0a1\nThe 'const' keyword has the same syntax as 'var' and 'let' to declare variables\n\uf0a1\nIt is used to declare the constant variables\n\uf0a1\nYou need to initialize the 'const' variables while defining them, and you can't change them later\n\uf0a1\n'const' keyword has the same rule for scoping and re-declaration as the variable declared using the 'let' \nkeyword\n\uf0a1\nBy convention, constant variable names are all uppercase\nconst VAR_NAME: var_type = value;\n\nCLASSES\n\uf0a1TypeScript is object-oriented JavaScript.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 5,
      "start_char": 539,
      "end_char": 1381,
      "chunk_index": 2,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_3",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1TypeScript supports object-oriented programming features like classes, interfaces, etc. \uf0a1A class in terms of OOP is a blueprint for creating objects. \uf0a1A class encapsulates data and behaviour for the object. \uf0a1Introduced in ES6\n\nCLASS EXAMPLE \n\nCLASS EXAMPLE TRANSPILE\n\nINHERITANCE\n\uf0a1\nTypeScript supports the concept of Inheritance. \uf0a1\nInheritance is the ability of a program to create new classes from an existing class. \uf0a1\nThe class that is extended to create newer classes is called the parent class/super class.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 10,
      "start_char": 1381,
      "end_char": 1892,
      "chunk_index": 3,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_4",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1\nThe newly created classes are called the child/sub classes. \uf0a1\nA class inherits from another class using the \u2018extends\u2019 keyword. \uf0a1\nChild classes inherit all properties and methods except private members and constructors from the \nparent class. \uf0a1\nA child constructor function must call super() which will execute the constructor of the base class.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 13,
      "start_char": 1892,
      "end_char": 2238,
      "chunk_index": 4,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_5",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "INHERITANCE\n\nINHERITANCE\n\nACCESS MODIFIERS \n\uf0a1\nThe access specifiers/modifiers define the visibility of a class\u2019s data members outside its defining class\n\uf0a1\npublic: \n\uf0a1\nA public data member has universal accessibility. \uf0a1\nData members in a class are public by default. \uf0a1\nprivate: \n\uf0a1\nPrivate data members are accessible only within the class that defines these members. \uf0a1\nIf an external class member tries to access a private member, the compiler throws an error.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 13,
      "start_char": 2238,
      "end_char": 2696,
      "chunk_index": 5,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_6",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1\nprotected:\n\uf0a1\nA protected data member is accessible by the members within the same class as that of the former \n\uf0a1\nand also by the members of the child classes\n\nINTERFACES\n\uf0a1Define custom types in typescript using interfaces. \uf0a1An interface in TypeScript is pretty much the same as an interface in just about any other \nstatically-typed language such as C# or Java. \uf0a1Interfaces are collections of related methods that typically enable you to tell objects what to \ndo, but not how to do it.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 16,
      "start_char": 2696,
      "end_char": 3183,
      "chunk_index": 6,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_7",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1http://www.typescriptlang.org/docs/handbook/interfaces.html\n\nINTERFACES\n\nENUMS\n\uf0a1Define custom types in typescript using enums. \uf0a1An enum type in its simplest form declares a set of constants represented by identifiers. \uf0a1Braces delimit an enum declaration\u2019s body. \uf0a1Inside the braces is a comma-separated list of enum constants, each representing a unique \nvalue. \uf0a1The identifiers in an enum must be unique. \uf0a1Variables of an enum type can be assigned only the constants declared in the enum.",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 17,
      "start_char": 3183,
      "end_char": 3672,
      "chunk_index": 7,
      "total_chunks": 9
    },
    {
      "chunk_id": "cf0829118424b4c2162d7c6bfa757792_chunk_8",
      "doc_id": "cf0829118424b4c2162d7c6bfa757792",
      "content": "\uf0a1http://www.typescriptlang.org/docs/handbook/enums.html\n\nENUMS EXAMPLE\n? Denotes an optional \narguments of parameters. QUESTIONS",
      "source_doc_title": "W2 - Custom Types and Classes",
      "section_id": null,
      "page_number": 19,
      "start_char": 3672,
      "end_char": 3800,
      "chunk_index": 8,
      "total_chunks": 9
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_0",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "MOBILE APPLICATIONS DEVELOPMENT 2\nIHAB SALAWDEH\nDEPARTMENT OF COMPUTER SCIENCE & APPLIED PHYSICS \n\nMISCELLANEOUS TOPICS\n\nINTRODUCTION\n\uf0a1Generics\n\uf0a1Arrow Functions\n\uf0a1This\n\uf0a1Arrays\n\uf0a1Map Function\n\uf0a1Switch Statements\n\uf0a1DOM Example\n\uf0a1Template Strings\n\nGENERICS\n\uf0a1The identity function is a function that will return back whatever is passed in. You can think of \nthis in a similar way to the echo command.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 391,
      "chunk_index": 0,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_1",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "\uf0a1Without generics, we would either have to give the identity function a specific type:\n\uf0a1Or, we could describe the identity function using the any type:\n\nGENERICS\n\uf0a1Use a type variable, a special kind of variable that works on types rather than values. \uf0a1We can call it in one of two ways. \uf0a1\nThe first way is to pass all of the arguments, including the type argument, to the function:\n\uf0a1\nThe second way is also perhaps the most common.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 4,
      "start_char": 391,
      "end_char": 822,
      "chunk_index": 1,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_2",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "Here we use type argument inference\n\nARROW FUNCTIONS \n\uf0a1A concise way to write the anonymous functions in TypeScript. \uf0a1They offer a shorter syntax compared to traditional function declarations. \uf0a1Arrow functions are also called lambda functions. Lambda refers to the anonymous functions \nin programming\n\nTHIS\n\uf0a1The this keyword in JavaScript (and thus \nTypeScript) behaves differently than it \ndoes in many other programming \nlanguages.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 5,
      "start_char": 822,
      "end_char": 1255,
      "chunk_index": 2,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_3",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "\uf0a1This can be very surprising, especially \nfor users of other languages that have \ncertain intuitions about how this should \nwork. ARRAYS\n\uf0a1\nAn array declaration allocates sequential memory blocks. \uf0a1\nArrays are static. This means that an array once initialized \ncannot be resized. \uf0a1\nEach memory block represents an array element. \uf0a1\nArray elements are identified by a unique integer called as \nthe subscript / index of the element. \uf0a1\nLike variables, arrays too, should be declared before they \nare used.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 7,
      "start_char": 1255,
      "end_char": 1755,
      "chunk_index": 3,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_4",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "Use the var keyword to declare an array. \uf0a1\nArray initialization refers to populating the array elements. \uf0a1\nArray element values can be updated or modified but \ncannot be deleted\n\nARRAYS (2D)\n\uf0a1https://www.tutorialspoint.com/typescript/typescript_arrays.htm\n\nARRAYS\n1. concat() Returns a new array comprised of this array joined \nwith other array(s) and/or value(s). 2. every() Returns true if every element in this array satisfies \nthe provided testing function. 3.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 8,
      "start_char": 1755,
      "end_char": 2219,
      "chunk_index": 4,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_5",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "3. filter() Creates a new array with all of the elements of this \narray for which the provided filtering function returns true. 4. forEach() Calls a function for each element in the array. 5. indexOf() Returns the first (least) index of an element \nwithin the array equal to the specified value, or -1 if none is \nfound. 6. join() Joins all elements of an array into a string. 7. lastIndexOf() Returns the last (greatest) index of an \nelement within the array equal to the specified value, or -1 \nif none is found. 8.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 10,
      "start_char": 2217,
      "end_char": 2734,
      "chunk_index": 5,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_6",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "8. map() Creates a new array with the results of calling a \nprovided function on every element in this array. 9. pop() Removes the last element from an array and returns \nthat element. 10. push() Adds one or more elements to the end of an array and \nreturns the new length of the array. 11. reduce() Apply a function simultaneously against two values of \nthe array (from left-to-right) as to reduce it to a single value. 12.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 10,
      "start_char": 2732,
      "end_char": 3156,
      "chunk_index": 6,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_7",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "12. reduceRight() Apply a function simultaneously against two \nvalues of the array (from right-to-left) as to reduce it to a single \nvalue. 13. reverse() Reverses the order of the elements of an array -- the \nfirst becomes the last, and the last becomes the first. 14. shift() Removes the first element from an array and returns that \nelement. 15. slice() Extracts a section of an array and returns a new array. 16. some() Returns true if at least one element in this array satisfies \nthe provided testing function. 17.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 10,
      "start_char": 3153,
      "end_char": 3672,
      "chunk_index": 7,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_8",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "17. sort() Sorts the elements of an array. 18. splice() Adds and/or removes elements from an array. 19. toString() Returns a string representing the array and its \nelements. 20. unshift() Adds one or more elements to the front of an array \nand returns the new length of the array. MAP FUNCTION \n\uf0a1Transform arrays\n\uf0a1A function is applied to each element within the array. \uf0a1After the function is applied to each of the elements of the array, each of those elements is \nadded to a new array.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 10,
      "start_char": 3669,
      "end_char": 4156,
      "chunk_index": 8,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_9",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "SWITCH STATEMENTS \n\uf0a1There can be any number of case \nstatements within a switch. \uf0a1The case statements can include only \nconstants. It cannot be a variable or an \nexpression. \uf0a1The data type of the variable \nexpression and the constant \nexpression must match. \uf0a1Unless you put a break after each block \nof code, execution flows into the next \nblock. \uf0a1The case expression must be unique. \uf0a1The default block is optional.",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 11,
      "start_char": 4156,
      "end_char": 4571,
      "chunk_index": 9,
      "total_chunks": 11
    },
    {
      "chunk_id": "a06d9bb972f0f4a5cb683bb82282f787_chunk_10",
      "doc_id": "a06d9bb972f0f4a5cb683bb82282f787",
      "content": "\uf0a1The default block is optional. DOM EXAMPLE \n\uf0a1HTML Page / Web Page\n\nDOM EXAMPLE \n\uf0a1Typescript\n\nDOM EXAMPLE \n\uf0a1Web page after button click\n\nTEMPLATE STRINGS\n\uf0a1Template Example\n\uf0a1Inline Example: \n\uf0a1Backticks (``) Example: \n\nRECOMMENDED READS\n\uf0a1Typescript tutorial\n \nhttps://www.tutorialspoint.com/typescript/index.htm\n\uf0a1Arrays:\n \nhttps://www.tutorialspoint.com/typescript/typescript_arrays.htm \n\uf0a1JavaScript HTML DOM\n \nhttps://www.w3schools.com/js/js_htmldom.asp \n\nQUESTIONS",
      "source_doc_title": "W2 - Miscellaneous Topics",
      "section_id": null,
      "page_number": 12,
      "start_char": 4540,
      "end_char": 5004,
      "chunk_index": 10,
      "total_chunks": 11
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_0",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "MOBILE APPLICATIONS DEVELOPMENT 2\nIHAB SALAWDEH\nDEPARTMENT OF COMPUTER SCIENCE & APPLIED PHYSICS \n\nINTRODUCTION TO ANGULAR\n\nINTRODUCTION\n\uf0a1Framework vs Library\n\uf0a1Angular\n\uf0a1Single Page Applications\n\uf0a1First Angular Application\n\nLIBRARY\n\uf0a1A library is a collection of pre-written code that helps \ndevelopers perform common tasks. \uf0a1It provides specific functionality without enforcing a \nstructure. \uf0a1You call the library when needed in your code. \uf0a1Example: jQuery, React.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 1,
      "start_char": 0,
      "end_char": 462,
      "chunk_index": 0,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_1",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "\uf0a1Example: jQuery, React. FRAMEWORK\n\uf0a1A framework is a pre-built structure that provides tools, \nlibraries, and guidelines for developing software \nefficiently. \uf0a1It defines the architecture and enforces best practices, \nreducing repetitive coding\n\uf0a1Example: Angular, Django, Flutter\n\nFRAMEWORK VS LIBRARY\n\uf0a1Frameworks and libraries are both code written by someone else that helps you perform \nsome common tasks in a less verbose way. \uf0a1A framework inverts the control of the program.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 4,
      "start_char": 438,
      "end_char": 917,
      "chunk_index": 1,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_2",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "\uf0a1A framework inverts the control of the program. It tells the developer what they need. A \nlibrary doesn\u2019t. The programmer calls the library where and when they need it. \uf0a1The degree of freedom a library or framework gives the developer, will dictate how \n\u201copinionated\u201d it is.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 6,
      "start_char": 869,
      "end_char": 1144,
      "chunk_index": 2,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_3",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "FRAMEWORK VS LIBRARY\nFramework\nLibrary\nControl Flow\nCalls your code (Inversion of Control)\nYou call the library\nStructure Provides a predefined structure\nNo enforced structure\nFlexibility\nOpinionated, enforces rules\nMore freedom in implementation\nExamples\nAngular, Django\njQuery, React\n\nFRAMEWORK\n\uf0a1Advantages\n\uf0a1\nFocuses on \u201cwhat not how\u201d\n\uf0a1\nFollows recognised design patterns\n\uf0a1Disadvantages\n\uf0a1\u201cLimited\u201d behaviour\n\uf0a1\nComplexity\n\nANGULAR\n\uf0a1Angular is a typescript-based framework for developing user interfaces (UI) \n\uf0a1Developed by Google and released in 2016\n\uf0a1Development platform for creating efficient and sophisticated single-page apps (SPAs).",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 6,
      "start_char": 1144,
      "end_char": 1783,
      "chunk_index": 3,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_4",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "\uf0a1It provides a set of CLI tools that make the development faster and more efficient (ng \ncommand)\n\uf0a1Angular website is: https://angular.io/ \n\nANGULAR\n\uf0a1A component-based framework for building scalable web applications\n\uf0a1A collection of well-integrated libraries that cover a wide variety of features, including \nrouting, forms management, client-server communication, and more\n\uf0a1A suite of developer tools to help you develop, build, test, and update your code\n\nSINGLE PAGE APPLICATIONS\n\uf0a1Improved speed and responsiveness.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 9,
      "start_char": 1783,
      "end_char": 2302,
      "chunk_index": 4,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_5",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "\uf0a1\nFaster loading times are the result of the SPA only needing to load the information necessary based on the \nuser\u2019s action rather than loading an entire page from scratch. \uf0a1Greater stability. \uf0a1\nThis includes caching capabilities and reduced bandwidth usage, which allow the pages to be displayed \neven where internet connections might be faulty. \uf0a1Better UX. \uf0a1\nSince pages load quicker and appear interactive, users are more engaged and have an improved \nexperience working with the content presented.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 11,
      "start_char": 2302,
      "end_char": 2803,
      "chunk_index": 5,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_6",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "\uf0a1Faster development. \uf0a1\nSPAs can take advantage of a number of different APIs, allowing developers to work separately on the \nfront end, back end, and connections that drive content more iteratively. SINGLE PAGE APPLICATIONS\n\uf0a1Easier debugging. \uf0a1\nThis flexibility also applies to testing and fixes since codebases tend to be more modular and allow \ndevelopers to work on different parts at the same time. \uf0a1Cross-platform compatibility.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 11,
      "start_char": 2803,
      "end_char": 3236,
      "chunk_index": 6,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_7",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "\uf0a1Cross-platform compatibility. \uf0a1\nSince SPAs can use a single codebase, they can also be designed to run on any platform or browser, \ngiving users a seamless experience as they switch devices. \uf0a1Mobile-friendliness. \uf0a1\nCode can also be reused to design mobile apps and responsive pages that display just as well on \nphone-based browsers and apps as they do on desktop devices, supporting mobile users without \nloss of functionality.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 12,
      "start_char": 3206,
      "end_char": 3635,
      "chunk_index": 7,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_8",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "ANGULAR\n\uf0a1A component-based framework\n\uf0a1Each component represents some \nfunctionality of the App. ANGULAR\n\uf0a1Components in Angular are composed of \nthree key elements:\n1. Meta-Data\n2. Templates\n3.",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 12,
      "start_char": 3635,
      "end_char": 3827,
      "chunk_index": 8,
      "total_chunks": 10
    },
    {
      "chunk_id": "8ca209afc1859ca51b75271c7e3256df_chunk_9",
      "doc_id": "8ca209afc1859ca51b75271c7e3256df",
      "content": "Meta-Data\n2. Templates\n3. Class\n\nFIRST ANGULAR APPLICATION\n\uf0a1Install the Angular CLI (Command Line Interface):\n\uf0a1npm install -g @angular/cli\n\uf0a1Create the Angular application:\n\uf0a1\nng new my-first-app\n\uf0a1Navigate to the Angular application\u2019s folder:\n\uf0a1\ncd my-first-app\n\uf0a1Run the Angular app:\n\uf0a1\nng serve\n\nFIRST ANGULAR APPLICATION\n\uf0a1Now the application is running on a port, usually port 4200\n\uf0a1Open a browser and navigate to the port\n\nFIRST ANGULAR APPLICATION\n\nQUESTIONS",
      "source_doc_title": "W3 - Introduction to Angular",
      "section_id": null,
      "page_number": 14,
      "start_char": 3802,
      "end_char": 4260,
      "chunk_index": 9,
      "total_chunks": 10
    }
  ]
}